{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Sentiment Using a Transformer\n",
    "\n",
    "This notebook provides you with a complete code example that predicts the sentiment of movie reviews using a transformer encoder network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the IMDB Dataset\n",
    "\n",
    "Start by downloading the Large Movie Review Dataset (often referred to as the IMDB dataset, as it’s available at https://huggingface.co/datasets/imdb). It contains 50,000 movie reviews, labeled as positive or negative. The dataset is divided into 25,000 reviews for training and 25,000 reviews for testing.\n",
    "\n",
    "Download the IMDB dataset ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"imdb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... splitting the training and validation datasets ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = dataset[\"train\"].train_test_split(\n",
    "    test_size=0.2,\n",
    "    stratify_by_column=\"label\",\n",
    "    seed=42,\n",
    ")\n",
    "train_dataset, val_dataset = split[\"train\"], split[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and print some example reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "examples = train_dataset.select(np.random.randint(0, len(train_dataset), 3))\n",
    "df = pd.DataFrame({\"Text\": examples[\"text\"], \"Label\": examples[\"label\"]})\n",
    "styled_df = df.style.set_properties(**{\"text-align\": \"left\"}).set_table_styles(\n",
    "    [{\"selector\": \"th\", \"props\": [(\"text-align\", \"center\")]}]\n",
    ")\n",
    "with pd.option_context(\"display.max_colwidth\", None):\n",
    "    display(styled_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the Reviews\n",
    "\n",
    "Implement a function to tokenize a sentence ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import contractions\n",
    "import re\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "def tokenize(text):\n",
    "    \"\"\"Tokenize text.\"\"\"\n",
    "    text = contractions.fix(text)\n",
    "    \n",
    "    replacements = {\"’\": \"'\", \"‘\": \"'\", \"“\": '\"', \"”\": '\"', \" ́\": \"'\", \" ́ ́\": '\"'}\n",
    "    for old, new in replacements.items():\n",
    "        text = text.replace(old, new)\n",
    "\n",
    "    tokens = tokenizer(text)\n",
    "\n",
    "    filtered_tokens = [\n",
    "        token for token in tokens\n",
    "        if re.match(r\"^[a-zA-Z0-9]+(-[a-zA-Z0-9]+)*(_[a-zA-Z0-9]+)*$\", token)\n",
    "    ]\n",
    "    \n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... create a vocabulary ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "def imdb_iterator(dataset):\n",
    "    \"\"\"Iterate over the IMBD dataset.\"\"\"\n",
    "    for data in dataset:\n",
    "        yield tokenize(data[\"text\"])\n",
    "\n",
    "vocab = build_vocab_from_iterator(\n",
    "    imdb_iterator(train_dataset),\n",
    "    specials=[\"<unk>\"],\n",
    ")\n",
    "vocab.set_default_index(vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and preprocess the training, validation, and testing datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(sample):\n",
    "    \"\"\"Preprocess the input data.\"\"\"\n",
    "    tokens = tokenize(sample[\"text\"])\n",
    "    indices = vocab(tokens)\n",
    "    sample.update({\"x\": indices})\n",
    "    return sample\n",
    "\n",
    "train_dataset = train_dataset.map(preprocessing)\n",
    "val_dataset = val_dataset.map(preprocessing)\n",
    "test_dataset = dataset[\"test\"].map(preprocessing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Transformer Encoder Layer\n",
    "\n",
    "Prepare a class to implement a multi-head attention layer ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deeplay as dl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MultiHeadAttentionLayer(dl.DeeplayModule):\n",
    "    \"\"\"\"Multi-head attention layer.\"\"\"\n",
    "    \n",
    "    def __init__(self, features, num_heads):\n",
    "        \"\"\"Initialize multi-head attention layer.\"\"\"\n",
    "        super().__init__()\n",
    "        self.features, self.num_heads = features, num_heads\n",
    "        self.layer = dl.Layer(nn.MultiheadAttention, features, num_heads)\n",
    "\n",
    "    def forward(self, x, batch_indices):\n",
    "        \"\"\"Calculate forward pass.\"\"\"\n",
    "        attn_mask = self._fetch_attn_mask(batch_indices)\n",
    "        y, *_ = self.layer(x, x, x, attn_mask=attn_mask)\n",
    "        return y\n",
    "\n",
    "    def _fetch_attn_mask(self, batch_indices):\n",
    "        \"\"\"Get attention mask.\"\"\"\n",
    "        return ~torch.eq(batch_indices.unsqueeze(1),\n",
    "                         batch_indices.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and a class to implement a transformer encoder layer ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn.norm import LayerNorm\n",
    "\n",
    "class TransformerEncoderLayer(dl.DeeplayModule):\n",
    "    \"\"\"Transformer encoder layer.\"\"\"\n",
    "\n",
    "    def __init__(self, d_model, num_heads, feedforward_dim, dropout_p=0.0):\n",
    "        \"\"\"Initialize transformer encoder layer.\"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.feedforward_dim = feedforward_dim\n",
    "        self.dropout_p = dropout_p\n",
    "\n",
    "        self.self_attn = MultiHeadAttentionLayer(d_model, num_heads)\n",
    "        self.attn_dropout = dl.Layer(nn.Dropout, dropout_p)\n",
    "        self.attn_skip = dl.Add()\n",
    "        self.attn_norm = dl.Layer(LayerNorm, d_model, eps=1e-6)\n",
    "        \n",
    "        self.feedforward = dl.Sequential(\n",
    "            dl.Layer(nn.Linear, d_model, feedforward_dim),\n",
    "            dl.Layer(nn.ReLU),\n",
    "            dl.Layer(nn.Linear, feedforward_dim, d_model),\n",
    "        )\n",
    "        self.feedforward_dropout = dl.Layer(nn.Dropout, dropout_p)\n",
    "        self.feedforward_skip = dl.Add()\n",
    "        self.feedforward_norm = dl.Layer(LayerNorm, d_model, eps=1e-6)\n",
    "        \n",
    "    def forward(self, x, batch_index):\n",
    "        \"\"\"Calculate forward pass.\"\"\"\n",
    "        y_attn = self.self_attn(x, batch_index)\n",
    "        y_attn = self.attn_dropout(y_attn)\n",
    "        y_attn = self.attn_skip(x, y_attn)\n",
    "        y_attn = self.attn_norm(y_attn, batch_index)\n",
    "\n",
    "        y = self.feedforward(y_attn)\n",
    "        y = self.feedforward_dropout(y)\n",
    "        y = self.feedforward_skip(y_attn, y)\n",
    "        y = self.feedforward_norm(y, batch_index)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Transformer Encoder Model\n",
    "\n",
    "Build a class to implement a transformer encoder model ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderModel(dl.DeeplayModule):\n",
    "    \"\"\"Transformer encoder model.\"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, d_model, num_heads, feedforward_dim,\n",
    "                 num_layers, out_dim, dropout_p=0.0):\n",
    "        \"\"\"Initialize transformer encoder model.\"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.feedforward_dim = feedforward_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout_p = dropout_p\n",
    "        self.out_dim = out_dim\n",
    "        \n",
    "        self.embedding = dl.Layer(nn.Embedding, vocab_size, d_model)\n",
    "\n",
    "        self.pos_encoder = dl.IndexedPositionalEmbedding(d_model)\n",
    "        self.pos_encoder.dropout.configure(p=dropout_p)\n",
    "        \n",
    "        self.blocks = dl.LayerList()\n",
    "        for _ in range(num_layers):\n",
    "            self.blocks.append(\n",
    "                TransformerEncoderLayer(\n",
    "                    d_model, num_heads, feedforward_dim, dropout_p=dropout_p\n",
    "                )\n",
    "            )\n",
    "            \n",
    "        self.out = dl.Sequential(\n",
    "            dl.Layer(nn.Dropout, dropout_p),\n",
    "            dl.Layer(nn.Linear, d_model, d_model // 2),\n",
    "            dl.Layer(nn.ReLU),\n",
    "            dl.Layer(nn.Linear, d_model // 2, out_dim), \n",
    "            dl.Layer(nn.Sigmoid),\n",
    "        )\n",
    "        \n",
    "    def forward(self, seq):\n",
    "        \"\"\"Calculate forward pass.\"\"\"\n",
    "        h = self.embedding(seq[\"x\"]) * self.d_model ** 0.5\n",
    "        h = self.pos_encoder(h, seq[\"batch_indices\"])\n",
    "        \n",
    "        for layer in self.blocks:\n",
    "            h = layer(h, seq[\"batch_indices\"])\n",
    "        \n",
    "        batch_size = torch.max(seq[\"batch_indices\"]) + 1\n",
    "        g = torch.zeros(batch_size, self.d_model, device=h.device)\n",
    "        g = g.scatter_add(0, seq[\"batch_indices\"][:, None].expand_as(h), h)\n",
    "        g = g / torch.bincount(seq[\"batch_indices\"])[:, None]\n",
    "\n",
    "        return self.out(g).squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... instantiate the transformer encoder model ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TransformerEncoderModel(\n",
    "    vocab_size=len(vocab),\n",
    "    d_model=300,\n",
    "    num_heads=12,\n",
    "    feedforward_dim=512,\n",
    "    num_layers=4,\n",
    "    out_dim=1,\n",
    "    dropout_p=0.1,\n",
    ").create()\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and add pretrained embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import GloVe\n",
    "\n",
    "glove = GloVe(name=\"42B\", dim=300, cache=\"glove_embeddings_dataset\")\n",
    "\n",
    "model.embedding.weight.data = glove.get_vecs_by_tokens(\n",
    "    vocab.get_itos(), lower_case_backup=True\n",
    ")\n",
    "model.embedding.weight.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "def collate(batch):\n",
    "    \"\"\"Combine data into a single batch that the model can process.\"\"\"\n",
    "    xs, ys, batch_indices = [], [], []\n",
    "    for i, b in enumerate(batch):\n",
    "        x, label = torch.tensor(b[\"x\"]), torch.tensor(b[\"label\"])\n",
    "        xs.append(x), ys.append(label)\n",
    "        batch_indices.append(torch.ones_like(x, dtype=torch.long) * i)\n",
    "    return Data(x=torch.cat(xs), batch_indices=torch.cat(batch_indices), \n",
    "                y=torch.Tensor(ys).float())\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, batch_size=8, shuffle=True, collate_fn=collate\n",
    ")\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset, batch_size=8, shuffle=False, collate_fn=collate\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset, batch_size=8, shuffle=False, collate_fn=collate\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "Compile the model ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdamW(dl.Optimizer):\n",
    "    \"\"\"AdamW optimizer.\"\"\"\n",
    "\n",
    "    def __pre_init__(self, **optimzer_kwargs):\n",
    "        \"\"\"Execute before initialization.\"\"\"\n",
    "        optimzer_kwargs.pop(\"classtype\", None)\n",
    "        super().__pre_init__(torch.optim.AdamW, **optimzer_kwargs)\n",
    "\n",
    "classifier = dl.BinaryClassifier(\n",
    "    model=model,\n",
    "    optimizer=AdamW(lr=1e-4),\n",
    ").create()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor=\"valBinaryAccuracy\",\n",
    "    dirpath=\"models\",\n",
    "    filename=\"ATT-model{epoch:02d}-val_accuracy{valBinaryAccuracy:.2f}\",\n",
    "    auto_insert_metric_name=False,\n",
    "    mode=\"max\",\n",
    ")\n",
    "trainer = dl.Trainer(max_epochs=5, callbacks=[checkpoint_callback])\n",
    "trainer.fit(classifier, train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Trained Model\n",
    "\n",
    "Load the best model ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, os\n",
    "\n",
    "best_model = glob.glob(\"./models/ATT-model*\")\n",
    "best_model = max(best_model, key=os.path.getctime)\n",
    "best_classifier = dl.BinaryClassifier \\\n",
    "    .load_from_checkpoint(best_model, model=model).create()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... test the trained model ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = trainer.test(best_classifier, test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and display the model’s prediction on some reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "best_classifier.model.eval()\n",
    "\n",
    "texts, labels, predictions = [], [], []\n",
    "for idx in random.sample(range(len(test_dataset)), 3):\n",
    "    sample = test_dataset[idx]\n",
    "    \n",
    "    input_tensor = torch.Tensor(vocab(tokenize(sample[\"text\"]))).long()\n",
    "    test_input = {\n",
    "        \"x\": input_tensor,\n",
    "        \"batch_indices\": torch.zeros_like(input_tensor, dtype=torch.long)\n",
    "    }\n",
    "\n",
    "    probability = classifier.model(test_input)\n",
    "    pred = probability > 0.5\n",
    "\n",
    "    texts.append(sample[\"text\"])\n",
    "    labels.append(sample[\"label\"])\n",
    "    predictions.append(pred.item() * 1)\n",
    "\n",
    "df = pd.DataFrame({\"text\": texts, \"label\": labels, \"prediction\": predictions})\n",
    "styled_df = df.style.set_properties(**{\"text-align\": \"left\"}).set_table_styles(\n",
    "    [{\"selector\": \"th\", \"props\": [(\"text-align\", \"center\")]}]\n",
    ")\n",
    "with pd.option_context(\"display.max_colwidth\", None):\n",
    "    display(styled_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py_env_dlcc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
