{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **1. Introduction to Transformers**\n",
    "\n",
    "Since Google introduced the Transformer architecture in 2017, it has rapidly become a fundamental component in the advancement of deep learning technologies. Its significance lies in its innovative approach to processing sequential data, which makes it critical for various state-of-the-art applications like [Vision Transformers](https://arxiv.org/abs/2010.11929) (ViT), [Contrastive Language-Image Pre-training](https://arxiv.org/abs/2103.00020v1)(CLIP), and [Alphafold](https://www.nature.com/articles/s41586-021-03819-2). The architecture's unique features address key challenges in deep learning, such as efficient data processing and the ability to capture complex relationships within the data.\n",
    "\n",
    "The Transformer comprises two main parts: the encoder and the decoder. This discussion first focuses on the encoder, introducing the basic principles of the Transformer architecture that have become popular in many deep learning models. \n",
    "\n",
    "**Figure 1** provides a schematic of the Transformer encoder,  structured as a stack of $N$ identical layers. Each layer consists of a *multi-head attention layer* (**Figure 1a**) and a *fully-connected dense neural network* (FCNN, **Figure 1b**). The multi-head attention layer is particularly important as it enables the neural network to leverage intrinsic relationships between the input data's features. Moreover, it provides Transformers with a key computational advantage: parallelization.  The fully-connected dense neural network, in turn, is used to integrate and refine data processed by the multi-head attention layer, enhancing the understanding and representation of the input data.\n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"images/transformer_fig_1.png\" width=200 height=auto>\n",
    "  <br>\n",
    "  <b>Figure 1. Transformer encoder.</b>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **2. Understanding Multi-Head Attention**\n",
    "\n",
    "The Transformer's encoder has a Multi-Head Attention layer at its core. This layer is crucial for the model to understand sequential data's intricacies. It has multiple attention \"*heads*\", each of which works independently to analyze various aspects of the input data's relationships (**Figure 2**).\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"images/transformer_fig_2.png\" width=400 height=auto>\n",
    "  <br>\n",
    "  <b>Figure 2. A multi-head attention layer operates as a set of parallel self-attention heads.</b>\n",
    "</p>\n",
    "\n",
    "A sentence is a natural way of representing a sequence. Words are arranged logically to convey meaning. For instance, consider the sentence in Figure 3a: \"*The man fell from the chair because it was flimsy*.\" For humans, deriving semantic information from this sentence is straightforward. We can answer questions like \"Does *flimsy* refer to the man or the *chair*?\" without much effort. However, for an algorithm, this task is more challenging.\n",
    "\n",
    "Attention heads use *self-attention* to comprehend the underlying relational structure of an input sequence.  In our example, this mechanism allows the model to evaluate the importance of each word in the sentence in relation to the others. Put simply, the model acquires the ability to identify the connections between words in the sentence. As the self-attention mechanism processes each word, it can recognize the correlation between the words 'flimsy' and 'chair', enabling each word to \"attend\" to other positions in the input sequence for hints that could assist in encoding its representation better.\n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"images/transformer_fig_3.png\" width=500 height=auto>\n",
    "  <br>\n",
    "  <b>Figure 3. Input sequence representation.</b>\n",
    "</p>\n",
    "\n",
    "The input sequence is represented by a matrix, denoted by $X \\in \\mathbb{R}^{n \\times d}$. Here, $n$ is the sequence length, and $d$ is the number of features that represent each element, as shown in **Figure 3b**. Words, for example, cannot be fed directly into the model. Instead, they are first transformed into a vector representation (how this is done will be discussed later). The input sequence is then transformed into three matrices: the query matrix $Q \\in \\mathbb{R}^{n \\times d_m}$, the key matrix $K \\in \\mathbb{R}^{n \\times d_m}$, and the value matrix $V \\in \\mathbb{R}^{n \\times d_m}$, where $d_m$ is the latent dimensionality of the self-attention module. These matrices are calculated by multiplying the input sequence by three different weight matrices, $W^Q$, $W^K$, and $W^V$, respectively (**Figure 4**). These weight matrices are initialized randomly and are learned during the training process.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"images/transformer_fig_4.png\" width=600 height=auto>\n",
    "  <br>\n",
    "  <b>Figure 4. The input sequence is transformed into query, key, and value matrices.</b>\n",
    "</p>\n",
    "\n",
    "The concept of self-attention is based on the idea that every element in a sequence can be represented as a linear combination of the other elements in the sequence. To achieve this, we calculate the dot product of the query and key vectors for each element, resulting in an $n × n$ matrix where each element $(i, j)$ is the dot product between the query vector of the $i$-th element and the key vector of the $j$-th element. This matrix is then normalized by dividing each element by the square root of the latent dimensionality of the self-attention module (i.e., $d_m$), and applying the softmax function (as shown in **Figure 5a**). The normalized matrix is called the attention matrix, $A$ (**Figure 5b**), in which each element $(i, j)$ represents the importance of the $j$-th element in the representation of the $i$-th element.\n",
    "\n",
    "For clarity, consider the example shown in **Figure 5b**, where the importance of each element is color-coded in a heat map format. A properly trained self-attention module should assign high values to semantically similar elements in the same row. For instance, the importance of the word \"flimsy\" in the representation of \"chair\" should be high because they're mutually descriptive words. \n",
    "\n",
    "To calculate the output of the self-attention module, we multiply the attention matrix $A$ by the value matrix $V$ (as shown in **Figure 5c**). The resulting matrix has a shape of $(n, d_m)$, where the representation of each element is a linear combination of the representations of all other elements in the sequence. This is why this module is called self-attention because it allows each element to interact with the others and learn which representations it should pay attention to.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"images/transformer_fig_5.png\" width=700 height=auto>\n",
    "  <br>\n",
    "  <b>Figure 5. Self-attention mechanism.</b>\n",
    "</p>\n",
    "\n",
    "Finally, the output of each attention head is concatenated and combined with a linear transformation to produce the final output of the multi-head attention layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **3. Implementing a Transformer Encoder for sentiment analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will implement a Transformer encoder to perform sentiment analysis, a well-known task in Natural Language Processing (NLP) that involves categorizing text.\n",
    "\n",
    "\n",
    "#### **3.1 The IMDB dataset**\n",
    "\n",
    "Let us start by downloading the [Large Movie Review Dataset](https://huggingface.co/datasets/imdb) (often referred to as the IMDB dataset). It contains 50,000 movie reviews, labeled as positive or negative. The dataset is divided into 25,000 reviews for training and 25,000 reviews for testing. \n",
    "\n",
    "Use the following code snippet to download and prepare the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('imdb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`dataset` is dictionary that contains the training and testing sets. Each set is also a dictionary with two keys: `\"text\"` and `\"label\"`. The `\"text\"` key contains movie reviews, while the `\"label\"` key contains the sentiment of the review, where 0 denotes a negative sentiment and 1 denotes a positive sentiment.\n",
    "\n",
    "Before moving forward, it's important to split the training set into two parts: training and validation. We will use 20% of the training set as a validation set during the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = dataset[\"train\"].train_test_split(\n",
    "    test_size=0.2, # (1)\n",
    "    stratify_by_column=\"label\", # (2)\n",
    "    seed=42,  # (3)\n",
    ")\n",
    "\n",
    "train_dataset = split[\"train\"] \n",
    "val_dataset = split[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above **(1)** splits the train portion of the dataset into two subsets: a training dataset and a validation dataset. The split is determined by the `test_size` parameter, which specifies that 20% of the data should be allocated for validation.\n",
    "\n",
    "**(2)** The `stratify_by_column=\"label\"` argument ensures that the split retains the same distribution of labels in both the training and validation datasets as in the original dataset, which is beneficial for maintaining a balanced representation of classes. \n",
    " \n",
    "**(3)** The `seed=42` argument sets a seed for the random number generator to ensure reproducibility of the split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize three example reviews from the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Selecting 3 examples from the training dataset\n",
    "examples = train_dataset.select(np.random.randint(0, len(train_dataset), 3))\n",
    "\n",
    "# Creating a DataFrame\n",
    "df_examples = pd.DataFrame({\"Text\": examples[\"text\"], \"Label\": examples[\"label\"]})\n",
    "\n",
    "styled_df = df_examples.style.set_properties(**{\"text-align\": \"left\"}).set_table_styles(\n",
    "    [{\"selector\": \"th\", \"props\": [(\"text-align\", \"center\")]}]\n",
    ")\n",
    "with pd.option_context(\"display.max_colwidth\", None):\n",
    "    display(styled_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **3.2 Data Preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step in processing the data is *tokenization*. It involves breaking down a sentence into individual words or subwords. This process is extremely important in Natural Language Processing, as it helps the model to comprehend the structure of the input data.\n",
    "\n",
    "There are a [variety of tokenization methods](https://huggingface.co/learn/nlp-course/chapter2/4) available, each with its own set of advantages and drawbacks. The following code snippet creates a `tokenizer` object that splits the input text into tokens by using punctuation and white spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "def tokenize(text):\n",
    "    return tokenizer(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"This is a simple example!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is tokenized into the following list of tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **3.2.1 Handling contractions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the training data is more complicated than the example given above. It contains various types of words, punctuations, contractions, and other linguistic elements that require proper handling. Contractions, in particular, can pose a challenge to model training.\n",
    "\n",
    "Contractions are linguistic shortcuts that combine two words into one, often to mimic the spoken language and make writing more fluid. Some common examples include “*don't*” (for “*do not*”), “*I'm*” (for “*I am*”), and “*I've*” (for “*I have*”).\n",
    "\n",
    "Contractions are commonly used in spoken and written English, yet they can create difficulties for language models as they introduce variability. This is because contractions can represent the same idea in multiple forms. For example, “*it is*”, “*it's*”, and sometimes even “*tis*” can all convey the same meaning but are expressed differently.\n",
    "\n",
    "Depending on the task, an NLP model may struggle to recognize that these different forms have the same underlying meaning, leading to errors in understanding and processing text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code snippet standardizes the text by converting contractions into their expanded forms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import contractions\n",
    "\n",
    "def tokenize(text):\n",
    "    standardized_text = contractions.fix(text)\n",
    "    return tokenizer(standardized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"I can't believe we're gonna go to the beach tomorrow. It's gonna be so much fun!\"\n",
    "\n",
    "tokens = tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that cases such as “*can´t*”, “*we´re*”, “*gonna*”, and “*it´s*” are handled correctly.\n",
    "\n",
    "This standardization is important since it guarantees semantic consistency and robustness for various writing styles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **3.2.2 Removing noise**\n",
    "\n",
    "Text, like any other data, contains noise. In the context of NLP, noise is everything that is not semantically relevant to the task at hand. Punctuation, question/exclamation marks, quotes, and non-alphabetic characters are often considered noise.\n",
    "\n",
    "The following code snippet performs additional processing on the text by removing any unnecessary noise or unwanted elements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    standardized_text = contractions.fix(text)\n",
    "\n",
    "    standardized_text = (  # (1)\n",
    "        standardized_text.replace(\"’\", \"'\")\n",
    "        .replace(\"‘\", \"'\")\n",
    "        .replace(\"“\", '\"')\n",
    "        .replace(\"”\", '\"')\n",
    "        .replace(\"´\", \"'\")\n",
    "        .replace(\"´´\", '\"')\n",
    "    )\n",
    "\n",
    "    tokens = tokenizer(standardized_text)\n",
    "\n",
    "    filtered_tokens = [\n",
    "        token\n",
    "        for token in tokens\n",
    "        if re.match(r\"^[a-zA-Z0-9]+(-[a-zA-Z0-9]+)*(_[a-zA-Z0-9]+)*$\", token)  # (2)\n",
    "    ]\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After standardizing contractions, **(1)** the text undergoes a second standardization process where different types of quotation marks and apostrophes (’, ‘, “, ”, ´, and ´´) are replaced with their ASCII equivalents (' for apostrophes and \" for quotation marks). This standardization makes the text easier to process by ensuring consistency in punctuation marks, which is especially important for tokenization and further text analysis tasks.\n",
    "\n",
    "**(2)** The code filters the tokens based on a regular expression pattern. The pattern `r\"^[a-zA-Z0-9]+(-[a-zA-Z0-9]+)*(_[a-zA-Z0-9]+)*$\"` is designed to match tokens that consist of alphanumeric characters (a-zA-Z0-9), and these characters can be optionally connected by hyphens (-) or underscores (_), in the case of compound words or phrases.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text with compound words connected with a hyphen\n",
    "text = \"You'll understand why cross-checking the data isn't just useful, it's crucial.\"\n",
    "\n",
    "tokens = tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **3.2.3 Creating a vocabulary**\n",
    "\n",
    "Up until now, we have a function that performs standardization, cleaning, and tokenization of the text. \n",
    "\n",
    "The following step is to transform the tokens into numerical representations. In order to do this, we need to create a vocabulary which maps each token to a unique integer. A vocabulary is a set of distinct words in a dataset, and it is formed by iterating through the tokens and adding them to the set.\n",
    "\n",
    "The following code snippet creates a vocabulary from the training dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "\n",
    "def imdb_iterator(dataset):  # (1)\n",
    "    for data in dataset:\n",
    "        yield tokenize(data[\"text\"])\n",
    "\n",
    "\n",
    "vocab = build_vocab_from_iterator(\n",
    "    imdb_iterator(train_dataset), # (2)\n",
    "    specials=[\"<unk>\"], # (3)\n",
    ")\n",
    "vocab.set_default_index(vocab[\"<unk>\"]) # (4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(1)** `imdb_iterator` is defined to iterate over the dataset. For each item in the dataset, it yields the tokenized version of the text data. \n",
    "\n",
    "**(2)** `build_vocab_from_iterator` is used to build the vocabulary. It takes an iterator as its first argument, which, in this case, is provided by calling the `imdb_iterator` function on the training dataset.\n",
    "\n",
    "**(3)** The specials argument is used to define special tokens that should be included in the vocabulary. In this case, `[\"<unk>\"]` is passed, which means a special token `<unk>` (usually representing \"unknown\" tokens that are not found in the vocabulary) is added to the vocabulary. \n",
    "\n",
    "**(4)** `vocab.set_default_index(vocab[\"<unk>\"])` sets the default index that will be returned by the vocabulary when a token is not found. This is crucial for handling out-of-vocabulary words during the processing of text, ensuring that the model can handle words it hasn't seen before in a standardized way.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`vocab` works as a lookup table that assigns a unique integer to each token in the dataset. \n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_tokens = [\"the\", \"movie\", \"was\", \"amazing\", \"and\", \"i\", \"loved\", \"it\"]\n",
    "\n",
    "for token in example_tokens:\n",
    "    print(f\"{token}: {vocab[token]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now apply the standardization, cleaning, tokenization, and numerical representation to the training dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(sample): # (1)\n",
    "    tokens = tokenize(sample[\"text\"])  # (2)\n",
    "    \n",
    "    sample.update( \n",
    "        {\"x\": vocab(tokens)}, # (3)\n",
    "    ) \n",
    "    return sample\n",
    "\n",
    "\n",
    "train_dataset = train_dataset.map(preprocessing)  # (4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(1)** defines the `preprocess` function. This function has two main steps. **(2)** First, it performs tokenization and data cleaning on the input data. **(3)** Second, it maps the tokens to their corresponding numerical indices using the `vocab` object. These indices are then added to the original sample dictionary under the key `\"x\"`.\n",
    "\n",
    "Finally, **(4)** applies the preprocessing function to every element in `train_dataset`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also apply the same preprocessing to the validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = val_dataset.map(preprocessing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = dataset[\"test\"].map(preprocessing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **3.3 Building a Transformer Encoder Layer**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now explore how to implement a Transformer encoder layer. As discussed earlier, this layer consists of two essential components: a multi-head attention layer and a fully-connected feedforward neural network.\n",
    "\n",
    "The workflow within a Transformer encoder layer starts with the input of a sequence of tokens. These tokens first encounter the multi-head attention layer, which processes them to produce an intermediate output (**Figure 6a**). This output is then merged back with the original input through a residual connection, and the combined result undergoes layer normalization to maintain stability in the network's learning process (**Figure 6b**).\n",
    "\n",
    "Following this, the process mirrors itself with the fully connected feedforward neural network. This network takes the normalized output from the multi-head layer and subjects it to a sequence of linear transformations and activation functions (**Figure 6c**). The resultant output is again combined with its input (the normalized output from the multi-head attention stage) through another residual connection. Finally, layer normalization is applied to this combined output (**Figure 6d**).\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"images/transformer_fig_6.png\" width=200 height=auto>\n",
    "  <br>\n",
    "  <b>Figure 6. Transformer encoder layer.</b>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by implementing the multi-head attention layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from deeplay import DeeplayModule, Layer\n",
    "\n",
    "\n",
    "class MultiHeadAttentionLayer(DeeplayModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        features,\n",
    "        num_heads,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.features = features\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        self.layer = Layer(nn.MultiheadAttention, features, num_heads)\n",
    "\n",
    "    def forward(self, x, batch_indices): # (1)\n",
    "        attn_mask = self._fetch_attn_mask(batch_indices)  # (2)\n",
    "        y, *_ = self.layer(x, x, x, attn_mask=attn_mask) # (3)\n",
    "        return y\n",
    "\n",
    "    def _fetch_attn_mask(self, batch_indices):\n",
    "        return ~torch.eq(batch_indices.unsqueeze(1), batch_indices.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(1)** The multi-head attention layer takes two main inputs: a concatenated tensor of sequences, `x`, and a tensor of batch indices, `batch_indices`. The tensor `x` stacks input sequences along its first dimension. The `batch_indices` tensor indicates the batch to which each element in `x` belongs. For instance, if `batch_indices = [0, 0, 0, 1, 1, 2, 2, 2, 2]`, it means the first three elements of `x` are from the first sequence in the batch, the next two elements are from the second sequence, and the remaining elements are from the third sequence.\n",
    "\n",
    "**(2)** An attention mask is dynamically generated based on `batch_indices` using the method `_fetch_attn_mask`. This mask ensures the model does not attend across elements of different sequences within the same batch. Specifically, the generated mask is a square tensor with dimensions `(batch_seq_len, batch_seq_len)`, where `batch_seq_len` represents the total number of elements across all sequences in the batch.\n",
    "\n",
    "**(3)** Finally, this mask is provided to the multi_head_attention layer as the `attn_mask` parameter. By doing so, it restricts the attention mechanism to focus only within individual sequences, preventing the layer from considering elements from separate sequences in the batch during attention operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `MultiHeadAttentionLayer`, we can now implement the Transformer encoder layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn.norm import LayerNorm\n",
    "\n",
    "from deeplay import Sequential, Add\n",
    "\n",
    "class TransformerEncoderLayer(DeeplayModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model,\n",
    "        num_heads,\n",
    "        feedforward_dim,\n",
    "        dropout_p=0.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.feedforward_dim = feedforward_dim\n",
    "        self.dropout_p = dropout_p\n",
    "\n",
    "        self.self_attn = MultiHeadAttentionLayer(d_model, num_heads)\n",
    "        self.attn_dropout = Layer(nn.Dropout, dropout_p) \n",
    "        self.attn_skip = Add() \n",
    "        self.attn_norm = Layer(LayerNorm, d_model, eps=1e-6)\n",
    "\n",
    "        self.feedforward = Sequential(\n",
    "            Layer(nn.Linear, d_model, feedforward_dim),\n",
    "            Layer(nn.ReLU),\n",
    "            Layer(nn.Linear, feedforward_dim, d_model),\n",
    "        )\n",
    "        self.feedforward_dropout = Layer(nn.Dropout, dropout_p) \n",
    "        self.feedforward_skip = Add() \n",
    "        self.feedforward_norm = Layer(LayerNorm, d_model, eps=1e-6)\n",
    "\n",
    "    def forward(self, x, batch_index):\n",
    "        y_attn = self.self_attn(x, batch_index) # (1)\n",
    "        y_attn = self.attn_dropout(y_attn) # (2)\n",
    "        y_attn = self.attn_skip(x, y_attn) # (3)\n",
    "        y_attn = self.attn_norm(y_attn, batch_index) # (4)\n",
    "\n",
    "        y = self.feedforward(y_attn) # (5)\n",
    "        y = self.feedforward_dropout(y) # (6)\n",
    "        y = self.feedforward_skip(y_attn, y) # (7)\n",
    "        y = self.feedforward_norm(y, batch_index) # (8)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code defines `TransformerEncoderLayer`. \n",
    "\n",
    "The forward method of this class performs the following operations:\n",
    "\n",
    "**(1)** Applies multi-head attention on input sequence `x`.\n",
    "\n",
    "**(2)** Applies dropout to the attention output.\n",
    "\n",
    "**(3)** Adds the original input `x` to the dropout-adjusted attention output, implementing a residual connection.\n",
    "\n",
    "**(4)** Normalizes the output of the skip connection with layer normalization.\n",
    "\n",
    "**(5)** Passes the normalized attention output through the feedforward network.\n",
    "\n",
    "**(6)** Applies dropout to the feedforward network's output.\n",
    "\n",
    "**(7)** Adds the feedforward output to the attention output (before the feedforward network), creating another residual connection.\n",
    "\n",
    "**(8)** Normalizes the final output with layer normalization.\n",
    "\n",
    "See **Figure 6** for a visual representation of the Transformer encoder layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **3.4 Building a Transformer Encoder Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code snippet implements the Transformer Encoder model. \n",
    "\n",
    "This model comprises an embedding layer, a positional encoding layer, a stack of Transformer encoder layers, and a dense top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deeplay import LayerList, IndexedPositionalEmbedding\n",
    "\n",
    "\n",
    "class TransformerEncoderModel(DeeplayModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size,\n",
    "        d_model,\n",
    "        num_heads,\n",
    "        feedforward_dim,\n",
    "        num_layers,\n",
    "        out_dim,\n",
    "        dropout_p=0.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.feedforward_dim = feedforward_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout_p = dropout_p\n",
    "        self.out_dim = out_dim\n",
    "\n",
    "        self.embedding = Layer(nn.Embedding, vocab_size, d_model)\n",
    "        \n",
    "        self.pos_encoder = IndexedPositionalEmbedding(d_model)\n",
    "        self.pos_encoder.dropout.configure(p=dropout_p)\n",
    "\n",
    "        self.blocks = LayerList()\n",
    "        for _ in range(num_layers):\n",
    "            self.blocks.append(\n",
    "                TransformerEncoderLayer(\n",
    "                    d_model, num_heads, feedforward_dim, dropout_p=dropout_p\n",
    "                )\n",
    "            )\n",
    "            \n",
    "        self.out = Sequential(\n",
    "            Layer(nn.Dropout, dropout_p),\n",
    "            Layer(nn.Linear, d_model, d_model//2),\n",
    "            Layer(nn.ReLU),\n",
    "            Layer(nn.Linear, d_model//2, out_dim),\n",
    "            Layer(nn.Sigmoid),\n",
    "        )\n",
    "\n",
    "    def forward(self, seq):\n",
    "        h = self.embedding(seq[\"x\"]) * self.d_model**0.5 # (1)\n",
    "        h = self.pos_encoder(h, seq[\"batch_indices\"]) # (2)\n",
    "        \n",
    "        for layer in self.blocks:\n",
    "           h = layer(h, seq[\"batch_indices\"]) # (3)\n",
    "\n",
    "        # (4)\n",
    "        batch_size = torch.max(seq[\"batch_indices\"]) + 1 \n",
    "        g = torch.zeros(batch_size, self.d_model, device=h.device) \n",
    "        g = g.scatter_add(\n",
    "            0, seq[\"batch_indices\"][:, None].expand_as(h), h \n",
    "        )  \n",
    "        g = g / torch.bincount(seq[\"batch_indices\"])[:, None]  \n",
    "\n",
    "        output = self.out(g) # (5)\n",
    "        return output.squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `TransformerEncoderModel` class expects its input as a dictionary that includes two key components: `\"x\"`, which contains the input sequence, and their corresponding `\"batch_indices\"`.\n",
    "\n",
    "The forward method directs a series of operations to transform these inputs into a meaningful output:\n",
    "\n",
    "**(1)** Initially, the embedding layer transforms the input token IDs into dense vector representations. This transformation is facilitated by an embedding matrix, whose dimensions are determined by the vocabulary size and the specified embedding dimension (`d_model`). Each token ID is mapped to a unique vector in this high-dimensional space, effectively converting sparse categorical data into a format suitable for neural network processing.\n",
    "\n",
    "**(2)** [Positional encoding](https://arxiv.org/abs/1706.03762) is then applied to incorporate information about the order of tokens within each sequence. Unlike traditional sequence processing models that inherently understand sequence order (like RNNs), transformers treat inputs as unordered sets. The positional encoding uniquely marks each position in the sequence, enabling the model to recognize and utilize the order of tokens. This layer adds a vector to each token embedding, where each vector is unique to its position in the sequence, thereby embedding sequence order into the model's processing. \n",
    "\n",
    "**(3)** The core of the model consists of multiple Transformer encoder layers. Each layer processes the sequence in turn, with the output from one layer feeding into the next. These layers allow the model to learn complex relationships between different parts of the input sequence by applying attention mechanisms and feedforward neural networks. The result is a sequence of token representations that have been contextually enriched by information from other tokens in the sequence.\n",
    "\n",
    "**(4)** After processing through the transformer layers, the model computes an aggregate representation for each sequence in the batch. This is achieved by averaging the output vectors across the sequence length dimension, resulting in a single vector that captures the essence of each entire sequence. This step condenses the sequence data, distilling it into a form that represents the sequence as a whole.\n",
    "\n",
    "**(5)** Finally, the aggregated sequence representations are passed through a series of dense layers. This part of the model further processes the aggregated data, ultimately producing the final output. The dense layers typically include nonlinear activations and dropout for regularization, allowing the model to map the sequence representations to the desired output space effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now instantiate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TransformerEncoderModel(\n",
    "    vocab_size=len(vocab),\n",
    "    d_model=300,\n",
    "    num_heads=12,\n",
    "    feedforward_dim=512,\n",
    "    num_layers=4,\n",
    "    out_dim=1,\n",
    "    dropout_p=0.1,\n",
    ").create()\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the model is ready for training, incorporating *pre-trained embeddings* can significantly improve its performance. \n",
    "\n",
    "Pre-trained embeddings are word representations that have been learned from a large corpus of text data. They capture rich semantic information about words and are often used to initialize the embedding layer of a model. This initialization can help the model start with a better understanding of the input data, which can lead to faster convergence and better generalization.\n",
    "\n",
    "Among the most popular pre-trained embeddings are those provided by [GloVe](https://nlp.stanford.edu/projects/glove/). These embeddings are trained on large corpora and capture rich semantic information about words. The following code snippet downloads the GloVe embeddings and initializes the model's embedding layer with these pre-trained weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import GloVe\n",
    "\n",
    "glove = GloVe(name=\"42B\", dim=300, cache=\"./.vector_cache\") # (1)\n",
    "\n",
    "model.embedding.weight.data = glove.get_vecs_by_tokens(\n",
    "    vocab.get_itos(), lower_case_backup=True # (2)\n",
    ")\n",
    "model.embedding.weight.requires_grad = False # (3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(1)** The GloVe embeddings are loaded using the `torchtext.vocab.GloVe` class. The parameters specify the version of GloVe to use (`\"42B\"` indicates the version trained on 42 billion tokens) and the dimensionality of the embeddings (`dim=300`). The cache parameter determines where the downloaded embeddings will be stored locally.\n",
    "\n",
    "**(2)** Next, the embeddings for the words in the model's vocabulary (`vocab`) are retrieved and assigned to the model's embedding layer. `get_vecs_by_tokens` fetches the GloVe vectors for each token in the model's vocabulary (`vocab.get_itos()` returns a list of tokens). The `lower_case_backup` option attempts to match a token's lowercase form if the original form is not found in GloVe's vocabulary.\n",
    "\n",
    "**(3)** Finally, `requires_grad` is set to `False` to freeze the embedding layer's weights. This prevents the pre-trained embeddings from being updated during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **3.5 Defining the Data Loaders**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before training the model, we need to define data loaders that will feed the training, validation, and test data to the model during the training and evaluation processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "\n",
    "def collate_fn(batch):  # (1)\n",
    "    xs, ys, batch_indices = [], [], []\n",
    "\n",
    "    for i, b in enumerate(batch):\n",
    "        x = torch.tensor(b[\"x\"]) \n",
    "        label = torch.tensor(b[\"label\"])\n",
    "\n",
    "        xs.append(x) # (2)\n",
    "        ys.append(label) # (3)\n",
    "\n",
    "        batch_indices.append(torch.ones_like(x, dtype=torch.long) * i) # (4)\n",
    "\n",
    "    return Data( # (5)\n",
    "        x=torch.cat(xs), \n",
    "        batch_indices=torch.cat(batch_indices), \n",
    "        y=torch.Tensor(ys).float(), \n",
    "    )\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, batch_size=8, shuffle=True, collate_fn=collate_fn\n",
    ")\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset, batch_size=8, shuffle=False, collate_fn=collate_fn\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset, batch_size=8, shuffle=False, collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(1)** The `collate_fn` function is essential for preprocessing batches of data. When loading a batch of data points, PyTorch's DataLoader uses `collate_fn` to determine how the data points are combined into a single batch that the model can process. \n",
    "\n",
    "In `collate_fn`, the input batch is a list of dictionaries, where each dictionary contains the input sequence (`\"x\"`) and the corresponding label (`\"label\"`). \n",
    "\n",
    "**(2)** For each sample in the batch, the input sequences (`\"x\"`) are converted into a tensor and appended to the list `xs`. \n",
    "\n",
    "**(3)** Similarly, the labels (`\"label\"`) are converted into a tensor and appended to the list `ys`. \n",
    "\n",
    "**(4)** The `batch_indices` list is populated with tensors that mark the batch index for each input sequence. This is achieved by creating a tensor of ones with the same shape as `x` and multiplying it by the index `i` of the current sample in the batch. \n",
    "\n",
    "**(5)** Finally, `collate_fn` concatenates the input sequences, labels, and batch indices into a single dictionary and returns it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **3.6 Training the Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following create a `Trainer` object to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deeplay import BinaryClassifier, Optimizer, Trainer\n",
    "\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "\n",
    "\n",
    "class AdamW(Optimizer):\n",
    "    def __pre_init__(self, **optimzer_kwargs):\n",
    "        optimzer_kwargs.pop(\"classtype\", None)\n",
    "        super().__pre_init__(torch.optim.AdamW, **optimzer_kwargs)\n",
    "\n",
    "\n",
    "classifier = BinaryClassifier( # (1)\n",
    "    model=model,\n",
    "    optimizer=AdamW(lr=1e-4),\n",
    ").create()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(1)** The model is compiled with the AdamW optimizer and the binary cross-entropy loss function.\n",
    "\n",
    "**(2)** The `ModelCheckpoint` callback from the `lightning.pytorch.callbacks` module is configured to monitor the validation binary accuracy (`\"valBinaryAccuracy\"`). This callback saves the model checkpoints to the specified directory (`\"./models\"`) with a filename pattern that includes both the epoch number and the validation accuracy. The `auto_insert_metric_name=False` parameter indicates that the metric name (`valBinaryAccuracy`) should not be automatically inserted into the checkpoint filename, as the filename already includes it explicitly. Furthermore, the `mode=\"max\"` argument indicates that the checkpointing should save the models with the maximum `valBinaryAccuracy`, capturing the best-performing model over the training epochs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_callback = ModelCheckpoint( # (2)\n",
    "    monitor=\"valBinaryAccuracy\",\n",
    "    dirpath=\"./models\",\n",
    "    filename=\"ATT-model{epoch:02d}-val_accuracy{valBinaryAccuracy:.2f}\",\n",
    "    auto_insert_metric_name=False,\n",
    "    mode=\"max\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    max_epochs=5,\n",
    "    callbacks=[checkpoint_callback],\n",
    ")\n",
    "trainer.fit(classifier, train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training the model and saving its weights, we can evaluate it on the test set by loading the best model with the following code snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load best model\n",
    "import os\n",
    "import glob\n",
    "\n",
    "\n",
    "best_model = glob.glob(\"/workspace/models/ATT-model*\")\n",
    "best_model = max(best_model, key=os.path.getctime)\n",
    "classifier = BinaryClassifier.load_from_checkpoint(best_model, model=model).create()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that after just 5 rounds of training, the model reached an accuracy of approximately 87% on the test data, demonstrating the effectiveness of Transformer models for NLP tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = trainer.test(classifier, test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now display the model's predictions on a few examples from the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "num_samples = 3  # Number of random samples to display\n",
    "max_index = len(test_dataset) - 1\n",
    "\n",
    "# Generate unique random indices\n",
    "random_sample_indices = random.sample(range(max_index + 1), num_samples)\n",
    "\n",
    "# Initialize lists to store sample data\n",
    "texts = []\n",
    "labels = []\n",
    "predictions = []\n",
    "\n",
    "classifier.model.eval()\n",
    "\n",
    "for idx in random_sample_indices:\n",
    "    sample = test_dataset[idx]\n",
    "    \n",
    "    # Assuming tokenize and vocab functions are defined elsewhere\n",
    "    test_input = {}\n",
    "    test_input[\"x\"] = torch.Tensor(vocab(tokenize(sample[\"text\"]))).long()\n",
    "    test_input[\"batch_indices\"] = torch.zeros_like(test_input[\"x\"], dtype=torch.long)\n",
    "\n",
    "    # Assuming classifier.model returns a probability\n",
    "    prob = classifier.model(test_input)\n",
    "    pred = prob > 0.5\n",
    "    \n",
    "    # Append data for current sample to lists\n",
    "    texts.append(sample[\"text\"])\n",
    "    labels.append(sample[\"label\"])\n",
    "    predictions.append(pred.item() * 1)\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame({\n",
    "    \"text\": texts,\n",
    "    \"label\": labels,\n",
    "    \"prediction\": predictions,\n",
    "})\n",
    "\n",
    "styled_df = df.style.set_properties(**{\"text-align\": \"left\"}).set_table_styles(\n",
    "    [{\"selector\": \"th\", \"props\": [(\"text-align\", \"center\")]}]\n",
    ")\n",
    "\n",
    "# Display the DataFrame without truncating text\n",
    "with pd.option_context('display.max_colwidth', None):\n",
    "    display(styled_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to note that the model's performance can be further improved by fine-tuning the hyperparameters, increasing the number of training epochs, or using more advanced techniques such as learning rate schedules, gradient clipping, and early stopping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Appendix: Understanding positional encoding**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformers employ a unique strategy to incorporate sequence order information into their architecture through the use of positional encodings. \n",
    "\n",
    "The positional encodings are generated using a combination of sine and cosine functions of different frequencies. For each position $i$ in the sequence and each dimension $j$ of the positional encoding vector, the encoding is defined as follows:\n",
    "\n",
    "\\begin{align}\n",
    "PE_{(i,2j)} &= \\sin\\left(\\frac{i}{10000^{2j/d_{\\text{m}}}}\\right)\\\\\n",
    "PE_{(i,2j+1)} &= \\cos\\left(\\frac{i}{10000^{2j/d_{\\text{m}}}}\\right)\n",
    "\\end{align}\n",
    "\n",
    "To visualize how positional encodings vary across different positions and dimensions, consider the following code snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import deeplay as dl\n",
    "\n",
    "d_model = 20 \n",
    "length = 100\n",
    "\n",
    "PElayer = dl.IndexedPositionalEmbedding(d_model, max_length=length)\n",
    "PE = PElayer.embs.detach().numpy()\n",
    "\n",
    "fig, ax = plt.subplots(2, 1, figsize=(12, 8))\n",
    "\n",
    "c = ax[0].pcolormesh(PE, cmap='viridis')\n",
    "fig.colorbar(c, ax=ax[0], label='Encoding value')\n",
    "ax[0].set_title('Positional Encoding Heatmap')\n",
    "ax[0].set_xlabel('Dimension')\n",
    "ax[0].set_ylabel('Position')\n",
    "\n",
    "for i in range(min(d_model, 10)):\n",
    "    ax[1].plot(PE[:, i], label=f'Dim {i}')\n",
    "ax[1].set_title('Positional Encoding for Selected Dimensions')\n",
    "ax[1].set_xlabel('Position')\n",
    "ax[1].set_ylabel('Encoding value')\n",
    "ax[1].legend()\n",
    "\n",
    "plt.subplots_adjust(hspace=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script generates a heatmap of positional encodings for a given sequence length and dimensions (`d_model`). It plots the encoding values across positions (vertical axis) and dimensions (horizontal axis).\n",
    "\n",
    "Furthermore, the bottom subplot displays line plots for selected dimensions (up to the first 10 for clarity) against positions. This more detailed view demonstrates how the encoding values change across positions for individual dimensions, giving insight into the oscillatory pattern created by the sine and cosine functions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
