{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Virtually Straining a Biological Tissue with a Conditional GAN\n",
    "\n",
    "The virtual_staining.ipynb notebook provides you with a complete code example to virtually stain a biological tissue with a conditional GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading the Human Motor Neurons Dataset\n",
    "\n",
    "You’ll use a dataset that was originally published in the article: E. M. Christiansen et al., Cell 173:792-803, 2018. Specifically, you’ll use the human motor neurons dataset that is designated as “Condition A” in the article. This dataset comprises 22 pairs of brightfield and corresponding fluorescent images, with each pair including spatially registered images showcasing two fluorescent channels: Hoechst stain, revealing nuclei with a blue stain, and anti-TuJ1 stain, highlighting neurons in green. Notably, the brightfield images encompass a z-stack of 13 images across different focal planes, offering a comprehensive view of the cellular structures in question.\n",
    "\n",
    "Download the dataset from Google Storage. The dataset is about 10 GB and will take at least a few minutes to download, or longer depending on your internet connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, subprocess\n",
    "\n",
    "url = \"gs://in-silico-labeling/paper_data/\"\n",
    "path = \"virtual_staining_dataset\"\n",
    "if not os.path.exists(path):\n",
    "    for dataset in [\"train\", \"test\"]:\n",
    "        dataset_url = url + dataset + \"/Rubin/scott_1_0\"\n",
    "        dataset_dir = os.path.join(path, dataset)\n",
    "        os.makedirs(dataset_dir, exist_ok=True)\n",
    "        \n",
    "        command = [\"gsutil\", \"-m\", \"cp\", \"-r\", dataset_url, dataset_dir]\n",
    "        subprocess.run(command, check=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Dataset\n",
    "\n",
    "Implement a class containing the dataset ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "class VirtualStainingDataset(Dataset):\n",
    "    \"\"\"Dataset containing the brighfield and fluorescence images.\"\"\"\n",
    "\n",
    "    _cache = {}  # Class variable to cache loaded images.\n",
    "\n",
    "    def __init__(self, directory, transform=None, preload=False):\n",
    "        \"\"\"Initialize dataset.\"\"\"\n",
    "        self.transform = transform\n",
    "        self.preload = preload\n",
    "        self.images = []\n",
    "\n",
    "        self.image_dir = os.path.join(directory, \"scott_1_0\")\n",
    "        pattern = (\"lab-Rubin,condition-scott_1_0,acquisition_date,\"\n",
    "                   \"year-2016,month-2,day-6,\"\n",
    "                   \"well-r0*c0*,\"\n",
    "                   \"depth_computation,\"\n",
    "                   \"value-MAXPROJECT,is_mask-false,kind,value-ORIGINAL.png\")   \n",
    "        self.image_list = glob.glob(os.path.join(self.image_dir, pattern))\n",
    "\n",
    "        self.cache_key = self.image_dir\n",
    "        if self.preload:\n",
    "            if self.cache_key in VirtualStainingDataset._cache:\n",
    "                self.images = VirtualStainingDataset._cache[self.cache_key]\n",
    "            else:\n",
    "                for image_path in tqdm(self.image_list,\n",
    "                                       total=len(self.image_list),\n",
    "                                       desc=\"Preloading images\"):\n",
    "                    self.images.append(self.load_image(image_path))\n",
    "                VirtualStainingDataset._cache[self.cache_key] = self.images\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return number of images.\"\"\"\n",
    "        return len(self.image_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Get next input-target image couple.\"\"\"\n",
    "        if self.preload:\n",
    "            input_image, target_image = self.images[idx]\n",
    "        else:\n",
    "            input_image, target_image = self.load_image(self.image_list[idx])\n",
    "\n",
    "        if self.transform:\n",
    "            seed = np.random.randint(1_000_000_000)\n",
    "            \n",
    "            torch.manual_seed(seed)\n",
    "            input_image = self.transform[0](input_image)\n",
    "\n",
    "            torch.manual_seed(seed)\n",
    "            target_image = self.transform[1](target_image)\n",
    "\n",
    "        return input_image, target_image\n",
    "\n",
    "    def load_image(self, image_path):\n",
    "        \"\"\"Load input-target image couple.\"\"\"\n",
    "        input_image = []\n",
    "        for i in range(0, 13):\n",
    "            img_path = (image_path\n",
    "                        .replace(\"depth_computation\", f\"z_depth-{i},channel\")\n",
    "                        .replace(\"value-MAXPROJECT\", \"value-BRIGHTFIELD\"))\n",
    "            input_image.append(np.array(Image.open(img_path).convert(\"L\")))\n",
    "        input_image = np.stack(input_image, axis=-1)\n",
    "\n",
    "        target_image = np.array(Image.open(image_path))\n",
    "\n",
    "        return input_image, target_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... define the image transformations and normalizations ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "trans_bright = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.RandomCrop((256, 256)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomVerticalFlip(p=0.5),\n",
    "    transforms.Normalize(mean=[0.5] * 13, std=[0.5] * 13)\n",
    "])\n",
    "trans_fluorescent = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.RandomCrop((256, 256)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomVerticalFlip(p=0.5),\n",
    "    transforms.Normalize(mean=[0.5] * 3, std=[0.5] * 3)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... create the training and testing datasets ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preloading images: 100%|██████████| 22/22 [01:30<00:00,  4.10s/it]\n",
      "Preloading images:   0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "train_dataset = VirtualStainingDataset(\n",
    "    directory=os.path.join(path, \"train\"),\n",
    "    transform=[trans_bright, trans_fluorescent],\n",
    "    preload=True,\n",
    ")\n",
    "test_dataset = VirtualStainingDataset(\n",
    "    directory=os.path.join(path, \"test\"),\n",
    "    transform=[trans_bright, trans_fluorescent],\n",
    "    preload=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and visualize brightfield and corresponding fluorescence images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "bf_img, fl_img = train_dataset[np.random.randint(0, len(train_dataset))]\n",
    "\n",
    "def denormalize(image):\n",
    "    \"\"\"Denormalize images for visualization.\"\"\"\n",
    "    return (image + 1) / 2\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(bf_img.mean(axis=0), cmap=\"gray\")\n",
    "plt.title(\"Brightfield Image\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(denormalize(fl_img[1, :, :].numpy()), cmap=\"Greens\")\n",
    "plt.title(\"Fluorescence - Green\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(denormalize(fl_img[2, :, :].numpy()), cmap=\"Blues\")\n",
    "plt.title(\"Fluorescence - Blue\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiating the Generator and Discriminator\n",
    "\n",
    "Determine the device to be used in the computations ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... define the generator ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deeplay as dl\n",
    "import torch.nn as nn\n",
    "\n",
    "generator = dl.UNet2d(\n",
    "    in_channels=13,\n",
    "    channels=[32, 64, 128, 256, 512],\n",
    "    out_channels=3,\n",
    ")\n",
    "generator[\"encoder\", ..., \"activation\"] \\\n",
    "    .configure(nn.LeakyReLU, negative_slope=0.2)\n",
    "generator[\"decoder\", ..., \"activation#:-1\"] \\\n",
    "    .configure(nn.LeakyReLU, negative_slope=0.2)\n",
    "generator[\"decoder\", ..., \"activation#-1\"].configure(nn.Tanh)\n",
    "generator[\"decoder\", \"blocks\", :-1].all.normalized(nn.InstanceNorm2d)\n",
    "generator[..., \"blocks\"] \\\n",
    "    .configure(order=[\"layer\", \"normalization\", \"activation\"])\n",
    "generator.build()\n",
    "generator.to(device);\n",
    "\n",
    "print(generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and define the discriminator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = dl.ConvolutionalNeuralNetwork(\n",
    "    in_channels=16,\n",
    "    hidden_channels=[8, 16, 32, 64],\n",
    "    out_channels=1,\n",
    ")\n",
    "discriminator[\"blocks\", ..., \"layer\"] \\\n",
    "    .configure(kernel_size=4, stride=2, padding=1)\n",
    "discriminator[\"blocks\", ..., \"activation#:-1\"] \\\n",
    "    .configure(nn.LeakyReLU, negative_slope=0.2)\n",
    "discriminator[\"blocks\", 1:-1].all.normalized(nn.InstanceNorm2d)\n",
    "discriminator[\"blocks\", ..., \"activation#-1\"].configure(nn.Sigmoid)\n",
    "discriminator[\"blocks\"] \\\n",
    "    .configure(order=[\"layer\", \"normalization\", \"activation\"])\n",
    "discriminator.build()\n",
    "discriminator.to(device);\n",
    "\n",
    "print(discriminator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling the Conditional GAN\n",
    "\n",
    "Define the losses ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.image.lpip import LearnedPerceptualImagePatchSimilarity\n",
    "\n",
    "loss_D = torch.nn.MSELoss()\n",
    "loss_G = torch.nn.L1Loss()\n",
    "loss_LPIPS = LearnedPerceptualImagePatchSimilarity(net_type=\"vgg\").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... define the optimizers ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_optimizer = torch.optim.Adam(generator.parameters(), lr=0.0002,\n",
    "                                       betas=(0.5, 0.999))\n",
    "discriminator_optimizer = torch.optim.Adam(discriminator.parameters(),\n",
    "                                           lr=0.00005, betas=(0.5, 0.999))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and define the data loaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=2,\n",
    "                                           shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=2, \n",
    "                                          shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Conditional GAN\n",
    "\n",
    "Define a function to train the discriminator ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_discriminator(inputs, targets, optimizer, loss_D=loss_D):\n",
    "    \"\"\"Train the discriminator.\"\"\"\n",
    "    generator.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Compute real loss.\n",
    "    output_real = discriminator(torch.cat([inputs, targets], dim=1))\n",
    "    label_real = torch.ones_like(output_real)\n",
    "    loss_real = loss_D(output_real, label_real)\n",
    "\n",
    "    # Compute fake loss.\n",
    "    output_fake = discriminator(torch.cat([inputs, generator(inputs)], dim=1))\n",
    "    label_fake = torch.zeros_like(output_fake)\n",
    "    loss_fake = loss_D(output_fake, label_fake)\n",
    "\n",
    "    loss = (loss_real + loss_fake) / 2\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... define a function to train the generator ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_generator(inputs, targets, optimizer, loss_D=loss_D, loss_G=loss_G,\n",
    "                    loss_LPIPS=loss_LPIPS, L1_Lambda=100, LPIPS_Lambda=10):\n",
    "    \"\"\"Train the generator.\"\"\"\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    generator_output = generator(inputs)\n",
    "    discriminator_output = discriminator(torch.cat([inputs, generator_output], \n",
    "                                                    dim=1))\n",
    "    \n",
    "    label = torch.ones_like(discriminator_output)\n",
    "    loss_GAN = loss_D(discriminator_output, label)\n",
    "    loss_L1 = loss_G(generator_output, targets)\n",
    "    loss_P = loss_LPIPS(generator_output, targets)\n",
    "    \n",
    "    loss = loss_GAN + L1_Lambda * loss_L1 + LPIPS_Lambda * loss_P\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss_GAN, loss_L1, loss_P\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and a function to evaluate the model on the test dataset ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(image, label):\n",
    "    \"\"\"Evaluate model on test data.\"\"\"\n",
    "    generator.eval()\n",
    "    with torch.no_grad():\n",
    "        prediction = generator(image)\n",
    "\n",
    "    fig, ax = plt.subplots(2, 3, figsize=(10, 5))\n",
    "\n",
    "    ax[0, 0].imshow(\n",
    "        denormalize(image[0].permute(1, 2, 0).cpu().numpy().mean(axis=-1)), \n",
    "        cmap=\"gray\"\n",
    "    )\n",
    "    ax[0, 0].axis(\"off\")\n",
    "    ax[0, 0].set_title(\"Input Image\")\n",
    "\n",
    "    ax[0, 1].imshow(\n",
    "        denormalize(prediction[0].permute(1, 2, 0).cpu().numpy())[:, :, 1], \n",
    "        cmap=\"Greens\",\n",
    "    )\n",
    "    ax[0, 1].axis(\"off\")\n",
    "    ax[0, 1].set_title(\"Prediction - Green\")\n",
    "\n",
    "    ax[1, 1].imshow(\n",
    "        denormalize(label[0].permute(1, 2, 0).cpu().numpy())[:, :, 1], \n",
    "        cmap=\"Greens\",\n",
    "    )\n",
    "    ax[1, 1].axis(\"off\")\n",
    "    ax[1, 1].set_title(\"Ground truth - Green\")\n",
    "\n",
    "    ax[0, 2].imshow(\n",
    "        denormalize(prediction[0].permute(1, 2, 0).cpu().numpy())[:, :, 2], \n",
    "        cmap=\"Blues\",\n",
    "    )\n",
    "    ax[0, 2].axis(\"off\")\n",
    "    ax[0, 2].set_title(\"Prediction - Blue\")\n",
    "\n",
    "    ax[1, 2].imshow(\n",
    "        denormalize(label[0].permute(1, 2, 0).cpu().numpy())[:, :, 2], \n",
    "        cmap=\"Blues\",\n",
    "    )\n",
    "    ax[1, 2].axis(\"off\")\n",
    "    ax[1, 2].set_title(\"Ground truth - Blue\")\n",
    "\n",
    "    ax[1, 0].axis(\"off\")  # Leave the [1, 0] subplot empty.\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and implement the training cycle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import timedelta\n",
    "\n",
    "epochs, L1_Lambda, LPIPS_Lambda = 500, 100, 10\n",
    "\n",
    "generator_losses = []\n",
    "discriminator_losses = []\n",
    "reconstruction_losses = []\n",
    "perceptual_losses = []\n",
    "for epoch in range(epochs + 1):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    print(\"\\n\" + f\"Epoch {epoch + 1}/{epochs}\" + \"\\n\" + \"-\" * 10)\n",
    "\n",
    "    gen_loss_epochs = []\n",
    "    disc_loss_epochs = []\n",
    "    recon_loss_epochs = []\n",
    "    percep_loss_epochs = []\n",
    "    for i, (inputs, labels) in enumerate(train_loader, 0):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # 1. Train the discriminator.\n",
    "        disc_loss = train_discriminator(\n",
    "            inputs=inputs, \n",
    "            targets=labels, \n",
    "            optimizer=discriminator_optimizer,\n",
    "        )\n",
    "\n",
    "        # 2. Train the generator.\n",
    "        for _ in range(2):\n",
    "            adv_loss, rec_loss, percep_loss = train_generator(\n",
    "                inputs=inputs, \n",
    "                targets=labels, \n",
    "                optimizer=generator_optimizer, \n",
    "                L1_Lambda=L1_Lambda, \n",
    "                LPIPS_Lambda=LPIPS_Lambda,\n",
    "            )\n",
    "\n",
    "        gen_loss = adv_loss + rec_loss + percep_loss\n",
    "        \n",
    "        if i % 5 == 0:\n",
    "            print(f\"Batch {i+1}/{len(train_loader)} : \"\n",
    "                  + f\"Generator Loss: {gen_loss.item():.4f}, \"\n",
    "                  + f\"Discriminator Loss: {disc_loss.item():.4f}\")\n",
    "        \n",
    "        gen_loss_epochs.append(adv_loss.item())\n",
    "        disc_loss_epochs.append(disc_loss.item())\n",
    "        recon_loss_epochs.append(rec_loss.item())\n",
    "        percep_loss_epochs.append(percep_loss.item())\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    print(\"-\" * 10 + \"\\n\" + f\"Epoch {epoch + 1}/{epochs} : \"\n",
    "          + f\"Generator loss: {np.mean(gen_loss_epochs):.4f}, \"\n",
    "          + f\"Discriminator Loss: {np.mean(disc_loss_epochs):.4f}, \"\n",
    "          + f\"Reconstruction Loss: {np.mean(recon_loss_epochs):.4f}, \"\n",
    "          + f\"Perception loss: {np.mean(percep_loss_epochs):.4f}\"\n",
    "          + \"\\n\" f\"Time taken: {timedelta(seconds=end_time - start_time)}\")\n",
    "\n",
    "    if epoch % 50 == 0:\n",
    "        for i, data, in enumerate(test_loader, 0):\n",
    "            test_inputs, test_labels = data\n",
    "            test_inputs = test_inputs.to(device)\n",
    "            test_labels = test_labels.to(device)\n",
    "            break\n",
    "        evaluate_model(test_inputs, test_labels)\n",
    "    \n",
    "    generator_losses.append(np.mean(gen_loss_epochs))\n",
    "    discriminator_losses.append(np.mean(disc_loss_epochs))\n",
    "    reconstruction_losses.append(np.mean(recon_loss_epochs))\n",
    "    perceptual_losses.append(np.mean(percep_loss_epochs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Trained Conditional GAN\n",
    "\n",
    "Plot the various losses during training ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(generator_losses, \"b-\", label=\"Generator Loss\")\n",
    "plt.plot(discriminator_losses, \"r--\", label=\"Discriminator Loss\")\n",
    "plt.plot(reconstruction_losses, \"g-.\", label=\"Reconstruction Loss\")\n",
    "plt.plot(perceptual_losses, \"m:\", label=\"Perceptual Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and determine the SSIM score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.image import StructuralSimilarityIndexMeasure as SSIM\n",
    "\n",
    "ssim_score = SSIM(data_range=2).to(device)\n",
    "\n",
    "for i, (test_inputs, test_labels) in enumerate(test_loader, 0):\n",
    "    test_inputs, test_labels = test_inputs.to(device), test_labels.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        ssim_green_channel = ssim_score(\n",
    "            generator(test_inputs)[:, 1, :, :].unsqueeze(1),\n",
    "            test_labels[:, 1, :, :].unsqueeze(1),\n",
    "        ).item()\n",
    "        print(f\"SSIM Green Channel: {ssim_green_channel:.4f}\")\n",
    "        \n",
    "        ssim_blue_channel = ssim_score(\n",
    "            generator(test_inputs)[:, 2, :, :].unsqueeze(1),\n",
    "            test_labels[:, 2, :, :].unsqueeze(1),\n",
    "        ).item()\n",
    "        print(f\"SSIM Blue Channel: {ssim_blue_channel:.4f}\")\n",
    "        \n",
    "        ssim_full_image = ssim_score(\n",
    "            generator(test_inputs), \n",
    "            test_labels\n",
    "        ).item()\n",
    "        print(f\"SSIM Full Image: {ssim_full_image:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py_env_dlcc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
