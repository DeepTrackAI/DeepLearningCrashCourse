{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Virtually Straining a Biological Tissue with a Conditional GAN\n",
    "\n",
    "The virtual_staining.ipynb notebook provides you with a complete code example to virtually stain a biological tissue with a conditional GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading the Human Motor Neurons Dataset\n",
    "\n",
    "You’ll use a dataset that was originally published in the article: E. M. Christiansen et al., Cell 173:792-803, 2018. Specifically, you’ll use the human motor neurons dataset that is designated as “Condition A” in the article. This dataset comprises 22 pairs of brightfield and corresponding fluorescent images, with each pair including spatially registered images showcasing two fluorescent channels: Hoechst stain, revealing nuclei with a blue stain, and anti-TuJ1 stain, highlighting neurons in green. Notably, the brightfield images encompass a z-stack of 13 images across different focal planes, offering a comprehensive view of the cellular structures in question.\n",
    "\n",
    "Download the dataset from Google Storage. The dataset is about 10 GB and will take at least a few minutes to download, or longer depending on your internet connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, subprocess\n",
    "\n",
    "url, path = \"gs://in-silico-labeling/paper_data/\", \"virtual_staining_dataset\"\n",
    "if not os.path.exists(path):\n",
    "    for dataset in [\"train\", \"test\"]:\n",
    "        dataset_url = url + dataset + \"/Rubin/scott_1_0\"\n",
    "        dataset_dir = os.path.join(path, dataset)\n",
    "        os.makedirs(dataset_dir, exist_ok=True)\n",
    "        \n",
    "        command = [\"gsutil\", \"-m\", \"cp\", \"-r\", dataset_url, dataset_dir]\n",
    "        subprocess.run(command, check=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Dataset\n",
    "\n",
    "Implement a class containing the dataset ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "class VirtualStainingDataset(Dataset):\n",
    "    \"\"\"Dataset containing the brighfield and fluorescence images.\"\"\"\n",
    "\n",
    "    _cache = {}  # Class variable to cache loaded images.\n",
    "\n",
    "    def __init__(self, directory, transform=None, preload=False):\n",
    "        \"\"\"Initialize dataset.\"\"\"\n",
    "        self.transform, self.preload = transform, preload\n",
    "        self.images = []\n",
    "\n",
    "        self.image_dir = os.path.join(directory, \"scott_1_0\")\n",
    "        pattern = (\"lab-Rubin,condition-scott_1_0,acquisition_date,\"\n",
    "                   \"year-2016,month-2,day-6,\"\n",
    "                   \"well-r0*c0*,\"\n",
    "                   \"depth_computation,\"\n",
    "                   \"value-MAXPROJECT,is_mask-false,kind,value-ORIGINAL.png\")   \n",
    "        self.image_list = glob.glob(os.path.join(self.image_dir, pattern))\n",
    "\n",
    "        self.cache_key = self.image_dir\n",
    "        if self.preload:\n",
    "            if self.cache_key in VirtualStainingDataset._cache:\n",
    "                self.images = VirtualStainingDataset._cache[self.cache_key]\n",
    "            else:\n",
    "                for image_path in tqdm(self.image_list,\n",
    "                                       total=len(self.image_list),\n",
    "                                       desc=\"Preloading images\"):\n",
    "                    self.images.append(self.load_image(image_path))\n",
    "                VirtualStainingDataset._cache[self.cache_key] = self.images\n",
    "\n",
    "    def load_image(self, image_path):\n",
    "        \"\"\"Load input-target image couple.\"\"\"\n",
    "        input_image = []\n",
    "        for i in range(0, 13):\n",
    "            img_path = (image_path\n",
    "                        .replace(\"depth_computation\", f\"z_depth-{i},channel\")\n",
    "                        .replace(\"value-MAXPROJECT\", \"value-BRIGHTFIELD\"))\n",
    "            input_image.append(np.array(Image.open(img_path).convert(\"L\")))\n",
    "        input_image = np.stack(input_image, axis=-1)\n",
    "\n",
    "        target_image = np.array(Image.open(image_path))\n",
    "\n",
    "        return input_image, target_image\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return number of images.\"\"\"\n",
    "        return len(self.image_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Get next input-target image couple.\"\"\"\n",
    "        if self.preload:\n",
    "            input_image, target_image = self.images[idx]\n",
    "        else:\n",
    "            input_image, target_image = self.load_image(self.image_list[idx])\n",
    "\n",
    "        if self.transform:\n",
    "            seed = np.random.randint(1_000_000_000)\n",
    "            \n",
    "            torch.manual_seed(seed)\n",
    "            input_image = self.transform[0](input_image)\n",
    "\n",
    "            torch.manual_seed(seed)\n",
    "            target_image = self.transform[1](target_image)\n",
    "\n",
    "        return input_image, target_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... define the image transformations and normalizations ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms as trans\n",
    "\n",
    "trans_bright = trans.Compose([\n",
    "    trans.ToTensor(), trans.RandomCrop((256, 256)), \n",
    "    trans.RandomHorizontalFlip(p=0.5), trans.RandomVerticalFlip(p=0.5),\n",
    "    trans.Normalize(mean=[0.5] * 13, std=[0.5] * 13),\n",
    "])\n",
    "trans_fluorescent = trans.Compose([\n",
    "    trans.ToTensor(), trans.RandomCrop((256, 256)), \n",
    "    trans.RandomHorizontalFlip(p=0.5), trans.RandomVerticalFlip(p=0.5),\n",
    "    trans.Normalize(mean=[0.5] * 3, std=[0.5] * 3),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... create the training and testing datasets ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = VirtualStainingDataset(\n",
    "    directory=os.path.join(path, \"train\"),\n",
    "    transform=[trans_bright, trans_fluorescent], preload=True,\n",
    ")\n",
    "test_dataset = VirtualStainingDataset(\n",
    "    directory=os.path.join(path, \"test\"),\n",
    "    transform=[trans_bright, trans_fluorescent], preload=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and visualize brightfield and corresponding fluorescence images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "bf_img, fl_img = train_dataset[np.random.randint(0, len(train_dataset))]\n",
    "\n",
    "def denormalize(image):\n",
    "    \"\"\"Denormalize images for visualization.\"\"\"\n",
    "    return (image + 1) / 2\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(bf_img.mean(axis=0), cmap=\"gray\")\n",
    "plt.title(\"Brightfield Image\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(denormalize(fl_img[1, :, :].numpy()), cmap=\"Greens\")\n",
    "plt.title(\"Stained Neurons\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(denormalize(fl_img[2, :, :].numpy()), cmap=\"Blues\")\n",
    "plt.title(\"Stained Nuclei\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiating the Generator and Discriminator\n",
    "\n",
    "Determine the device to be used in the computations ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device():\n",
    "    \"\"\"Select device where to perform computations.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda:0\")\n",
    "    elif torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\")\n",
    "    else:\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "device = get_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... define the generator ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deeplay as dl\n",
    "import torch.nn as nn\n",
    "\n",
    "gen = dl.UNet2d(\n",
    "    in_channels=13, channels=[32, 64, 128, 256, 512], out_channels=3,\n",
    ")\n",
    "gen[\"encoder\", ..., \"activation\"].configure(nn.LeakyReLU, negative_slope=0.2)\n",
    "gen[\"decoder\", ..., \"activation#:-1\"] \\\n",
    "    .configure(nn.LeakyReLU, negative_slope=0.2)\n",
    "gen[\"decoder\", ..., \"activation#-1\"].configure(nn.Tanh)\n",
    "gen[\"decoder\", \"blocks\", :-1].all.normalized(nn.InstanceNorm2d)\n",
    "gen[..., \"blocks\"].configure(order=[\"layer\", \"normalization\", \"activation\"])\n",
    "gen.build()\n",
    "gen.to(device);\n",
    "\n",
    "print(gen)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and define the discriminator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disc = dl.ConvolutionalNeuralNetwork(\n",
    "    in_channels=16, hidden_channels=[8, 16, 32, 64], out_channels=1,\n",
    ")\n",
    "disc[\"blocks\", ..., \"layer\"].configure(kernel_size=4, stride=2, padding=1)\n",
    "disc[\"blocks\", ..., \"activation#:-1\"] \\\n",
    "    .configure(nn.LeakyReLU, negative_slope=0.2)\n",
    "disc[\"blocks\", 1:-1].all.normalized(nn.InstanceNorm2d)\n",
    "disc[\"blocks\", ..., \"activation#-1\"].configure(nn.Sigmoid)\n",
    "disc[\"blocks\"].configure(order=[\"layer\", \"normalization\", \"activation\"])\n",
    "disc.build()\n",
    "disc.to(device);\n",
    "\n",
    "print(disc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling the Conditional GAN\n",
    "\n",
    "Define the losses ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.image.lpip import LearnedPerceptualImagePatchSimilarity\n",
    "\n",
    "loss_disc = torch.nn.MSELoss()\n",
    "loss_gen = torch.nn.L1Loss()\n",
    "loss_LPIPS = LearnedPerceptualImagePatchSimilarity(net_type=\"vgg\").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... define the optimizers ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim_gen = torch.optim.Adam(gen.parameters(), lr=.0002, betas=(.5, .999))\n",
    "optim_disc = torch.optim.Adam(disc.parameters(), lr=.00005, betas=(.5, .999))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and define the data loaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=2,\n",
    "                                           shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=2, \n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Conditional GAN\n",
    "\n",
    "Define a function to train the discriminator ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_disc(inputs, targets, optim, loss_disc=loss_disc):\n",
    "    \"\"\"Train the discriminator.\"\"\"\n",
    "    optim.zero_grad()\n",
    "\n",
    "    # Compute real loss.\n",
    "    output_real = disc(torch.cat([inputs, targets], dim=1))\n",
    "    label_real = torch.ones_like(output_real)\n",
    "    loss_real = loss_disc(output_real, label_real)\n",
    "\n",
    "    # Compute fake loss.\n",
    "    output_fake = disc(torch.cat([inputs, gen(inputs)], dim=1))\n",
    "    label_fake = torch.zeros_like(output_fake)\n",
    "    loss_fake = loss_disc(output_fake, label_fake)\n",
    "\n",
    "    loss = (loss_real + loss_fake) / 2\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... define a function to train the generator ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gen(inputs, targets, optim, loss_disc=loss_disc, loss_gen=loss_gen,\n",
    "              loss_LPIPS=loss_LPIPS, L1_Lambda=100, LPIPS_Lambda=10):\n",
    "    \"\"\"Train the generator.\"\"\"\n",
    "    optim.zero_grad()\n",
    "\n",
    "    gen_output = gen(inputs)\n",
    "    disc_output = disc(torch.cat([inputs, gen_output], dim=1))\n",
    "    \n",
    "    label = torch.ones_like(disc_output)\n",
    "    loss_GAN = loss_disc(disc_output, label)\n",
    "    loss_L1 = loss_gen(gen_output, targets)\n",
    "    loss_P = loss_LPIPS(gen_output, targets)\n",
    "    \n",
    "    loss = loss_GAN + L1_Lambda * loss_L1 + LPIPS_Lambda * loss_P\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "\n",
    "    return loss_GAN, loss_L1, loss_P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and a function to evaluate the model on the test dataset ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(image, label):\n",
    "    \"\"\"Evaluate model on test data.\"\"\"\n",
    "    gen.eval()\n",
    "    with torch.no_grad():\n",
    "        prediction = gen(image)\n",
    "    gen.train()\n",
    "\n",
    "    fig, ax = plt.subplots(2, 3, figsize=(10, 5))\n",
    "\n",
    "    ax[0, 0].imshow(\n",
    "        denormalize(image[0].permute(1, 2, 0).cpu().numpy().mean(axis=-1)), \n",
    "        cmap=\"gray\"\n",
    "    )\n",
    "    ax[0, 0].set_title(\"Input Image\")\n",
    "    ax[0, 0].axis(\"off\")\n",
    "\n",
    "    ax[0, 1].imshow(\n",
    "        denormalize(prediction[0].permute(1, 2, 0).cpu().numpy())[:, :, 1], \n",
    "        cmap=\"Greens\",\n",
    "    )\n",
    "    ax[0, 1].set_title(\"Prediction - Neurons\")\n",
    "    ax[0, 1].axis(\"off\")\n",
    "\n",
    "    ax[1, 1].imshow(\n",
    "        denormalize(label[0].permute(1, 2, 0).cpu().numpy())[:, :, 1], \n",
    "        cmap=\"Greens\",\n",
    "    )\n",
    "    ax[1, 1].set_title(\"Ground truth - Neurons\")\n",
    "    ax[1, 1].axis(\"off\")\n",
    "\n",
    "    ax[0, 2].imshow(\n",
    "        denormalize(prediction[0].permute(1, 2, 0).cpu().numpy())[:, :, 2], \n",
    "        cmap=\"Blues\",\n",
    "    )\n",
    "    ax[0, 2].set_title(\"Prediction - Nuclei\")\n",
    "    ax[0, 2].axis(\"off\")\n",
    "\n",
    "    ax[1, 2].imshow(\n",
    "        denormalize(label[0].permute(1, 2, 0).cpu().numpy())[:, :, 2], \n",
    "        cmap=\"Blues\",\n",
    "    )\n",
    "    ax[1, 2].set_title(\"Ground truth - Nuclei\")\n",
    "    ax[1, 2].axis(\"off\")\n",
    "\n",
    "    ax[1, 0].axis(\"off\")  # Leave the [1, 0] subplot empty.\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and implement the training cycle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import timedelta\n",
    "\n",
    "epochs, L1_Lambda, LPIPS_Lambda = 500, 100, 10\n",
    "\n",
    "gen_losses, disc_losses, recon_losses, percep_losses = [], [], [], []\n",
    "for epoch in range(epochs + 1):\n",
    "    start_time = time.time()\n",
    " \n",
    "    print(\"\\n\" + f\"Epoch {epoch + 1}/{epochs}\" + \"\\n\" + \"-\" * 10)\n",
    "\n",
    "    gen_loss_epochs, disc_loss_epochs, recon_loss_epochs, percep_loss_epochs \\\n",
    "        = [], [], [], []\n",
    "    for i, (inputs, labels) in enumerate(train_loader, 0):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # 1. Train the discriminator.\n",
    "        disc_loss = train_disc(\n",
    "            inputs=inputs, \n",
    "            targets=labels, \n",
    "            optim=optim_disc,\n",
    "        )\n",
    "\n",
    "        # 2. Train the generator.\n",
    "        for _ in range(2):\n",
    "            adv_loss, rec_loss, percep_loss = train_gen(\n",
    "                inputs=inputs, \n",
    "                targets=labels, \n",
    "                optim=optim_gen, \n",
    "                L1_Lambda=L1_Lambda, \n",
    "                LPIPS_Lambda=LPIPS_Lambda,\n",
    "            )\n",
    "\n",
    "        gen_loss = adv_loss + rec_loss + percep_loss\n",
    "\n",
    "        if i % 5 == 0:\n",
    "            print(f\"Batch {i+1}/{len(train_loader)} : \"\n",
    "                  + f\"Generator Loss: {gen_loss.item():.4f}, \"\n",
    "                  + f\"Discriminator Loss: {disc_loss.item():.4f}\")\n",
    "            \n",
    "        gen_loss_epochs.append(adv_loss.item())\n",
    "        disc_loss_epochs.append(disc_loss.item())\n",
    "        recon_loss_epochs.append(rec_loss.item())\n",
    "        percep_loss_epochs.append(percep_loss.item())\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    if epoch % 50 == 0:\n",
    "        for i, data, in enumerate(test_loader, 0):\n",
    "            test_inputs, test_labels = data\n",
    "            test_inputs = test_inputs.to(device)\n",
    "            test_labels = test_labels.to(device)\n",
    "            break\n",
    "        evaluate_model(test_inputs, test_labels)\n",
    "        \n",
    "    print(\"-\" * 10 + \"\\n\" + f\"Epoch {epoch + 1}/{epochs} : \"\n",
    "          + f\"Generator loss: {np.mean(gen_loss_epochs):.4f}, \"\n",
    "          + f\"Discriminator Loss: {np.mean(disc_loss_epochs):.4f}, \"\n",
    "          + f\"Reconstruction Loss: {np.mean(recon_loss_epochs):.4f}, \"\n",
    "          + f\"Perception Loss: {np.mean(percep_loss_epochs):.4f}\"\n",
    "          + \"\\n\" f\"Time taken: {timedelta(seconds=end_time - start_time)}\")\n",
    "    \n",
    "    gen_losses.append(np.mean(gen_loss_epochs))\n",
    "    disc_losses.append(np.mean(disc_loss_epochs))\n",
    "    recon_losses.append(np.mean(recon_loss_epochs))\n",
    "    percep_losses.append(np.mean(percep_loss_epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py_env_dlcc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
