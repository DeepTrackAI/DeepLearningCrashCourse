{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating New MNIST Digits with a GAN\n",
    "\n",
    "This notebook provides you with a complete code example to generate MNIST digits with a GAN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the MNIST Dataset with PyTorch\n",
    "\n",
    "Implement the digit transformations ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5], inplace=True),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... import the MNIST digits ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import MNIST\n",
    "\n",
    "trainset = MNIST(root=\"data\", train=True, transform=transform, download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and plot some of the transformed MNIST digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axs = plt.subplots(1, 8, figsize=(15, 3))\n",
    "for ax in axs.ravel():\n",
    "    img, label = trainset[torch.randint(0, len(trainset), (1,)).squeeze()]\n",
    "    ax.imshow(img.squeeze(), cmap=\"gray\")\n",
    "    ax.set_title(f\"Label: {label}\", fontsize=16)\n",
    "    ax.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Generator and Discriminator\n",
    "\n",
    "Determine the device to be used in the computations ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... instantiating the generator ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deeplay as dl\n",
    "\n",
    "latent_dim = 100\n",
    "\n",
    "generator = dl.DCGANGenerator(\n",
    "    latent_dim=latent_dim,\n",
    "    features_dim=64,\n",
    "    output_channels=1,\n",
    ")\n",
    "generator.build()\n",
    "generator.to(device);\n",
    "\n",
    "print(generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and instantiate the discriminator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = dl.DCGANDiscriminator(\n",
    "    input_channels=1,\n",
    "    features_dim=64,\n",
    ")\n",
    "discriminator.build()\n",
    "discriminator.to(device);\n",
    "\n",
    "print(discriminator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the GAN\n",
    "\n",
    "Define the data loader ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size=128\n",
    "\n",
    "loader = DataLoader(trainset, batch_size=batch_size, shuffle=True, \n",
    "                    num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... define the loss function ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = torch.nn.BCELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... define the optimizers ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_G = torch.optim.Adam(generator.parameters(), lr=0.0002,\n",
    "                               betas=(0.5, 0.999))\n",
    "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=0.0002,\n",
    "                               betas=(0.5, 0.999))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... implement the adversarial training ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import timedelta\n",
    "\n",
    "epochs = 20\n",
    "\n",
    "num_batches = len(loader)\n",
    "gen_losses_avg, disc_losses_avg = [], []\n",
    "fixed_latent_vector = torch.randn(30, latent_dim, 1, 1).to(device)\n",
    "for epoch in range(epochs):\n",
    "    generator.train(), discriminator.train()\n",
    "    \n",
    "    print(\"\\n\" + f\"Epoch {epoch + 1}/{epochs}\" + \"\\n\" + \"-\" * 10)\n",
    "    start_time = time.time()\n",
    "    running_gen_loss, running_disc_loss = 0.0, 0.0\n",
    "    for batch_idx, (real_images, class_labels) in enumerate(loader, start=0):\n",
    "        real_images = real_images.to(device)\n",
    "\n",
    "        noise = torch.randn(batch_size, latent_dim, 1, 1).to(device)\n",
    "        fake_images = generator(noise)\n",
    "\n",
    "        # 1.  Discriminator training: minimize - log(D(x)) - log(1 - D(G(z))).\n",
    "        real_output = discriminator(real_images).reshape(-1)\n",
    "        fake_output = discriminator(fake_images).reshape(-1)\n",
    "\n",
    "        real_loss = loss(real_output, torch.ones_like(real_output))\n",
    "        fake_loss = loss(fake_output, torch.zeros_like(fake_output))\n",
    "\n",
    "        discriminator_loss = (real_loss + fake_loss) / 2\n",
    "\n",
    "        optimizer_D.zero_grad()\n",
    "        discriminator_loss.backward(retain_graph=True)\n",
    "        optimizer_D.step()\n",
    "\n",
    "        # 2.  Generator training: minimize - log(D(G(z))).\n",
    "        fake_output = discriminator(fake_images).reshape(-1)\n",
    "        generator_loss = loss(fake_output, torch.ones_like(fake_output))\n",
    "\n",
    "        optimizer_G.zero_grad()\n",
    "        generator_loss.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f\"Batch {batch_idx + 1}/{num_batches}: \" \n",
    "                  + f\"Generator Loss: {generator_loss.item():.4f}, \" \n",
    "                  + f\"Discriminator Loss: {discriminator_loss.item():.4f}\")\n",
    "\n",
    "        running_gen_loss += generator_loss.item()\n",
    "        running_disc_loss += discriminator_loss.item()\n",
    "\n",
    "    gen_losses_avg.append(running_gen_loss / num_batches)\n",
    "    disc_losses_avg.append(running_disc_loss / num_batches)\n",
    "    end_time = time.time()\n",
    "\n",
    "    print(\"-\" * 10 + \"\\n\"\n",
    "          + f\"Epoch {epoch+1}/{epochs}: \" \n",
    "          + f\"Generator Loss: {gen_losses_avg[-1]:.4f}, \"\n",
    "          + f\"Discriminator Loss: {disc_losses_avg[-1]:.4f}, \"\n",
    "          + f\"Time taken: {timedelta(seconds=end_time - start_time)}\")\n",
    "    \n",
    "    generator.eval(), discriminator.eval()\n",
    "    fake_images = generator(fixed_latent_vector).detach().cpu().numpy()\n",
    "    fig, axs = plt.subplots(3, 10, figsize=(20, 6))\n",
    "    for i, ax in enumerate(axs.ravel()):\n",
    "        ax.imshow(fake_images[i][0], cmap=\"gray\")\n",
    "        ax.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the Training Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "plt.plot(np.arange(len(gen_losses_avg)), gen_losses_avg, \"g--o\",\n",
    "         label=\"Generator Loss\")\n",
    "plt.plot(np.arange(len(disc_losses_avg)), disc_losses_avg, \"r-o\",\n",
    "         label=\"Discriminator Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py_env_dlcc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
