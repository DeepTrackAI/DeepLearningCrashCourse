{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "\\section{Identifying Cell Trajectories with MAGIK}\n",
    "\n",
    "Tracking the movement of cells is a common task in biological image analysis. \n",
    "This process usually involves two steps: \n",
    "first, the cells are detected and segmented in each frame, \n",
    "and then the cells are linked across frames to form trajectories. \n",
    "In previous chapters, you've learned different methods for detecting and segmenting cells. \n",
    "In this project, you'll focus on linking the cell detections across frames to create trajectories \n",
    "from segmented images using MAGIK, as algorithm based on graph neural networks recently published \n",
    "in the scientific article: J. Pineda \\it et al.}, ``??'', Nature Machine Intelligence {\\bf ??}, ???."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\\subsection{Loading the Cell Tracking Data}\n",
    "\n",
    "You'll use the DIC-C2DH-HeLa dataset from the Cell Tracking Challenge, consisting of two videos showing proliferating HeLa cells, which are commonly employed cell line of metastatic cells. \n",
    "\n",
    "You can download and extract this dataset using Listing~\\ref{cd:11:B:download}."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "import os\n",
    "from torchvision.datasets.utils import download_url, _extract_zip\n",
    "\n",
    "dataset_name = \"DIC-C2DH-HeLa\"\n",
    "dataset_path = os.path.join(\".\", \"cell_detection_dataset\")\n",
    "\n",
    "if not os.path.exists(dataset_path):\n",
    "    url = f\"http://data.celltrackingchallenge.net/training-datasets/{dataset_name}.zip\"\n",
    "\n",
    "    download_url(url, \".\") # (1)\n",
    "    _extract_zip(f\"{dataset_name}.zip\", dataset_path, None) # (2)\n",
    "    os.remove(f\"{dataset_name}.zip\") # (3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torchvision.datasets.utils import download_url, _extract_zip\n",
    "\n",
    "dataset_name = \"DIC-C2DH-HeLa\"\n",
    "dataset_path = os.path.join(\".\", \"cell_detection_dataset\")\n",
    "if not os.path.exists(dataset_path):\n",
    "    url = (\"http://data.celltrackingchallenge.net/training-datasets/\"\n",
    "           + f\"{dataset_name}.zip\")\n",
    "    download_url(url, \".\") # (1)\n",
    "    _extract_zip(f\"{dataset_name}.zip\", dataset_path, None) # (2)\n",
    "    os.remove(f\"{dataset_name}.zip\") # (3)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "\\begin{lstlisting}[\n",
    "    label=cd:11:B:download,\n",
    "    caaption=Downloading the dataset\n",
    "]\n",
    "import os\n",
    "from torchvision.datasets.utils import download_url, _extract_zip\n",
    "\n",
    "dataset_name = \"DIC-C2DH-HeLa\"\n",
    "dataset_path = os.path.join(\".\", \"cell_detection_dataset\")\n",
    "if not os.path.exists(dataset_path):\n",
    "    url = (\"http://data.celltrackingchallenge.net/training-datasets/\"\n",
    "           + f\"{dataset_name}.zip\")\n",
    "    (@\\codewingding{1}@)download_url(url, \".\")\n",
    "    (@\\codewingding{2}@)_extract_zip(f\"{dataset_name}.zip\", dataset_path, None)\n",
    "    (@\\codewingding{3}@)os.remove(f\"{dataset_name}.zip\")\n",
    "\\begin{end}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script downloads the dataset \\wingding{1}, unzips it into the \\lstinline{dataset_path} directory \\wingding{2}, and removes the downloaded zip file \\wingding{3}."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "The dataset comprises the following subdirectories:\n",
    "\\begin{lstlisting}\n",
    "DIC-C2DH-HeLa\n",
    "- 01\n",
    "| - t0000.tif\n",
    "| - ...\n",
    "- 01_GT\n",
    "| - SEG\n",
    "| | - man_seg0000.tif\n",
    "| | - ...\n",
    "| - TRA\n",
    "| | - man_track.txt\n",
    "| | - man_track0000.tif\n",
    "| | - ...\n",
    "- 01_ST\n",
    "| - SEG\n",
    "| | - man_seg0000.tif\n",
    "| | - ...\n",
    "- 02\n",
    "| - t0000.tif\n",
    "| - ...\n",
    "- 02_GT\n",
    "| - SEG\n",
    "| | - man_seg0000.tif\n",
    "| | - ...\n",
    "| - TRA\n",
    "| | - man_track.txt\n",
    "| | - man_track0000.tif\n",
    "| | - ...\n",
    "- 02_ST\n",
    "| - SEG\n",
    "| | - man_seg0000.tif\n",
    "| | - ...\n",
    "\\end{lstlisting}\n",
    "The data consists of two sets of image sequences stored in the \\emph{01} and \\emph{02} folders. \n",
    "Each set includes segmentation masks in two quality levels: \n",
    "the gold-standard corpus containing human-origin reference annotations as the gold truth (\\emph{GT}) \n",
    "and the silver-standard corpus containing computer-origin reference annotations as the silver truth (\\emph{ST}). \n",
    "Furthermore, each \\emph{GT} folder has an additional \\emph{TRA} folder \n",
    "containing ground-truth cell trajectories in the form of both text files and images."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "You'll use sequence 02 for training and sequence 01 for testing. \n",
    "In both cases, you'll use the silver-standard segmentation masks to simulate real-world scenarios where the segmentation masks are not perfect."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "train_image_path = os.path.join(dataset_path, \"DIC-C2DH-HeLa\", \"02\")\n",
    "train_segmentation_path = os.path.join(dataset_path, \"DIC-C2DH-HeLa\", \"02_ST\", \"SEG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image_path = os.path.join(dataset_path, \"DIC-C2DH-HeLa\", \"02\")\n",
    "train_seg_path = os.path.join(dataset_path, \"DIC-C2DH-HeLa\", \"02_ST\", \"SEG\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "You can now load the the data using Listing~\\ref{cd:11:B:loading}."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import glob\n",
    "import cv2\n",
    "\n",
    "def load_images(path):\n",
    "    images = []\n",
    "    for file in glob.glob(path + \"/*.tif\"): # (1)\n",
    "        image = cv2.imread(file, cv2.IMREAD_UNCHANGED) # (2)\n",
    "        images.append(image) # (3)\n",
    "\n",
    "    return images\n",
    "\n",
    "train_images = load_images(train_image_path)\n",
    "train_segmentations = load_images(train_segmentation_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2, glob\n",
    "\n",
    "def load_images(path):\n",
    "    \"\"\"Load images.\"\"\"\n",
    "    images = []\n",
    "    for file in glob.glob(path + \"/*.tif\"): # (1)\n",
    "        image = cv2.imread(file, cv2.IMREAD_UNCHANGED) # (2)\n",
    "        images.append(image) # (3)\n",
    "    return images\n",
    "\n",
    "train_images = load_images(train_image_path)\n",
    "train_segs = load_images(train_seg_path)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "\\begin{lstlisting}[\n",
    "    label=cd:11:B:loading,\n",
    "    caption=Loading the images\n",
    "]\n",
    "import cv2, glob\n",
    "\n",
    "def load_images(path):\n",
    "    \"\"\"Load images.\"\"\"\n",
    "    images = []\n",
    "    (@\\codewingding{1}@)for file in glob.glob(path + \"/*.tif\"):\n",
    "        (@\\codewingding{2}@)image = cv2.imread(file, cv2.IMREAD_UNCHANGED)\n",
    "        (@\\codewingding{3}@)images.append(image)\n",
    "    return images\n",
    "\n",
    "train_images = load_images(train_image_path)\n",
    "train_segs = load_images(train_seg_path)\n",
    "\\end{lstlisting}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "This code implements the \\lstinline{load_images} function \n",
    "to iterate over each file in the specified directory path containing a \\emph{.tif} extension \\wingding{1}, \n",
    "read the image using OpenCV's \\lstinline{cv2.imread()} function \n",
    "with the \\lstinline{IMREAD_UNCHANGED} flag indicating that the image should be loaded without any modification or conversion \\wingding{2}, \n",
    "and appends the loaded image to the \\lstinline{images} list \\wingding{3}."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "\\subsection{Visualizing Some Images}\n",
    "\n",
    "You can now visualize some frames from the training image sequence along with their corresponding segmentation masks with Listing~\\ref{cd:11:B:view}."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "number_of_frames = 5 \n",
    "\n",
    "total_frames = len(train_segmentations) \n",
    "\n",
    "plot_interval = total_frames // number_of_frames \n",
    "frames_to_plot = [i for i in range(0, total_frames, plot_interval)]\n",
    "\n",
    "fig, axs = plt.subplots(2, number_of_frames + 1, figsize=(20, 6)) # (5)\n",
    "fig.patch.set_facecolor('white')\n",
    "\n",
    "for i, frame in enumerate(frames_to_plot):\n",
    "    if i == 0:\n",
    "        axs[0, i].set_ylabel(\"Intensity image\", fontsize=16)\n",
    "        axs[1, i].set_ylabel(\"Segmentation\", fontsize=16)\n",
    "\n",
    "    axs[0, i].imshow(train_images[frame], cmap='gray')\n",
    "    axs[0, i].set_title(f\"Frame {frame}\", fontsize=16)\n",
    "    axs[0, i].tick_params(axis='both', which='both', bottom=False, top=False, left=False, right=False, labelleft=False, labelbottom=False)\n",
    "\n",
    "    # Plot segmentation\n",
    "    axs[1, i].imshow(train_segmentations[frame], cmap='tab20b')\n",
    "    axs[1, i].tick_params(axis='both', which='both', bottom=False, top=False, left=False, right=False, labelleft=False, labelbottom=False)\n",
    "\n",
    "plt.subplots_adjust(wspace=0.02, hspace=0.02)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "frames_to_plot = [i for i in range(0, len(train_segs), len(train_segs) // 5)]\n",
    "\n",
    "fig, axs = plt.subplots(2, len(frames_to_plot), figsize=(20, 6))\n",
    "fig.patch.set_facecolor(\"white\")\n",
    "for i, frame in enumerate(frames_to_plot):\n",
    "    axs[0, i].imshow(train_images[frame], cmap=\"gray\")\n",
    "    axs[0, i].set_title(f\"Frame {frame}\", fontsize=16)\n",
    "    axs[0, i].tick_params(axis=\"both\", which=\"both\", bottom=False, top=False, \n",
    "                          left=False, right=False, labelleft=False, \n",
    "                          labelbottom=False)\n",
    "\n",
    "    axs[1, i].imshow(train_segs[frame], cmap=\"tab20b\")\n",
    "    axs[1, i].tick_params(axis=\"both\", which=\"both\", bottom=False, top=False, \n",
    "                          left=False, right=False, labelleft=False, \n",
    "                          labelbottom=False)\n",
    "    \n",
    "    if i == 0:\n",
    "        axs[0, i].set_ylabel(\"Intensity image\", fontsize=16)\n",
    "        axs[1, i].set_ylabel(\"Segmentation\", fontsize=16)    \n",
    "\n",
    "plt.subplots_adjust(wspace=0.02, hspace=0.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "\\begin{lstlisting}[\n",
    "    label=cd:11:B:view,\n",
    "    caption=\n",
    "]\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "(@\\codewingding{1}@)frames_to_plot = [i for i in range(0, len(train_segs), len(train_segs) // 5)]\n",
    "\n",
    "fig, axs = plt.subplots(2, len(frames_to_plot), figsize=(20, 6))\n",
    "fig.patch.set_facecolor(\"white\")\n",
    "for i, frame in enumerate(frames_to_plot):\n",
    "    axs[0, i].imshow(train_images[frame], cmap=\"gray\")\n",
    "    axs[0, i].set_title(f\"Frame {frame}\", fontsize=16)\n",
    "    axs[0, i].tick_params(axis=\"both\", which=\"both\", bottom=False, top=False, \n",
    "                          left=False, right=False, labelleft=False, \n",
    "                          labelbottom=False)\n",
    "\n",
    "    axs[1, i].imshow(train_segs[frame], cmap=\"tab20b\")\n",
    "    axs[1, i].tick_params(axis=\"both\", which=\"both\", bottom=False, top=False, \n",
    "                          left=False, right=False, labelleft=False, \n",
    "                          labelbottom=False)\n",
    "    \n",
    "    if i == 0:\n",
    "        axs[0, i].set_ylabel(\"Intensity image\", fontsize=16)\n",
    "        axs[1, i].set_ylabel(\"Segmentation\", fontsize=16)    \n",
    "\n",
    "plt.subplots_adjust(wspace=0.02, hspace=0.02)\n",
    "plt.show()\n",
    "\\end{lstlisting}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "This code plots six frames distributed evenly over the training image sequence \\wingding{1}. \n",
    "The resulting plot is shown by Figure~\\ref{fig:11:B:frames}.\n",
    "\n",
    "\\begin{figure}[H]\n",
    "    fig_11_B1.png\n",
    "    Cell images and corresponding segmentations\n",
    "    fig:11:B:frames\n",
    "\\end\n",
    "\n",
    "The top rows shows the cell images and the bottom row the corresponding ground-truth segmentations."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "\\subsection{Creating a Graph From Segmented Images}\n",
    "\n",
    "To use MAGIK, you need to model cell motion and interactions as a directed graph, \n",
    "where nodes represent segmented cells and edges connect spatially close cells across frames.\n",
    "\n",
    "You'll implement the \\lstinline{GraphFromSegmentations} class to generate a graph from the segmented video frames, \n",
    "as shown in Listing~\\ref{cd:11:B:GraphFromSegmentations}."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "import numpy as np\n",
    "from skimage import measure\n",
    "\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "\n",
    "class GraphFromSegmentations:\n",
    "    def __init__(self, connectivity_radius, max_frame_distance):\n",
    "        self.connectivity_radius = connectivity_radius # (1)\n",
    "        self.max_frame_distance = max_frame_distance # (2)\n",
    "\n",
    "    def __call__(self, segmentations, relation):\n",
    "        x, node_index_labels, frames = [], [], []\n",
    "        for frame, segmentation in enumerate(segmentations): # (3)\n",
    "            features, index_labels = self.compute_node_features(segmentation) # (4)\n",
    "\n",
    "            x.append(features) # (5)\n",
    "            node_index_labels.append(index_labels) # (6)\n",
    "            frames.append([frame] * len(features)) # (7)\n",
    "\n",
    "        x = np.concatenate(x)\n",
    "        node_index_labels = np.concatenate(node_index_labels)\n",
    "        frames = np.concatenate(frames)\n",
    "\n",
    "        edge_index, edge_attr = self.compute_connectivity(x, frames) # (8)\n",
    "        edge_ground_truth = self.compute_ground_truth( # (9)\n",
    "            node_index_labels, edge_index, relation\n",
    "        )\n",
    "\n",
    "        edge_index = edge_index.T\n",
    "        edge_attr = edge_attr[:, None]\n",
    "        edge_ground_truth = edge_ground_truth[:, None]\n",
    "\n",
    "        graph = Data( # (10)\n",
    "            x=torch.tensor(x, dtype=torch.float),\n",
    "            edge_index=torch.tensor(edge_index, dtype=torch.long),\n",
    "            edge_attr=torch.tensor(edge_attr, dtype=torch.float),\n",
    "            distance=torch.tensor(edge_attr, dtype=torch.float),\n",
    "            frames=torch.tensor(frames, dtype=torch.float),\n",
    "            y=torch.tensor(edge_ground_truth, dtype=torch.float),\n",
    "        )\n",
    "\n",
    "        return graph\n",
    "\n",
    "    def compute_node_features(self, segmentation):\n",
    "        labels = np.unique(segmentation)\n",
    "\n",
    "        x, indices = [], []\n",
    "        for label in labels[1:]:\n",
    "            mask = segmentation == label\n",
    "            props = measure.regionprops(mask.astype(np.int32))[0]\n",
    "\n",
    "            centroids = props.centroid / np.array(segmentation.shape)\n",
    "            eccentricity = props.eccentricity\n",
    "\n",
    "            x.append([*centroids, eccentricity])\n",
    "            indices.append(label)\n",
    "\n",
    "        return x, indices\n",
    "\n",
    "    def compute_connectivity(self, x, frames):\n",
    "        positions = x[:, :2]\n",
    "        distances = np.linalg.norm(positions[:, None] - positions, axis=-1)\n",
    "\n",
    "        frame_diff = (frames[:, None] - frames) * -1\n",
    "\n",
    "        mask = (distances < self.connectivity_radius) & ( \n",
    "            (frame_diff <= self.max_frame_distance) & (frame_diff > 0)\n",
    "        )\n",
    "\n",
    "        edge_index = np.argwhere(mask) \n",
    "        edge_attr = distances[mask] \n",
    "\n",
    "        return edge_index, edge_attr\n",
    "\n",
    "    def compute_ground_truth(self, indices, edge_index, relation):\n",
    "        sender = indices[edge_index[:, 0]] \n",
    "        receiver = indices[edge_index[:, 1]]\n",
    "        self_connections_mask = sender == receiver\n",
    "\n",
    "        relation_indices = relation[:, [-1, 0]] \n",
    "        relation_indices = relation_indices[relation_indices[:, 0] != 0]\n",
    "\n",
    "        relation_mask = np.zeros(len(edge_index), dtype=bool)\n",
    "        for i, (s, r) in enumerate(zip(sender, receiver)):\n",
    "            if np.any((relation_indices == [s, r]).all(1)): \n",
    "                relation_mask[i] = True\n",
    "\n",
    "        ground_truth = self_connections_mask | relation_mask\n",
    "\n",
    "        return ground_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from skimage import measure\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "class GraphFromSegmentations:\n",
    "    \"\"\"Graph representation of the motion of cells.\"\"\"\n",
    "    \n",
    "    def __init__(self, connectivity_radius, max_frame_distance):\n",
    "        \"\"\"Initialize graph.\"\"\"\n",
    "        self.connectivity_radius = connectivity_radius # (1)\n",
    "        self.max_frame_distance = max_frame_distance # (2)\n",
    "\n",
    "    def __call__(self, segmentations, relation):\n",
    "        \"\"\"\"\"\"\n",
    "        x, node_index_labels, frames = [], [], []\n",
    "        for frame, segmentation in enumerate(segmentations): # (3)\n",
    "            features, index_labels = self.compute_node_features(segmentation) # (4)\n",
    "            x.append(features) # (5)\n",
    "            node_index_labels.append(index_labels) # (6)\n",
    "            frames.append([frame] * len(features)) # (7)\n",
    "        x = np.concatenate(x)\n",
    "        node_index_labels = np.concatenate(node_index_labels)\n",
    "        frames = np.concatenate(frames)\n",
    "\n",
    "        edge_index, edge_attr = self.compute_connectivity(x, frames) # (8)\n",
    "        edge_ground_truth = self.compute_ground_truth( # (9)\n",
    "            node_index_labels, edge_index, relation\n",
    "        )\n",
    "\n",
    "        edge_index = edge_index.T\n",
    "        edge_attr = edge_attr[:, None]\n",
    "        edge_ground_truth = edge_ground_truth[:, None]\n",
    "\n",
    "        graph = Data( # (10)\n",
    "            x=torch.tensor(x, dtype=torch.float),\n",
    "            edge_index=torch.tensor(edge_index, dtype=torch.long),\n",
    "            edge_attr=torch.tensor(edge_attr, dtype=torch.float),\n",
    "            distance=torch.tensor(edge_attr, dtype=torch.float),\n",
    "            frames=torch.tensor(frames, dtype=torch.float),\n",
    "            y=torch.tensor(edge_ground_truth, dtype=torch.float),\n",
    "        )\n",
    "\n",
    "        return graph\n",
    "\n",
    "    def compute_node_features(self, segmentation):\n",
    "        \"\"\"Compute node features.\"\"\"\n",
    "        labels = np.unique(segmentation)\n",
    "        \n",
    "        x, indices = [], []\n",
    "        for label in labels[1:]:\n",
    "            mask = segmentation == label\n",
    "            props = measure.regionprops(mask.astype(np.int32))[0]\n",
    "\n",
    "            centroids = props.centroid / np.array(segmentation.shape)\n",
    "            eccentricity = props.eccentricity\n",
    "\n",
    "            x.append([*centroids, eccentricity])\n",
    "            indices.append(label)\n",
    "            \n",
    "        return x, indices\n",
    "\n",
    "    def compute_connectivity(self, x, frames):\n",
    "        \"\"\"Compute connectivity.\"\"\"\n",
    "        positions = x[:, :2]\n",
    "        distances = np.linalg.norm(positions[:, None] - positions, axis=-1)\n",
    "\n",
    "        frame_diff = (frames[:, None] - frames) * -1\n",
    "\n",
    "        mask = (distances < self.connectivity_radius) & ( \n",
    "            (frame_diff <= self.max_frame_distance) & (frame_diff > 0)\n",
    "        )\n",
    "\n",
    "        edge_index = np.argwhere(mask) \n",
    "        edge_attr = distances[mask] \n",
    "\n",
    "        return edge_index, edge_attr\n",
    "\n",
    "    def compute_ground_truth(self, indices, edge_index, relation):\n",
    "        \"\"\"Compute ground truth.\"\"\"\n",
    "        sender = indices[edge_index[:, 0]] \n",
    "        receiver = indices[edge_index[:, 1]]\n",
    "        self_connections_mask = sender == receiver\n",
    "\n",
    "        relation_indices = relation[:, [-1, 0]] \n",
    "        relation_indices = relation_indices[relation_indices[:, 0] != 0]\n",
    "\n",
    "        relation_mask = np.zeros(len(edge_index), dtype=bool)\n",
    "        for i, (s, r) in enumerate(zip(sender, receiver)):\n",
    "            if np.any((relation_indices == [s, r]).all(1)): \n",
    "                relation_mask[i] = True\n",
    "\n",
    "        ground_truth = self_connections_mask | relation_mask\n",
    "\n",
    "        return ground_truth"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "\\begin{lstlisting}[\n",
    "    label=cd:11:B:GraphFromSegmentations,\n",
    "    caption=Class to construct a graph from segmentations\n",
    "]\n",
    "import numpy as np\n",
    "import torch\n",
    "from skimage import measure\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "class GraphFromSegmentations:\n",
    "    \"\"\"Graph representation of the motion of cells.\"\"\"\n",
    "    \n",
    "    def __init__(self, connectivity_radius, max_frame_distance):\n",
    "        \"\"\"Initialize graph.\"\"\"\n",
    "        (@\\codewingding{1}@)self.connectivity_radius = connectivity_radius\n",
    "        (@\\codewingding{2}@)self.max_frame_distance = max_frame_distance\n",
    "\n",
    "    def __call__(self, segmentations, relation):\n",
    "        \"\"\"\"\"\"\n",
    "        x, node_index_labels, frames = [], [], []\n",
    "        (@\\codewingding{3}@)for frame, segmentation in enumerate(segmentations):\n",
    "            (@\\codewingding{4}@)features, index_labels = self.compute_node_features(segmentation)\n",
    "            (@\\codewingding{5}@)x.append(features)\n",
    "            (@\\codewingding{6}@)node_index_labels.append(index_labels)\n",
    "            (@\\codewingding{7}@)frames.append([frame] * len(features))\n",
    "        x = np.concatenate(x)\n",
    "        node_index_labels = np.concatenate(node_index_labels)\n",
    "        frames = np.concatenate(frames)\n",
    "\n",
    "        (@\\codewingding{8}@)edge_index, edge_attr = self.compute_connectivity(x, frames)\n",
    "        (@\\codewingding{9}@)edge_ground_truth = self.compute_ground_truth(\n",
    "            node_index_labels, edge_index, relation\n",
    "        )\n",
    "\n",
    "        edge_index = edge_index.T\n",
    "        edge_attr = edge_attr[:, None]\n",
    "        edge_ground_truth = edge_ground_truth[:, None]\n",
    "\n",
    "        (@\\codewingding{10}@)graph = Data(\n",
    "            x=torch.tensor(x, dtype=torch.float),\n",
    "            edge_index=torch.tensor(edge_index, dtype=torch.long),\n",
    "            edge_attr=torch.tensor(edge_attr, dtype=torch.float),\n",
    "            distance=torch.tensor(edge_attr, dtype=torch.float),\n",
    "            frames=torch.tensor(frames, dtype=torch.float),\n",
    "            y=torch.tensor(edge_ground_truth, dtype=torch.float),\n",
    "        )\n",
    "\n",
    "        return graph\n",
    "\n",
    "    def compute_node_features(self, segmentation):\n",
    "        \"\"\"Compute node features.\"\"\"\n",
    "        labels = np.unique(segmentation)\n",
    "        \n",
    "        x, indices = [], []\n",
    "        for label in labels[1:]:\n",
    "            mask = segmentation == label\n",
    "            props = measure.regionprops(mask.astype(np.int32))[0]\n",
    "\n",
    "            centroids = props.centroid / np.array(segmentation.shape)\n",
    "            eccentricity = props.eccentricity\n",
    "\n",
    "            x.append([*centroids, eccentricity])\n",
    "            indices.append(label)\n",
    "            \n",
    "        return x, indices\n",
    "\n",
    "    def compute_connectivity(self, x, frames):\n",
    "        \"\"\"Compute connectivity.\"\"\"\n",
    "        positions = x[:, :2]\n",
    "        distances = np.linalg.norm(positions[:, None] - positions, axis=-1)\n",
    "\n",
    "        frame_diff = (frames[:, None] - frames) * -1\n",
    "\n",
    "        mask = (distances < self.connectivity_radius) & ( \n",
    "            (frame_diff <= self.max_frame_distance) & (frame_diff > 0)\n",
    "        )\n",
    "\n",
    "        edge_index = np.argwhere(mask) \n",
    "        edge_attr = distances[mask] \n",
    "\n",
    "        return edge_index, edge_attr\n",
    "\n",
    "    def compute_ground_truth(self, indices, edge_index, relation):\n",
    "        \"\"\"Compute ground truth.\"\"\"\n",
    "        sender = indices[edge_index[:, 0]] \n",
    "        receiver = indices[edge_index[:, 1]]\n",
    "        self_connections_mask = sender == receiver\n",
    "\n",
    "        relation_indices = relation[:, [-1, 0]] \n",
    "        relation_indices = relation_indices[relation_indices[:, 0] != 0]\n",
    "\n",
    "        relation_mask = np.zeros(len(edge_index), dtype=bool)\n",
    "        for i, (s, r) in enumerate(zip(sender, receiver)):\n",
    "            if np.any((relation_indices == [s, r]).all(1)): \n",
    "                relation_mask[i] = True\n",
    "\n",
    "        ground_truth = self_connections_mask | relation_mask\n",
    "\n",
    "        return ground_truth\n",
    "\\end{lstlisting}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "The \\lstinline{GraphFromSegmentations} class is initialized with two parameters: \n",
    "\\lstinline{connectivity_radius} \\wingding{1)} and \\lstinline{max_frame_distance} \\wingding{2}. \n",
    "These parameters determine the spatial and temporal thresholds necessary for determining connectivity \n",
    "between nodes within the graph structure.\n",
    "\n",
    "In the \\lstinline{__call__()} method, this class receives two inputs: \n",
    "the segmented video frames (\\lstinline{segmentations}) \n",
    "and the parent-child relationships between cells (\\lstinloine{relation}). \n",
    "This method identifies separate objects in each frame of the segmented video data using their index labels \\wingding{3}. \n",
    "Next, it calculates the relevant features such as normalized centroids and eccentricity \\wingding{4}. \n",
    "**(5)** These features are stored in a set called \\lstinline{x} \\wingding{5}. \n",
    "The algorithm repeats this process for every object in the frame, creating a collection of node features (\\lstinline{x}), \n",
    "index labels (\\lstinline{node_index_labels}) \\wingding{6}, and their corresponding frame numbers (\\lstinline{frames}) \\wingding{7}.\n",
    "\n",
    "Using the extracted node features, this class proceeds to calculate pairwise distances between the positions of the nodes \\wingding{8}. \n",
    "Simultaneously, it computes the temporal difference between frames. \n",
    "Based on the specified thresholds, namely \\lstinline{connectivity_radius} and \\lstinline{max_frame_distance}, \n",
    "it identifies nodes that are both spatially and temporally close. \n",
    "The result is a set of edge indices (\\lstinline{edge_index}) and corresponding distances (\\lstinline{edge_attr}) \n",
    "representing the connectivity between nodes.\n",
    "\n",
    "Finally, the ground-truth edges are computed \\wingding{9}. \n",
    "The generated graph includes a redundant number of edges with respect to the actual associations between cells. \n",
    "MAGIK aims to prune the redundant edges while retaining the true connections. \n",
    "Therefore, the ground truth for each edge is a binary value indicating whether an edge should connect two detections, \n",
    "that is, an edge classification problem. \n",
    "The class defines the ground truth by comparing the node index labels and parent-child relationships. \n",
    "Firstly, it identifies self-connections where sender and receiver nodes have the same node index labels. \n",
    "Next, it explores the cell relationships to find relational connections, such as cell divisions. \n",
    "The ground truths are derived from the combination of self-connections and relational connections. \n",
    "\n",
    "Finally, it constructs a PyTorch Data object using node features, edge indices, attributes, distances, frames, and ground truth \\wingding{10}. \n",
    "This object encapsulates all necessary information for training and testing."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "You can now instantiate the \\lstinline{GraphFromSegmentations} class with a connectivity radius of 0.2 \n",
    "(equivalent to 20% of the image size) and a maximum frame distance of 2 to reconnect cells not detected in consecutive frames, using: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_constructor = GraphFromSegmentations(connectivity_radius=0.2, \n",
    "                                           max_frame_distance=2)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\\begin{lstlisting}\n",
    "graph_constructor = GraphFromSegmentations(connectivity_radius=0.2, \n",
    "                                           max_frame_distance=2)\n",
    "\\end{lstlisting}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "You can then use \\lstinline{graph_constructor} to construct the training graph as shown in Listing~\\ref{cd:11:B:graphs}."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "train_relation_path = os.path.join(dataset_path, \"DIC-C2DH-HeLa\", \"02_GT\", \"TRA\") # (1)\n",
    "train_relation = np.loadtxt(train_relation_path + \"/man_track.txt\", dtype=int)\n",
    "\n",
    "train_graph = graph_constructor(train_segmentations, train_relation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_graph = graph_constructor(\n",
    "    segmentations=train_segs, \n",
    "    relation=np.loadtxt(os.path.join(dataset_path, \"DIC-C2DH-HeLa\", \"02_GT\", \\\n",
    "        \"TRA\", \"man_track.txt\"), dtype=int)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{lstlisting}[\n",
    "    label=cd:11:B:graphs,\n",
    "    caption=Constructing the training graph\n",
    "]\n",
    "train_graph = graph_constructor(\n",
    "    segmentations=train_segs, \n",
    "    train_relation=np.loadtxt(os.path.join(dataset_path, \"DIC-C2DH-HeLa\", \\\n",
    "        \"02_GT\", \"TRA\", \"man_track.txt\"), dtype=int)\n",
    "    )\n",
    "\\end{lstlisting}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "This code constructs the graph using the training segmentations \n",
    "and the parent-child relationships between cells contained in the \\emph{man_track.txt} file."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "\\subsubsection{Exploring the Graph Data Structure}\n",
    "\n",
    "You can now explore the properties of the graph. Start by identifying the number of nodes and edges with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 988\n",
      "Number of edges: 3036\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of nodes:\", len(train_graph.x))\n",
    "print(\"Number of edges:\", len(train_graph.edge_index[0]))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\\begin{lstlisting}\n",
    "print(\"Number of nodes:\", len(train_graph.x))\n",
    "print(\"Number of edges:\", len(train_graph.edge_index[0]))\n",
    "\\end{lstlisting}\n",
    "which prints:\n",
    "\\begin{lstlisting}\n",
    "Number of nodes: 988\n",
    "Number of edges: 3036\n",
    "\\end{lstlisting}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "You can then plot the structure of the graph with Listing~\\ref{cd:11:B:plot}."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "\n",
    "for i, j in train_graph.edge_index.T:\n",
    "    plt.plot(\n",
    "        [train_graph.x[i, 1], train_graph.x[j, 1]],\n",
    "        [train_graph.x[i, 0], train_graph.x[j, 0]],\n",
    "        c=\"black\",\n",
    "        alpha=0.5,\n",
    "    )\n",
    "\n",
    "plt.scatter(\n",
    "    train_graph.x[:, 1],\n",
    "    train_graph.x[:, 0],\n",
    "    c=train_graph.frames,\n",
    "    cmap=\"viridis\",\n",
    "    zorder=10,\n",
    ")\n",
    "# label colorbar\n",
    "cb = plt.colorbar()\n",
    "cb.ax.set_title('Frame', fontsize=14)\n",
    "plt.xlabel(\"x\", fontsize=14)\n",
    "plt.ylabel(\"y\", fontsize=14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "for i, j in train_graph.edge_index.T:\n",
    "    plt.plot([train_graph.x[i, 1], train_graph.x[j, 1]],\n",
    "             [train_graph.x[i, 0], train_graph.x[j, 0]],\n",
    "             c=\"black\", alpha=0.5)\n",
    "plt.scatter(train_graph.x[:, 1], train_graph.x[:, 0], \n",
    "            c=train_graph.frames, cmap=\"viridis\", zorder=10)\n",
    "cb = plt.colorbar()\n",
    "cb.ax.set_title('Frame', fontsize=14)\n",
    "plt.xlabel(\"x\", fontsize=14)\n",
    "plt.ylabel(\"y\", fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "\\begin{lstlisting}[\n",
    "    label=cd:11:B:plot,\n",
    "    caption=Plotting the graph\n",
    "]\n",
    "plt.figure(figsize=(8, 8))\n",
    "for i, j in train_graph.edge_index.T:\n",
    "    plt.plot([train_graph.x[i, 1], train_graph.x[j, 1]],\n",
    "             [train_graph.x[i, 0], train_graph.x[j, 0]],\n",
    "             c=\"black\", alpha=0.5)\n",
    "plt.scatter(train_graph.x[:, 1], train_graph.x[:, 0], \n",
    "            c=train_graph.frames, cmap=\"viridis\", zorder=10)\n",
    "cb = plt.colorbar()\n",
    "cb.ax.set_title('Frame', fontsize=14)\n",
    "plt.xlabel(\"x\", fontsize=14)\n",
    "plt.ylabel(\"y\", fontsize=14)\n",
    "plt.show()\n",
    "\\end{lstlisting}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "This code produces a plot of the graph like Figure~\\ref{fig:11:B:graph}.\n",
    "\n",
    "\\begin{figure}[H]\n",
    "    fig_11_B2.png\n",
    "    Plot of the graph representing the cells and their relations\n",
    "    fig:11:B:graph\n",
    "\\end\n",
    "\n",
    "\n",
    "This scatter plot depicts a graph with nodes represented as dots. \n",
    "The x and y coordinates represent the normalized node centroids. \n",
    "The color of each dot corresponds to the frame number, as shown on the color bar. \n",
    "The black lines on the plot illustrate the edges."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "\\subsection{Building a Dataset for Training}\n",
    "\n",
    "The training dataset consists of a single graph derived from the training video sequence. \n",
    "Although this may initially appear as limited data, it's sufficient to effectively training the MAGIK model. \n",
    "To address the scarcity of data, you can augment the training graph by splitting it into smaller temporal subgraphs. \n",
    "The \\lstinline{CellTracingDataset} class in Listing~\\ref{cd:11:B:CellTracingDataset} implements this augmentation strategy."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class CellTracingDataset(Dataset):\n",
    "    def __init__(self, graph, window_size, dataset_size, transform=None):\n",
    "        self.graph = graph\n",
    "\n",
    "        self.window_size = window_size # (1)\n",
    "        self.dataset_size = dataset_size\n",
    "\n",
    "        frames, edge_index = graph.frames, graph.edge_index\n",
    "        self.pair_frames = torch.stack(\n",
    "            [frames[edge_index[0, :]], frames[edge_index[1, :]]], axis=1\n",
    "        )\n",
    "        self.frames = frames\n",
    "        self.max_frame = frames.max()\n",
    "\n",
    "        self.transform = transform \n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataset_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        frame_idx = np.random.randint(self.window_size, self.max_frame + 1) # (2)\n",
    "\n",
    "        start_frame = frame_idx - self.window_size\n",
    "        node_mask = (self.frames >= start_frame) & (self.frames < frame_idx) # (3)\n",
    "        x = self.graph.x[node_mask] # (4)\n",
    "\n",
    "        edge_mask = (self.pair_frames >= start_frame) & (self.pair_frames < frame_idx) # (5)\n",
    "        edge_mask = edge_mask.all(axis=1) \n",
    "\n",
    "        edge_index = self.graph.edge_index[:, edge_mask] # (6)\n",
    "        edge_index -= edge_index.min() \n",
    "\n",
    "        edge_attr = self.graph.edge_attr[edge_mask] # (7)\n",
    "\n",
    "        # sample ground truth edges\n",
    "        ground_truth_edges = self.graph.y[edge_mask] # (8)\n",
    "\n",
    "        graph = Data( # (9)\n",
    "            x=x,\n",
    "            edge_index=edge_index,\n",
    "            edge_attr=edge_attr,\n",
    "            distance=edge_attr,\n",
    "            y=ground_truth_edges,\n",
    "        )\n",
    "\n",
    "        if self.transform: # (10)\n",
    "            graph = self.transform(graph)\n",
    "\n",
    "        return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CellTracingDataset(Dataset):\n",
    "    \"\"\"Class to prepare the graph dataset.\"\"\"\n",
    "    \n",
    "    def __init__(self, graph, window_size, dataset_size, transform=None):\n",
    "        \"\"\"Initialize the graph dataset.\"\"\"\n",
    "        self.graph = graph\n",
    "\n",
    "        self.window_size = window_size # (1)\n",
    "        self.dataset_size = dataset_size\n",
    "\n",
    "        frames, edge_index = graph.frames, graph.edge_index\n",
    "        self.pair_frames = torch.stack(\n",
    "            [frames[edge_index[0, :]], frames[edge_index[1, :]]], axis=1\n",
    "        )\n",
    "        self.frames = frames\n",
    "        self.max_frame = frames.max()\n",
    "\n",
    "        self.transform = transform \n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Obtain length of dataset.\"\"\"\n",
    "        return self.dataset_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        frame_idx = np.random.randint(self.window_size, self.max_frame + 1) # (2)\n",
    "\n",
    "        start_frame = frame_idx - self.window_size\n",
    "        node_mask = (self.frames >= start_frame) & (self.frames < frame_idx) # (3)\n",
    "        x = self.graph.x[node_mask] # (4)\n",
    "\n",
    "        edge_mask = ((self.pair_frames >= start_frame) \n",
    "                     & (self.pair_frames < frame_idx)) # (5)\n",
    "        edge_mask = edge_mask.all(axis=1) \n",
    "\n",
    "        edge_index = self.graph.edge_index[:, edge_mask] # (6)\n",
    "        edge_index -= edge_index.min() \n",
    "\n",
    "        edge_attr = self.graph.edge_attr[edge_mask] # (7)\n",
    "\n",
    "        # sample ground truth edges\n",
    "        ground_truth_edges = self.graph.y[edge_mask] # (8)\n",
    "\n",
    "        graph = Data( # (9)\n",
    "            x=x,\n",
    "            edge_index=edge_index,\n",
    "            edge_attr=edge_attr,\n",
    "            distance=edge_attr,\n",
    "            y=ground_truth_edges,\n",
    "        )\n",
    "\n",
    "        if self.transform: # (10)\n",
    "            graph = self.transform(graph)\n",
    "\n",
    "        return graph"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "\\begin{lstlisting}[\n",
    "    label=cd:11:B:CellTracingDataset,\n",
    "    caption=Class to prepare the graph dataset.\"\"\"\n",
    "]\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CellTracingDataset(Dataset):\n",
    "    \"\"\"Class to prepare the graph dataset.\"\"\"\n",
    "    \n",
    "    def __init__(self, graph, window_size, dataset_size, transform=None):\n",
    "        \"\"\"Initialize the graph dataset.\"\"\"\n",
    "        self.graph = graph\n",
    "\n",
    "        (@\\codewingding{1}@)self.window_size = window_size\n",
    "        self.dataset_size = dataset_size\n",
    "\n",
    "        frames, edge_index = graph.frames, graph.edge_index\n",
    "        self.pair_frames = torch.stack(\n",
    "            [frames[edge_index[0, :]], frames[edge_index[1, :]]], axis=1\n",
    "        )\n",
    "        self.frames = frames\n",
    "        self.max_frame = frames.max()\n",
    "\n",
    "        self.transform = transform \n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Obtain length of dataset.\"\"\"\n",
    "        return self.dataset_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        (@\\codewingding{2}@)frame_idx = np.random.randint(self.window_size, self.max_frame + 1)\n",
    "\n",
    "        start_frame = frame_idx - self.window_size\n",
    "        (@\\codewingding{3}@)node_mask = (self.frames >= start_frame) & (self.frames < frame_idx)\n",
    "        (@\\codewingding{4}@)x = self.graph.x[node_mask]\n",
    "\n",
    "        (@\\codewingding{5}@)edge_mask = ((self.pair_frames >= start_frame) \n",
    "                     & (self.pair_frames < frame_idx))\n",
    "        edge_mask = edge_mask.all(axis=1) \n",
    "\n",
    "        (@\\codewingding{6}@)edge_index = self.graph.edge_index[:, edge_mask]\n",
    "        edge_index -= edge_index.min() \n",
    "\n",
    "        (@\\codewingding{7}@)edge_attr = self.graph.edge_attr[edge_mask]\n",
    "\n",
    "        # sample ground truth edges\n",
    "        (@\\codewingding{8}@)ground_truth_edges = self.graph.y[edge_mask]\n",
    "\n",
    "        (@\\codewingding{9}@)graph = Data(\n",
    "            x=x,\n",
    "            edge_index=edge_index,\n",
    "            edge_attr=edge_attr,\n",
    "            distance=edge_attr,\n",
    "            y=ground_truth_edges,\n",
    "        )\n",
    "\n",
    "        (@\\codewingding{10}@)if self.transform:\n",
    "            graph = self.transform(graph)\n",
    "\n",
    "        return graph\n",
    "\\end{lstlisting}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class divides the training graph into smaller subgraphs.\n",
    "The \\lstinline{window_size} parameter determines the number of frames in each subgraph \\wingding{1}. \n",
    "The dataset generates subgraphs by randomly sliding a window across the training graph \\wingding{2}. \n",
    "The subgraph is constructed by extracting nodes \\wingding{3}\\wingding{4} \n",
    "and edges \\wingding{5}\\wingding{6}\\wingding{7}\\wingding{8} within the window. \n",
    "The dataset returns the subgraph as a PyTorch data object \\wingding{9}. \n",
    "\n",
    "To further enhance the training dataset, additional augmentations can be applied to the subgraphs. \n",
    "This class provides the flexibility to include custom augmentations by specifying the \\lstinline{transform} parameter \\wingding{10}. \n",
    "For examples, Listing~\\ref{cd:11:B:aug} defines two augmentations."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "\n",
    "class RandomRotation: # (1)\n",
    "    def __call__(self, graph):\n",
    "        graph = graph.clone()\n",
    "        centered_features = graph.x[:, :2] - 0.5\n",
    "\n",
    "        angle = np.random.rand() * 2 * np.math.pi\n",
    "        rotation_matrix = torch.tensor(\n",
    "            [\n",
    "                [math.cos(angle), -math.sin(angle)],\n",
    "                [math.sin(angle), math.cos(angle)],\n",
    "            ]\n",
    "        )\n",
    "        rotated_features = torch.matmul(centered_features, rotation_matrix)\n",
    "\n",
    "        graph.x[:, :2] = rotated_features + 0.5\n",
    "        return graph\n",
    "    \n",
    "class RandomFlip: # (2)\n",
    "    def __call__(self, graph):\n",
    "        graph = graph.clone()\n",
    "        centered_features = graph.x[:, :2] - 0.5\n",
    "\n",
    "        if np.random.randint(2):\n",
    "            centered_features[:, 0] *= -1\n",
    "        \n",
    "        if np.random.randint(2):\n",
    "            centered_features[:, 1] *= -1\n",
    "        \n",
    "        graph.x[:, :2] = centered_features + 0.5\n",
    "        return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, torch\n",
    "\n",
    "class RandomRotation: # (1)\n",
    "    \"\"\"Random rotation.\"\"\"\n",
    "    \n",
    "    def __call__(self, graph):\n",
    "        \"\"\"Call the random rotation.\"\"\"\n",
    "        graph = graph.clone()\n",
    "        centered_features = graph.x[:, :2] - 0.5\n",
    "\n",
    "        angle = np.random.rand() * 2 * np.math.pi\n",
    "        rotation_matrix = torch.tensor(\n",
    "            [\n",
    "                [math.cos(angle), -math.sin(angle)],\n",
    "                [math.sin(angle), math.cos(angle)],\n",
    "            ]\n",
    "        )\n",
    "        rotated_features = torch.matmul(centered_features, rotation_matrix)\n",
    "\n",
    "        graph.x[:, :2] = rotated_features + 0.5\n",
    "        return graph\n",
    "    \n",
    "class RandomFlip: # (2)\n",
    "    \"\"\"Random flip.\"\"\"\n",
    "    \n",
    "    def __call__(self, graph):\n",
    "        \"\"\"Call the random flip.\"\"\"\n",
    "        graph = graph.clone()\n",
    "        centered_features = graph.x[:, :2] - 0.5\n",
    "\n",
    "        if np.random.randint(2):\n",
    "            centered_features[:, 0] *= -1\n",
    "        \n",
    "        if np.random.randint(2):\n",
    "            centered_features[:, 1] *= -1\n",
    "        \n",
    "        graph.x[:, :2] = centered_features + 0.5\n",
    "        return graph"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "\\begin{lstlisting}[\n",
    "    label=cd:11:B:aug,\n",
    "    caption=Classes to define random rotation and random flip\n",
    "]\n",
    "import math, torch\n",
    "\n",
    "(@\\codewingding{1}@)class RandomRotation:\n",
    "    \"\"\"Random rotation.\"\"\"\n",
    "    \n",
    "    def __call__(self, graph):\n",
    "        \"\"\"Call the random rotation.\"\"\"\n",
    "        graph = graph.clone()\n",
    "        centered_features = graph.x[:, :2] - 0.5\n",
    "\n",
    "        angle = np.random.rand() * 2 * np.math.pi\n",
    "        rotation_matrix = torch.tensor(\n",
    "            [\n",
    "                [math.cos(angle), -math.sin(angle)],\n",
    "                [math.sin(angle), math.cos(angle)],\n",
    "            ]\n",
    "        )\n",
    "        rotated_features = torch.matmul(centered_features, rotation_matrix)\n",
    "\n",
    "        graph.x[:, :2] = rotated_features + 0.5\n",
    "        return graph\n",
    "    \n",
    "(@\\codewingding{2}@)class RandomFlip:\n",
    "    \"\"\"Random flip.\"\"\"\n",
    "    \n",
    "    def __call__(self, graph):\n",
    "        \"\"\"Call the random flip.\"\"\"\n",
    "        graph = graph.clone()\n",
    "        centered_features = graph.x[:, :2] - 0.5\n",
    "\n",
    "        if np.random.randint(2):\n",
    "            centered_features[:, 0] *= -1\n",
    "        \n",
    "        if np.random.randint(2):\n",
    "            centered_features[:, 1] *= -1\n",
    "        \n",
    "        graph.x[:, :2] = centered_features + 0.5\n",
    "        return graph\n",
    "\\end{lstlisting}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "This code defines a class to randomly rotates the positional features of the nodes within the subgraph \\wingding{1} \n",
    "and a class to randomly flip the positional features of the nodes along the x-axis or y-axis \\wingding{2}."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "You can finally create the training dataset using Listin~\\ref{cd:11:B:dataset}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "train_dataset = CellTracingDataset(\n",
    "    train_graph,\n",
    "    window_size=5,\n",
    "    dataset_size=512, # (1)\n",
    "    transform=transforms.Compose([RandomRotation(), RandomFlip()]),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{lstlisting}[\n",
    "    label=cd:11:B:dataset,\n",
    "    caption=Creating the training dataset\n",
    "]\n",
    "from torchvision import transforms\n",
    "\n",
    "train_dataset = CellTracingDataset(\n",
    "    train_graph,\n",
    "    window_size=5,\n",
    "    (@\\codewingding{1}@)dataset_size=512,\n",
    "    transform=transforms.Compose([RandomRotation(), RandomFlip()]),\n",
    ")\n",
    "\\end{lstlisting}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "This code creates the trainign dataset with a temporal window size of 5 \\wingding{1} and 512 subgraphs generated from the training graph per epoch \\wingding{2} using the composition of the augmentations defined above \\wingding{3}."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\\subsubsection{Defining the Data Loaders}\n",
    "\n",
    "You can now define the data loaders, which are responsible for feeding the data to the model during training, using Listing~\\ref{cd:11:B:dataloaders}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jesus\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch_geometric\\deprecation.py:22: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "\\begin{lstlisting}[\n",
    "    label=cd:11:B:dataloaders,\n",
    "    caption=Defining the data loaders\n",
    "]\n",
    "from torch_geometric.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "\\end{lstlisting}\n",
    "This code dfefines the dataloaders with a batch size of 8."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\subsection{Training MAGIK}\n",
    "\n",
    "MAGIK is similar to message-passing neural networks that we have seen in previous examples. However, the main difference is that MAGIK implements a local attention mechanism that allows the model to concentrate on specific nodes and edges during message passing. This mechanism comes into play when aggregating messages to a node. Each message's contribution has a weight that depends on the distance between the connected nodes through a function with learnable parameters defining a learnable local receptive field. With this mechanism, MAGIK can focus on relevant distance-based features during message passing, which is crucial for cell tracking tasks.\n",
    "\n",
    "\\subsubsection{Instantiating MAGIK}\n",
    "\n",
    "You can instantiate the MAGIK model using Listing~\\ref{cd:11:B:MAGIK}."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "from deeplay import GraphToEdgeMAGIK\n",
    "import torch.nn as nn\n",
    "\n",
    "model = GraphToEdgeMAGIK([96,] * 4, 1, out_activation=nn.Sigmoid) # (1)\n",
    " \n",
    "model.encoder[0].configure(hidden_features=[32, 64], out_features=96, out_activation=nn.ReLU) # (2)\n",
    "model.encoder[1].configure(hidden_features=[32, 64], out_features=96, out_activation=nn.ReLU) # (3)\n",
    "model.head.configure(hidden_features=[64, 32])\n",
    "\n",
    "print(model) # (4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deeplay as dl\n",
    "import torch.nn as nn\n",
    "\n",
    "model = dl.GraphToEdgeMAGIK([96,] * 4, 1, out_activation=nn.Sigmoid) # (1)\n",
    "model.encoder[0].configure(hidden_features=[32, 64], out_features=96, out_activation=nn.ReLU) # (2)\n",
    "model.encoder[1].configure(hidden_features=[32, 64], out_features=96, out_activation=nn.ReLU) # (3)\n",
    "model.head.configure(hidden_features=[64, 32])\n",
    "\n",
    "print(model) # (4)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\\begin{lstlisting}[\n",
    "    label=cd:11:B:MAGIK,\n",
    "    caption=Defining the MAGIK model\n",
    "]\n",
    "import deeplay as dl\n",
    "import torch.nn as nn\n",
    "\n",
    "(@\\codewingding{1}@)model = dl.GraphToEdgeMAGIK([96,] * 4, 1, out_activation=nn.Sigmoid)\n",
    "(@\\codewingding{2}@)model.encoder[0].configure(hidden_features=[32, 64], out_features=96, out_activation=nn.ReLU)\n",
    "(@\\codewingding{3}@)model.encoder[1].configure(hidden_features=[32, 64], out_features=96, out_activation=nn.ReLU)\n",
    "model.head.configure(hidden_features=[64, 32])\n",
    "\\end{lstlisting}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code instantiates a simplified version of MAGIK, which is a message-passing neural network \\wingding{1}. The model has four layers, and each layer contains 96 hidden features. Along with the message-passing layers, the model also includes a node encoder, an edge encoder, and a classification head. \n",
    "\n",
    "The node and edge encoder are configured with three hidden layers, each with 32, 64, and 96 hidden features. The classification head consists of two hidden layers with 64 and 32 hidden features and a final output layer with a single output and a sigmoid activation function \\wingding{2}\\wingding{3}. You can print a detailed model summary using \\lstinline{print{model}}."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "\\subsubsection{Training MAGIK}\n",
    "\n",
    "You can now use Listing~\\ref{cd:11:B:train} to train the model."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "from deeplay import BinaryClassifier, Adam, Trainer\n",
    "\n",
    "classifier = BinaryClassifier(model=model, optimizer=Adam(lr=1e-3))\n",
    "classifier = classifier.create()\n",
    "\n",
    "trainer = Trainer(max_epochs=10)\n",
    "trainer.fit(classifier, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = dl.BinaryClassifier(model=model, optimizer=dl.Adam(lr=1e-3))\n",
    "classifier = classifier.create()\n",
    "\n",
    "trainer = dl.Trainer(max_epochs=10)\n",
    "trainer.fit(classifier, train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{lstlisting}[\n",
    "    label=cd:11:B:train,\n",
    "    caption=Training the MAGIK model\n",
    "]\n",
    "classifier = dl.BinaryClassifier(model=model, optimizer=dl.Adam(lr=1e-3))\n",
    "classifier = classifier.create()\n",
    "\n",
    "trainer = dl.Trainer(max_epochs=10)\n",
    "trainer.fit(classifier, train_loader)\n",
    "\\end{lstlisting}\n",
    "This compiles the MAGIK model as a binary classifier and then trains it for 10 epochs."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "\\subsection{Evaluating the Performance of the Trained MAGIK Model}\n",
    "\n",
    "Once the model is trained, you can evaluate its performance on the test dataset. \n",
    "Start by loading the test data with Listing~\\ref{cd:11:B:testdata}."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "test_image_path = os.path.join(dataset_path, \"DIC-C2DH-HeLa\", \"01\")\n",
    "test_segmentation_path = os.path.join(dataset_path, \"DIC-C2DH-HeLa\", \"01_ST\", \"SEG\")\n",
    "\n",
    "test_images = load_images(test_image_path)\n",
    "test_segmentations = load_images(test_segmentation_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image_path = os.path.join(dataset_path, \"DIC-C2DH-HeLa\", \"01\")\n",
    "test_seg_path = os.path.join(dataset_path, \"DIC-C2DH-HeLa\", \"01_ST\", \"SEG\")\n",
    "\n",
    "test_images = load_images(test_image_path)\n",
    "test_segs = load_images(test_seg_path)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "\\begin{lstlisting}[\n",
    "    label=cd:11:B:testdata,\n",
    "    caption=Loading the test data\n",
    "]\n",
    "test_image_path = os.path.join(dataset_path, \"DIC-C2DH-HeLa\", \"01\")\n",
    "test_seg_path = os.path.join(dataset_path, \"DIC-C2DH-HeLa\", \"01_ST\", \"SEG\")\n",
    "\n",
    "test_images = load_images(test_image_path)\n",
    "test_segs = load_images(test_seg_path)\n",
    "\\end{lstlisting}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### and the corresponding parent-child relationships:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "test_relation_path = os.path.join(dataset_path, \"DIC-C2DH-HeLa\", \"01_GT\", \"TRA\")\n",
    "test_relation = np.loadtxt(test_relation_path + \"/man_track.txt\", dtype=int)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### We now construct the test graph using `graph_constructor`:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "test_graph = graph_constructor(test_segmentations, test_relation)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "You can now construct the test graph with Listing~\\ref{cd:11:B:testgraph}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_graph = graph_constructor(\n",
    "    segmentations=test_segs, \n",
    "    relation=np.loadtxt(os.path.join(dataset_path, \"DIC-C2DH-HeLa\", \"01_GT\", \\\n",
    "        \"TRA\", \"man_track.txt\"), dtype=int)\n",
    "    )"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "\\begin{lstlisting}[\n",
    "    label=cd:11:B:testgraph,\n",
    "    caption=Constructing the test graph\n",
    "]\n",
    "test_graph = graph_constructor(\n",
    "    segmentations=test_segs, \n",
    "    relation=np.loadtxt(os.path.join(dataset_path, \"DIC-C2DH-HeLa\", \"01_GT\", \\\n",
    "        \"TRA\", \"man_track.txt\"), dtype=int)\n",
    "    )\n",
    "\\end{lstlisting}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "This code uses the parent-child relationships corresponding to the test data."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "\\subsubsection{Calcualting the F1-Score}\n",
    "\n",
    "After having created the test graph, you can assess the model's performance by calculating the F1-score \n",
    "of the predicted and ground-truth edge classification, as shown in Listing~\\ref{cd:11:B:f1}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test F1 score: 0.9927178764388067\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "classifier.eval()\n",
    "pred = classifier(test_graph)\n",
    "predictions = pred.detach().numpy() > 0.5\n",
    "\n",
    "ground_truth = test_graph.y\n",
    "\n",
    "score = f1_score(ground_truth, predictions)\n",
    "\n",
    "print(f\"Test F1 score: {score}\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "\\begin{lstlisting}[\n",
    "    label=cd:11:B:f1,\n",
    "    caption=Assessing the model performance with the F1-score\n",
    "]\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "classifier.eval()\n",
    "pred = classifier(test_graph)\n",
    "predictions = pred.detach().numpy() > 0.5\n",
    "\n",
    "ground_truth = test_graph.y\n",
    "\n",
    "score = f1_score(ground_truth, predictions)\n",
    "\n",
    "print(f\"Test F1 score: {score}\")\n",
    "\\end{lstlisting}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "You can expect an F1-score of approximately 0.99 on the test graph, \n",
    "exhibiting the model's ability to accurately predict cell temporal associations."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "\\subsubsection{Obtaining the Cell Trajectories}\n",
    "\n",
    "MAGIK does not output cell trajectories, but a graph structure that shows the connections between cells across frames. \n",
    "To generate cell trajectories, you need to apply a post-processing algorithm to the predicted graph structure.\n",
    "\n",
    "The \\lstinline{ComputeTrajectories} class in Listing~\\ref{cd:11:B:ComputeTrajectories} \n",
    "implements a simple post-processing algorithm to compute cell trajectories from MAGIK predictions."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "import networkx as nx\n",
    "\n",
    "class compute_trajectories:\n",
    "\n",
    "    def __call__(self, graph, predictions):\n",
    "        pruned_edges = self.prune_edges(graph, predictions)\n",
    "\n",
    "        pruned_graph = nx.Graph()\n",
    "        pruned_graph.add_edges_from(pruned_edges)\n",
    "\n",
    "        trajectories = list(nx.connected_components(pruned_graph))\n",
    "\n",
    "        return trajectories\n",
    "\n",
    "    def prune_edges(self, graph, predictions):\n",
    "        pruned_edges = []\n",
    "\n",
    "        frame_pairs = np.stack(\n",
    "            [graph.frames[graph.edge_index[0]], graph.frames[graph.edge_index[1]]],\n",
    "            axis=1,\n",
    "        )\n",
    "\n",
    "        senders = np.unique(graph.edge_index[0])\n",
    "        for sender in senders: \n",
    "            sender_mask = graph.edge_index[0] == sender # (1)\n",
    "            candidate = predictions[sender_mask] == True # (2)\n",
    "\n",
    "            frame_diff = frame_pairs[sender_mask, 1] - frame_pairs[sender_mask, 0]\n",
    "            candidates_frame_diff = frame_diff[candidate]\n",
    "\n",
    "            if not np.any(candidate):\n",
    "                continue\n",
    "            else:\n",
    "                candidate_min_frame_diff = candidates_frame_diff.min()\n",
    "            \n",
    "            candidate_edge_index = graph.edge_index[:, sender_mask][ # (3)\n",
    "                :, candidate & (frame_diff == candidate_min_frame_diff)\n",
    "            ]\n",
    "            candidate_edge_index = candidate_edge_index.reshape(-1, 2)\n",
    "\n",
    "            if len(candidate_edge_index) == 1: # (4)\n",
    "                pruned_edges.append(tuple(*candidate_edge_index.numpy()))\n",
    "\n",
    "        return pruned_edges\n",
    "\n",
    "post_processor = compute_trajectories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "class ComputeTrajectories:\n",
    "    \"\"\"Computation of trajectories.\"\"\"\n",
    "\n",
    "    def __call__(self, graph, predictions):\n",
    "        \"\"\"Compute trajectories.\"\"\"\n",
    "        pruned_edges = self.prune_edges(graph, predictions)\n",
    "\n",
    "        pruned_graph = nx.Graph()\n",
    "        pruned_graph.add_edges_from(pruned_edges)\n",
    "\n",
    "        trajectories = list(nx.connected_components(pruned_graph))\n",
    "\n",
    "        return trajectories\n",
    "\n",
    "    def prune_edges(self, graph, predictions):\n",
    "        \"\"\"Prune edges.\"\"\"\n",
    "        pruned_edges = []\n",
    "\n",
    "        frame_pairs = np.stack(\n",
    "            [graph.frames[graph.edge_index[0]], graph.frames[graph.edge_index[1]]],\n",
    "            axis=1,\n",
    "        )\n",
    "\n",
    "        senders = np.unique(graph.edge_index[0])\n",
    "        for sender in senders: \n",
    "            sender_mask = graph.edge_index[0] == sender # (1)\n",
    "            candidate = predictions[sender_mask] == True # (2)\n",
    "\n",
    "            frame_diff = frame_pairs[sender_mask, 1] - frame_pairs[sender_mask, 0]\n",
    "            candidates_frame_diff = frame_diff[candidate]\n",
    "\n",
    "            if not np.any(candidate):\n",
    "                continue\n",
    "            else:\n",
    "                candidate_min_frame_diff = candidates_frame_diff.min()\n",
    "            \n",
    "            candidate_edge_index = graph.edge_index[:, sender_mask][ # (3)\n",
    "                :, candidate & (frame_diff == candidate_min_frame_diff)\n",
    "            ]\n",
    "            candidate_edge_index = candidate_edge_index.reshape(-1, 2)\n",
    "\n",
    "            if len(candidate_edge_index) == 1: # (4)\n",
    "                pruned_edges.append(tuple(*candidate_edge_index.numpy()))\n",
    "\n",
    "        return pruned_edges"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "\\begin{lstlisting}[\n",
    "    label=cd:11:B:ComputeTrajectories,\n",
    "    caption=Class to compute trajectories from MAGIK results\n",
    "]\n",
    "import networkx as nx\n",
    "\n",
    "class ComputeTrajectories:\n",
    "    \"\"\"Computation of trajectories.\"\"\"\n",
    "\n",
    "    def __call__(self, graph, predictions):\n",
    "        \"\"\"Compute trajectories.\"\"\"\n",
    "        pruned_edges = self.prune_edges(graph, predictions)\n",
    "\n",
    "        pruned_graph = nx.Graph()\n",
    "        pruned_graph.add_edges_from(pruned_edges)\n",
    "\n",
    "        trajectories = list(nx.connected_components(pruned_graph))\n",
    "\n",
    "        return trajectories\n",
    "\n",
    "    def prune_edges(self, graph, predictions):\n",
    "        \"\"\"Prune edges.\"\"\"\n",
    "        pruned_edges = []\n",
    "\n",
    "        frame_pairs = np.stack(\n",
    "            [graph.frames[graph.edge_index[0]], graph.frames[graph.edge_index[1]]],\n",
    "            axis=1,\n",
    "        )\n",
    "\n",
    "        senders = np.unique(graph.edge_index[0])\n",
    "        for sender in senders: \n",
    "            sender_mask = graph.edge_index[0] == sender # (1)\n",
    "            candidate = predictions[sender_mask] == True # (2)\n",
    "\n",
    "            frame_diff = frame_pairs[sender_mask, 1] - frame_pairs[sender_mask, 0]\n",
    "            candidates_frame_diff = frame_diff[candidate]\n",
    "\n",
    "            if not np.any(candidate):\n",
    "                continue\n",
    "            else:\n",
    "                candidate_min_frame_diff = candidates_frame_diff.min()\n",
    "            \n",
    "            candidate_edge_index = graph.edge_index[:, sender_mask][ # (3)\n",
    "                :, candidate & (frame_diff == candidate_min_frame_diff)\n",
    "            ]\n",
    "            candidate_edge_index = candidate_edge_index.reshape(-1, 2)\n",
    "\n",
    "            if len(candidate_edge_index) == 1: # (4)\n",
    "                pruned_edges.append(tuple(*candidate_edge_index.numpy()))\n",
    "\n",
    "        return pruned_edges\n",
    "\\end{lstlisting}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "The algorithm starts by selecting a node in the first frame (at time $t=0$) \\wingding{1} \n",
    "and then links it to other nodes in the following frames, using only edges labeled as ``linked'' by MAGIK \\wingding{2}. \n",
    "If there are no ``linked'' edges connecting the sender node at time $t$ to any receiver nodes at time $t+1$, \n",
    "the algorithm checks future frames up to a maximum time delay \\wingding{3}. \n",
    "If no ``linked'' edges are found within this timeframe, the trajectory ends.\n",
    "When a sender node has two ``linked'' edges connecting it to two receiver nodes in a later frame, it's identified as a division. \n",
    "In this case, the algorithm creates two new trajectories \\wingding{4}. This process repeats until all ``linked'' edges are dealt with.\n",
    "\n",
    "You can now compute the trajectories with Listing~\\ref{cd:11:B:compute}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_trajectories = ComputeTrajectories()\n",
    "trajectories = compute_trajectories(test_graph, predictions.squeeze())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "\\begin{lstlisting}[\n",
    "    label=cd:11:B:compute,\n",
    "    caption=Computing the trajectories\n",
    "]\n",
    "compute_trajectories = ComputeTrajectories()\n",
    "trajectories = compute_trajectories(test_graph, predictions.squeeze())\n",
    "\\end{lstlisting}\n",
    "This code instatiates the class to compute the trajectiers and it applies to the test graph obtained from MAGIK."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "trajectories = post_processor(test_graph, predictions.squeeze())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "\\subsubsection{Visualizing the Cell Trajectories}\n",
    "\n",
    "Finally, you can visualize the computed cell trajectories on top of the segmented video frames using Listing~\\ref{cd:11:B:view}."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython.display import HTML\n",
    "from skimage import measure\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "list_of_colors = plt.cm.get_cmap(\"tab20b\", len(trajectories))\n",
    "np.random.shuffle(list_of_colors.colors)\n",
    "\n",
    "def update(frame):\n",
    "    ax.clear()\n",
    "    ax.imshow(test_images[frame], cmap=\"gray\")\n",
    "\n",
    "    segmentation = test_segmentations[frame]\n",
    "    labels = np.unique(segmentation)\n",
    "\n",
    "    for label in labels[1:]:\n",
    "        contour = measure.find_contours(segmentation == label, 0.5)[0]\n",
    "        ax.fill(\n",
    "            contour[:, 1],\n",
    "            contour[:, 0],\n",
    "            color=list_of_colors(label),\n",
    "            alpha=0.5,\n",
    "            linewidth=2,\n",
    "        )\n",
    "    ax.text(0, -5, f\"Frame: {frame}\", fontsize=16, color=\"black\")\n",
    "\n",
    "    for idx, t in enumerate(trajectories):\n",
    "        coordinates = test_graph.x[list(t)]\n",
    "        frames = test_graph.frames[list(t)]\n",
    "\n",
    "        coordinates_in_frame = coordinates[frames == frame]\n",
    "\n",
    "        if len(coordinates_in_frame) == 0:\n",
    "            continue\n",
    "\n",
    "        ax.scatter(coordinates_in_frame[:, 1] * 512, coordinates_in_frame[:, 0] * 512)\n",
    "        ax.text(\n",
    "            coordinates_in_frame[0, 1] * 512,\n",
    "            coordinates_in_frame[0, 0] * 512,\n",
    "            str(idx),\n",
    "            fontsize=16,\n",
    "            color=\"white\",\n",
    "        )\n",
    "\n",
    "        coordinates_previous_frames = coordinates[\n",
    "            (frames <= frame) & (frames >= frame - 10)\n",
    "        ]\n",
    "        ax.plot(\n",
    "            coordinates_previous_frames[:, 1] * 512,\n",
    "            coordinates_previous_frames[:, 0] * 512,\n",
    "            color=\"white\",\n",
    "        )\n",
    "\n",
    "        ax.plot(\n",
    "            coordinates_in_frame[max(0, frame - 10) : frame, 1] * 512,\n",
    "            coordinates_in_frame[max(0, frame - 10) : frame, 0] * 512,\n",
    "            color=\"red\",\n",
    "        )\n",
    "\n",
    "    return ax\n",
    "\n",
    "\n",
    "ani = FuncAnimation(fig, update, frames=len(test_segmentations))\n",
    "\n",
    "html_video = HTML(ani.to_html5_video())\n",
    "\n",
    "plt.close()\n",
    "html_video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from skimage import measure\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "list_of_colors = plt.cm.get_cmap(\"tab20b\", len(trajectories))\n",
    "np.random.shuffle(list_of_colors.colors)\n",
    "\n",
    "def update(frame):\n",
    "    \"\"\"Update frame.\"\"\"\n",
    "    ax.clear()\n",
    "    ax.imshow(test_images[frame], cmap=\"gray\")\n",
    "\n",
    "    segmentation = test_segs[frame]\n",
    "    labels = np.unique(segmentation)\n",
    "\n",
    "    for label in labels[1:]:\n",
    "        contour = measure.find_contours(segmentation == label, 0.5)[0]\n",
    "        ax.fill(\n",
    "            contour[:, 1],\n",
    "            contour[:, 0],\n",
    "            color=list_of_colors(label),\n",
    "            alpha=0.5,\n",
    "            linewidth=2,\n",
    "        )\n",
    "    ax.text(0, -5, f\"Frame: {frame}\", fontsize=16, color=\"black\")\n",
    "\n",
    "    for idx, t in enumerate(trajectories):\n",
    "        coordinates = test_graph.x[list(t)]\n",
    "        frames = test_graph.frames[list(t)]\n",
    "\n",
    "        coordinates_in_frame = coordinates[frames == frame]\n",
    "\n",
    "        if len(coordinates_in_frame) == 0:\n",
    "            continue\n",
    "\n",
    "        ax.scatter(coordinates_in_frame[:, 1] * 512, coordinates_in_frame[:, 0] * 512)\n",
    "        ax.text(\n",
    "            coordinates_in_frame[0, 1] * 512,\n",
    "            coordinates_in_frame[0, 0] * 512,\n",
    "            str(idx),\n",
    "            fontsize=16,\n",
    "            color=\"white\",\n",
    "        )\n",
    "\n",
    "        coordinates_previous_frames = coordinates[\n",
    "            (frames <= frame) & (frames >= frame - 10)\n",
    "        ]\n",
    "        ax.plot(\n",
    "            coordinates_previous_frames[:, 1] * 512,\n",
    "            coordinates_previous_frames[:, 0] * 512,\n",
    "            color=\"white\",\n",
    "        )\n",
    "\n",
    "        ax.plot(\n",
    "            coordinates_in_frame[max(0, frame - 10) : frame, 1] * 512,\n",
    "            coordinates_in_frame[max(0, frame - 10) : frame, 0] * 512,\n",
    "            color=\"red\",\n",
    "        )\n",
    "\n",
    "    return ax\n",
    "\n",
    "ani = FuncAnimation(fig, update, frames=len(test_segs))\n",
    "\n",
    "html_video = HTML(ani.to_html5_video())\n",
    "plt.close()\n",
    "html_video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{lstlisting}[\n",
    "    label=cd:11:B:view,\n",
    "    caption=Visualizing the cell trajectories as a video\n",
    "]\n",
    "from IPython.display import HTML\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from skimage import measure\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "list_of_colors = plt.cm.get_cmap(\"tab20b\", len(trajectories))\n",
    "np.random.shuffle(list_of_colors.colors)\n",
    "\n",
    "def update(frame):\n",
    "    \"\"\"Update frame.\"\"\"\n",
    "    ax.clear()\n",
    "    ax.imshow(test_images[frame], cmap=\"gray\")\n",
    "\n",
    "    segmentation = test_segs[frame]\n",
    "    labels = np.unique(segmentation)\n",
    "\n",
    "    for label in labels[1:]:\n",
    "        contour = measure.find_contours(segmentation == label, 0.5)[0]\n",
    "        ax.fill(\n",
    "            contour[:, 1],\n",
    "            contour[:, 0],\n",
    "            color=list_of_colors(label),\n",
    "            alpha=0.5,\n",
    "            linewidth=2,\n",
    "        )\n",
    "    ax.text(0, -5, f\"Frame: {frame}\", fontsize=16, color=\"black\")\n",
    "\n",
    "    for idx, t in enumerate(trajectories):\n",
    "        coordinates = test_graph.x[list(t)]\n",
    "        frames = test_graph.frames[list(t)]\n",
    "\n",
    "        coordinates_in_frame = coordinates[frames == frame]\n",
    "\n",
    "        if len(coordinates_in_frame) == 0:\n",
    "            continue\n",
    "\n",
    "        ax.scatter(coordinates_in_frame[:, 1] * 512, coordinates_in_frame[:, 0] * 512)\n",
    "        ax.text(\n",
    "            coordinates_in_frame[0, 1] * 512,\n",
    "            coordinates_in_frame[0, 0] * 512,\n",
    "            str(idx),\n",
    "            fontsize=16,\n",
    "            color=\"white\",\n",
    "        )\n",
    "\n",
    "        coordinates_previous_frames = coordinates[\n",
    "            (frames <= frame) & (frames >= frame - 10)\n",
    "        ]\n",
    "        ax.plot(\n",
    "            coordinates_previous_frames[:, 1] * 512,\n",
    "            coordinates_previous_frames[:, 0] * 512,\n",
    "            color=\"white\",\n",
    "        )\n",
    "\n",
    "        ax.plot(\n",
    "            coordinates_in_frame[max(0, frame - 10) : frame, 1] * 512,\n",
    "            coordinates_in_frame[max(0, frame - 10) : frame, 0] * 512,\n",
    "            color=\"red\",\n",
    "        )\n",
    "\n",
    "    return ax\n",
    "\n",
    "ani = FuncAnimation(fig, update, frames=len(test_segs))\n",
    "\n",
    "html_video = HTML(ani.to_html5_video())\n",
    "plt.close()\n",
    "html_video\n",
    "\\end{lstlisting}\n",
    "This code produces a video showing the linked cell trajectories. Some frames are shown in Figure~\\ref{fig:11:B:video}.\n",
    "\n",
    "\\begin{figure}[H]\n",
    "    fig_11_B3.png\n",
    "    Frames from the reconstructed videos with the cell trajectories\n",
    "    fig:11:B:video\n",
    "\\end\n",
    "\n",
    "When you watch the video, you should see that the cell trajectories are mostly correctly reconstructed, aslo in the cases where a single cell divides into two.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
