{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Denoising\n",
    "\n",
    "We'll demonstrate the use of a typical encoderâ€“decoder by denoising a very noisy brightfield image of a single particle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulate a Clean and Noisy Particle\n",
    "\n",
    "We use `Deeptrack 2.1` to simulate a spherical particle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deeptrack as dt\n",
    "import numpy as np\n",
    "\n",
    "particle = dt.Sphere(\n",
    "    position=np.array([0.5, 0.5]) * 64,\n",
    "    position_unit=\"pixel\",\n",
    "    # z=0,  ### BM: line can be removed\n",
    "    radius=500 * dt.units.nm,\n",
    "    refractive_index=1.45 + 0.02j,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the optical sysetm to image the particle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brightfield_microscope = dt.Brightfield(\n",
    "    wavelength=500 * dt.units.nm,\n",
    "    NA=1,\n",
    "    resolution=1 * dt.units.um,\n",
    "    magnification=10,\n",
    "    refractive_index_medium=1.33,\n",
    "    output_region=(0, 0, 64, 64),\n",
    "    upsample=2,  ### Carlo: present in original version, seems to make a big difference in generalization\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create the sample by applying the optical system to the particle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "illuminated_sample = brightfield_microscope(particle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a workflow to simulate _clean_ particle images. We also corrupt them with some Poisson noise to have their _noisy_ version. We join the two workflows into a single simulation pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "clean_particle = (  ### Carlo: as in the book chapter\n",
    "    illuminated_sample\n",
    "    >> dt.NormalizeMinMax()\n",
    "    >> dt.MoveAxis(2, 0)\n",
    "    >> dt.pytorch.ToTensor(dtype=torch.float)\n",
    ")\n",
    "\n",
    "# pipeline = (\n",
    "#     (illuminated_sample >> noise & illuminated_sample)\n",
    "#     >> dt.NormalizeMinMax()\n",
    "#     >> dt.MoveAxis(2, 0)\n",
    "#     >> dt.pytorch.ToTensor(dtype=torch.float)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = dt.Poisson(snr=lambda: 2.0 + np.random.rand())  ### Carlo: *1 not needed\n",
    "\n",
    "noisy_particle = (\n",
    "    illuminated_sample\n",
    "    >> noise\n",
    "    >> dt.NormalizeMinMax()\n",
    "    >> dt.MoveAxis(2, 0)\n",
    "    >> dt.pytorch.ToTensor(dtype=torch.float)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = (noisy_particle) & (clean_particle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot an example of the generated clean and noisy particle. \n",
    "\n",
    "For this, we write a simple function to plot the images and add it to `for_denoising.py`:\n",
    "\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_image(title, image):\n",
    "    \"\"\"Plot a grayscale image with a title.\"\"\"\n",
    "    \n",
    "    plt.imshow(image, cmap=\"gray\")\n",
    "    plt.title(title, fontsize=24)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from for_denoising import plot_image\n",
    "\n",
    "for i in range(5):\n",
    "    input, target = pipeline.update().resolve()\n",
    "\n",
    "    plot_image(f\"Input Image {i}\", input.permute(1, 2, 0))\n",
    "    plot_image(f\"Target Image {i}\", target.permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Dataset\n",
    "\n",
    "We define a dataset class that uses the simulation pipeline to generate data. When calling the class, we establish the size of dataset but also introduce the possibility to replace some of the initial data on the fly. In this way we can increase the amount of data that the network sees during the whole training while we keep the dataset size fixed at each epoch, thus establishing a trade off between simulation/training time and generalization ability.\n",
    "\n",
    "For this we'll create a class and add it to `for_denoising.py`:\n",
    "\n",
    "```python\n",
    "class SimulatedDataset(Dataset):\n",
    "    \"\"\"Simulated dataset generating pairs of noisy and clean images.\"\"\"\n",
    "    \n",
    "    def __init__(self, pipeline, buffer_size, replace=0):\n",
    "        \"\"\"Initialize the dataset.\"\"\"\n",
    "        \n",
    "        self.buffer_size = buffer_size\n",
    "        self.pipeline = pipeline\n",
    "        self.replace = replace\n",
    "        self.images = [pipeline.update().resolve() for _ in range(buffer_size)]\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the size of the dataset buffer.\"\"\"\n",
    "        \n",
    "        return self.buffer_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Retrieve a noisy-clean image pair from the dataset.\"\"\"\n",
    "        \n",
    "        if np.random.rand() < self.replace:\n",
    "            self.images[idx] = self.pipeline.update().resolve()\n",
    "            \n",
    "        image_pair = self.images[idx]\n",
    "        noisy_image, clean_image = image_pair[0], image_pair[1]\n",
    "        \n",
    "        return noisy_image, clean_image\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create the datasets and the data loaders. These will be used both for the training and for the testing, as each new image is never seen before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from for_denoising import SimulatedDataset\n",
    "\n",
    "dataset = SimulatedDataset(\n",
    "    pipeline, buffer_size=256, replace=0.1\n",
    ")  ### Carlo: with 0.1, training more stable\n",
    "loader = DataLoader(dataset, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define and Train the Encoder-decoder\n",
    "\n",
    "We define the encoder-decoder architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deeplay as dl\n",
    "\n",
    "# from torch.nn import Sigmoid ### Carlo: not necessary\n",
    "\n",
    "\n",
    "ed = dl.ConvolutionalEncoderDecoder2d(\n",
    "    in_channels=1,\n",
    "    encoder_channels=[16, 16],\n",
    "    decoder_channels=[16, 16],\n",
    "    out_channels=1,\n",
    "    # out_activation=Sigmoid,  ### Carlo: already by default\n",
    ")\n",
    "\n",
    "print(ed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the loss and the optimizer..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import L1Loss\n",
    "\n",
    "regressor_template = dl.Regressor(model=ed, loss=L1Loss(), optimizer=dl.Adam())\n",
    "ed_regressor = regressor_template.create()\n",
    "print(ed_regressor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and start the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ed_trainer = dl.Trainer(max_epochs=100, accelerator=\"auto\")\n",
    "\n",
    "ed_trainer.fit(ed_regressor, loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We visualize the encoder-decoder predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    input, target = pipeline.update().resolve()\n",
    "    predicted = ed_regressor(input.unsqueeze(0)).detach()\n",
    "\n",
    "    plot_image(f\"Input Image {i}\", input[0, :, :])\n",
    "    plot_image(f\"Target Image {i}\", target[0, :, :])\n",
    "    plot_image(f\"Predicted Image {i}\", predicted[0, 0, :, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the Absence of Mode Collapse\n",
    "\n",
    "The training images are very similar. It can be that the encoder-decoder just learn to provide a centered particle, no matter what is the input (mode collapse). Let's check it out by making a prediction out of noise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "blank = brightfield_microscope(particle ^ 0)\n",
    "\n",
    "blank_pipeline = (\n",
    "    blank\n",
    "    >> noise\n",
    "    >> dt.NormalizeMinMax()\n",
    "    >> dt.MoveAxis(2, 0)\n",
    "    >> dt.pytorch.ToTensor(dtype=torch.float)\n",
    ")\n",
    "\n",
    "for i in range(5):\n",
    "    blank_image = blank_pipeline.update().resolve()\n",
    "    blank_predicted = ed_regressor(blank_image.unsqueeze(0)).detach()\n",
    "\n",
    "    plot_image(f\"Input Image {i}\", blank_image[0, :, :])\n",
    "    plot_image(\n",
    "        f\"Predicted Image {i}\", np.square(blank_predicted[0, 0, :, :])\n",
    "    )  ### BM: maybe normalize the visualization so that the images are comparable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Generalization Capabilities\n",
    "\n",
    "Let's see if the encoder-decoder is able to generalize its predictions to particles located off center and with different size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diverse_particle = dt.Sphere(\n",
    "    position=lambda: np.array([0.2, 0.2] + np.random.rand(2) * 0.6) * 64,\n",
    "    # z=0,  ### Carlo: not necessary\n",
    "    position_unit=\"pixel\",\n",
    "    radius=lambda: 500 * dt.units.nm * (1 + np.random.rand()),\n",
    "    ### Carlo: changed for consistency\n",
    "    refractive_index=1.45 + 0.02j,\n",
    ")\n",
    "\n",
    "diverse_illuminated_sample = brightfield_microscope(diverse_particle)\n",
    "\n",
    "# ### Carlo: changed for consistency\n",
    "# diverse_pipeline = (\n",
    "#     (diverse_illuminated_sample >> noise & diverse_illuminated_sample)\n",
    "#     >> dt.NormalizeMinMax()\n",
    "#     >> dt.MoveAxis(2, 0)\n",
    "#     >> dt.pytorch.ToTensor(dtype=torch.float)\n",
    "# )\n",
    "\n",
    "### Carlo: changed back as in he book\n",
    "diverse_clean_particle = (\n",
    "    diverse_illuminated_sample\n",
    "    >> dt.NormalizeMinMax()\n",
    "    >> dt.MoveAxis(2, 0)\n",
    "    >> dt.pytorch.ToTensor(dtype=torch.float)\n",
    ")\n",
    "\n",
    "diverse_noisy_particle = (\n",
    "    diverse_illuminated_sample\n",
    "    >> noise\n",
    "    >> dt.NormalizeMinMax()\n",
    "    >> dt.MoveAxis(2, 0)\n",
    "    >> dt.pytorch.ToTensor(dtype=torch.float)\n",
    ")\n",
    "\n",
    "diverse_pipeline = (diverse_noisy_particle) & (diverse_clean_particle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We check the predictions on this more challenging test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    diverse_input, diverse_target = diverse_pipeline.update().resolve()\n",
    "    diverse_predicted = ed_regressor(diverse_input.unsqueeze(0)).detach()\n",
    "\n",
    "    plot_image(f\"Input Image {i}\", diverse_input[0, :, :])\n",
    "    plot_image(f\"Target Image {i}\", diverse_target[0, :, :])\n",
    "    plot_image(f\"Predicted Image {i}\", diverse_predicted[0, 0, :, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can train on the new dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diverse_dataset = SimulatedDataset(diverse_pipeline, buffer_size=256, replace=0.1)\n",
    "diverse_loader = DataLoader(diverse_dataset, batch_size=8, shuffle=True)\n",
    "diverse_ed_regressor = regressor_template.create()\n",
    "diverse_ed_trainer = dl.Trainer(max_epochs=100, accelerator=\"auto\")\n",
    "diverse_ed_trainer.fit(diverse_ed_regressor, diverse_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    diverse_input, diverse_target = diverse_pipeline.update().resolve()\n",
    "    diverse_predicted = diverse_ed_regressor(diverse_input.unsqueeze(0)).detach()\n",
    "\n",
    "    plot_image(f\"Input Image {i}\", diverse_input[0, :, :])\n",
    "    plot_image(f\"Target Image {i}\", diverse_target[0, :, :])\n",
    "    plot_image(f\"Predicted Image {i}\", diverse_predicted[0, 0, :, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving the predictions\n",
    "We can use a model with a larger number of activations to improve the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "better_ed = dl.ConvolutionalEncoderDecoder2d(\n",
    "    in_channels=1,\n",
    "    encoder_channels=[32, 64, 128],\n",
    "    decoder_channels=[128, 64, 32],\n",
    "    out_channels=1,\n",
    "    # out_activation=Sigmoid, ### Carlo: already in the default\n",
    ")\n",
    "\n",
    "print(better_ed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "better_regressor_template = dl.Regressor(\n",
    "    model=better_ed,\n",
    "    loss=L1Loss(),\n",
    "    optimizer=dl.Adam(),\n",
    ")\n",
    "better_ed_regressor = better_regressor_template.create()\n",
    "better_ed_trainer = dl.Trainer(\n",
    "    max_epochs=150, accelerator=\"auto\"\n",
    ")  ### BM: 1000 epochs is a bit overkill. doesn't seem to improve much after 200 epochs\n",
    "better_ed_trainer.fit(better_ed_regressor, diverse_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    diverse_input, diverse_target = diverse_pipeline.update().resolve()\n",
    "    diverse_predicted = better_ed_regressor(diverse_input.unsqueeze(0)).detach()\n",
    "\n",
    "    plot_image(f\"Input Image {i}\", diverse_input[0, :, :])\n",
    "    plot_image(f\"Target Image {i}\", diverse_target[0, :, :])\n",
    "    plot_image(f\"Predicted Image {i}\", diverse_predicted[0, 0, :, :])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dt_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
