{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Generation with Variational Autoencoders\n",
    "\n",
    "We'll build a variational autoencoder (VAE) to generate images of handwritten digits inspired by the MNIST dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The MNIST dataset\n",
    "\n",
    "The MNIST dataset consists of grayscale images of hand-written digits from 0 to 9. Each image is 28 pixels by 28 pixels. There're 60,000 training images and 10,000 test images.\n",
    "\n",
    "We've organized these images in two folders named `train` and `test` in the GitHub repository https://github.com/DeepTrackAI/MNIST_dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.path.exists(\"MNIST_dataset\"):\n",
    "    os.system(\"git clone https://github.com/DeepTrackAI/MNIST_dataset\")\n",
    "\n",
    "dir = os.path.join(\"MNIST_dataset\", \"mnist\")                                 # BM: Might want to rename from dir. root_dir? dir is a built-in function in python and it's not good practice to overwrite them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deeptrack as dt\n",
    "\n",
    "train_files = dt.sources.ImageFolder(root=os.path.join(dir, \"train\"))\n",
    "test_files = dt.sources.ImageFolder(root=os.path.join(dir, \"test\"))\n",
    "files = dt.sources.Join(train_files, test_files)\n",
    "\n",
    "print(f\"{len(train_files)} training images\")\n",
    "print(f\"{len(test_files)} test images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the data using `Deeptrack2.0`. We normalize the images in the range `[0, 1]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "image_pipeline = (\n",
    "    dt.LoadImage(files.path)\n",
    "    >> dt.NormalizeMinMax()\n",
    "    >> dt.MoveAxis(2, 0)\n",
    "    >> dt.pytorch.ToTensor(dtype=torch.float)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will randomly choose and visualize some of the images in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "fig, axs = plt.subplots(3, 10, figsize=((10, 3)))\n",
    "for ax, train_file in zip(axs.ravel(), np.random.choice(train_files, axs.size)):\n",
    "    image = image_pipeline(train_file)\n",
    "    ax.imshow(image.squeeze(), cmap=\"gray\")\n",
    "    ax.set_axis_off()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational autoencoder\n",
    "We define the autoencoder architecture with a bidimensional latent space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deeplay as dl\n",
    "\n",
    "vae = dl.VariationalAutoEncoder(latent_dim=2).create()\n",
    "print(vae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the datasets, the dataloader and the trainer. We train the autoencder for `100` epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataset = dt.pytorch.Dataset(image_pipeline & image_pipeline, \n",
    "                                   inputs=train_files)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "vae_trainer = dl.Trainer(max_epochs=100, accelerator=\"auto\")\n",
    "vae_trainer.fit(vae, train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Generation\n",
    "We generate images by sampling a continuous distribution of latent representations and reconstructing the images using the trained VAEâ€™s decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions.normal import Normal\n",
    "\n",
    "img_num, img_size = 21, 28\n",
    "\n",
    "z0_grid = Normal(0, 1).icdf(torch.linspace(0.001, 0.999, img_num))\n",
    "z1_grid = Normal(0, 1).icdf(torch.linspace(0.001, 0.999, img_num))\n",
    "\n",
    "image = np.zeros((img_num * img_size, img_num * img_size))\n",
    "\n",
    "for i0, z0 in enumerate(z0_grid):\n",
    "    for i1, z1 in enumerate(z1_grid):\n",
    "        z = torch.stack((z0, z1)).unsqueeze(0)\n",
    "        generated_image = vae.decode(z).clone().detach()\n",
    "        image[\n",
    "            i1 * img_size : (i1 + 1) * img_size,\n",
    "            i0 * img_size : (i0 + 1) * img_size,\n",
    "        ] = generated_image.numpy().squeeze()\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(image, cmap=\"gray\")\n",
    "plt.xticks(np.arange(0.5 * img_size, (0.5 + img_num) * img_size, img_size), \n",
    "           np.round(z0_grid.numpy(), 1))\n",
    "plt.yticks(np.arange(0.5 * img_size, (0.5 + img_num) * img_size, img_size), \n",
    "           np.round(z1_grid.numpy(), 1))\n",
    "plt.xlabel(\"z0\", fontsize=20)\n",
    "plt.ylabel(\"z1\", fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering in the latent space\n",
    "We will use the VAE as a clustering algorithm, i.e., to cluster the input images into different classes with respect to the latent space encoding.\n",
    "\n",
    "We define a pipeline to get images and labels of the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_pipeline = dt.Value(files.label_name[0]) >> int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = dt.pytorch.Dataset(image_pipeline & label_pipeline, inputs=test_files)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We encode the images of the test dataset into latent space representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_list, test_labels = [], []\n",
    "for image, label in test_loader:\n",
    "    z, _ = vae.encode(image)\n",
    "    z_list.append(z)\n",
    "    test_labels.append(label)\n",
    "z_tensor = torch.cat(z_list, dim=0).detach().numpy()\n",
    "test_labels = torch.cat(test_labels, dim=0).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot the latent-space representation, color coded according to the image label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "plt.scatter(z_tensor[:, 0], z_tensor[:, 1], s=3, c=test_labels, cmap=\"tab10\")\n",
    "plt.xlabel(\"z_tensor[:, 0]\")\n",
    "plt.ylabel(\"z_tensor[:, 1]\")\n",
    "plt.colorbar()\n",
    "plt.axis('equal')\n",
    "plt.gca().add_patch(Rectangle((-3.1, -3.1), 6.2, 6.2, fc=\"none\", ec=\"k\", lw=1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dt_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
