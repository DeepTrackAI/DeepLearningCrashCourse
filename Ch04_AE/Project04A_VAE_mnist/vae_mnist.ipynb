{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Generation with Variational Autoencoders\n",
    "\n",
    "We'll build a variational autoencoder (VAE) to generate images of handwritten digits inspired by the MNIST dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The MNIST dataset\n",
    "\n",
    "The MNIST dataset consists of grayscale images of hand-written digits from 0 to 9. Each image is 28 pixels by 28 pixels. There're 60,000 training images and 10,000 test images.\n",
    "\n",
    "We've organized these images in two folders named `train` and `test` in the GitHub repository https://github.com/DeepTrackAI/MNIST_dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.path.exists(\"MNIST_dataset\"):\n",
    "    os.system(\"git clone https://github.com/DeepTrackAI/MNIST_dataset\")\n",
    "\n",
    "dir = os.path.join(\"MNIST_dataset\", \"mnist\", \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{len(os.listdir(dir))} training images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the data using `Deeptrack2.0`. We will only use `6000` images for training and `6000` for test. We normalize the images in the range `[0, 1]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deeptrack as dt\n",
    "import torch\n",
    "\n",
    "dt.config.disable_image_wrapper()                                               ### Why this line?\n",
    "\n",
    "paths = dt.sources.ImageFolder(root=dir)\n",
    "train_paths, test_paths, _ = dt.sources.random_split(paths, [0.1, 0.1, 0.8])\n",
    "\n",
    "sources = dt.sources.Sources(train_paths, test_paths)\n",
    "\n",
    "pipeline = (\n",
    "    dt.LoadImage(sources.path)\n",
    "    >> dt.NormalizeMinMax()\n",
    "    >> dt.MoveAxis(2, 0)\n",
    "    >> dt.pytorch.ToTensor(dtype=torch.float)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will randomly choose and visualize some of the images in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from numpy import random, squeeze\n",
    "\n",
    "fig, axs = plt.subplots(3, 10, figsize=((10, 3)))\n",
    "for ax, path in zip(axs.ravel(), random.choice(train_paths, axs.size)):\n",
    "    image = pipeline(path)\n",
    "    ax.imshow(squeeze(image), cmap=\"gray\")\n",
    "    ax.set_axis_off()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational autoencoder\n",
    "We define the autoencoder architecture with a bidimensional latent space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deeplay as dl\n",
    "import torchmetrics as tm\n",
    "\n",
    "vae = dl.VariationalAutoEncoder(latent_dim=2).create()\n",
    "\n",
    "print(vae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the datasets, the dataloader and the trainer. We train the autoencder for `100` epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataset = dt.pytorch.Dataset(pipeline & pipeline, inputs=train_paths)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "vae_trainer = dl.Trainer(max_epochs=100, accelerator=\"auto\")\n",
    "vae_trainer.fit(vae, train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Generation\n",
    "We generate images by sampling a continuous distribution of latent representations and reconstructing the images using the trained VAEâ€™s decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions.normal import Normal\n",
    "import numpy as np\n",
    "\n",
    "NUM_OF_IMAGES, IMG_SIZE = 20, 28\n",
    "grid_x = Normal(0, 1).icdf(torch.linspace(0.001, 0.999, NUM_OF_IMAGES))\n",
    "grid_y = Normal(0, 1).icdf(torch.linspace(0.001, 0.999, NUM_OF_IMAGES))\n",
    "\n",
    "image = np.zeros((IMG_SIZE * NUM_OF_IMAGES,) * 2)\n",
    "for i, yi in enumerate(grid_x):\n",
    "    for j, xi in enumerate(grid_y):\n",
    "        z = torch.stack((xi, yi)).unsqueeze(0)\n",
    "        gimg = vae.decode(z).clone().detach()\n",
    "        image[\n",
    "            i * IMG_SIZE : (i + 1) * IMG_SIZE,\n",
    "            j * IMG_SIZE : (j + 1) * IMG_SIZE,\n",
    "        ] = gimg.numpy().squeeze()\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(image, cmap=\"gray\")\n",
    "start = IMG_SIZE // 2\n",
    "end = start + NUM_OF_IMAGES * IMG_SIZE\n",
    "pixel_range = np.arange(start, end, IMG_SIZE)\n",
    "sample_range_x = np.round(grid_x.numpy(), 1)\n",
    "sample_range_y = np.round(grid_y.numpy(), 1)\n",
    "plt.xticks(pixel_range, sample_range_x)\n",
    "plt.yticks(pixel_range, sample_range_y)\n",
    "plt.xlabel(\"z_0\")\n",
    "plt.ylabel(\"z_1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering in the latent space\n",
    "We will use the VAE as a clustering algorithm, i.e., to cluster the input images into different classes with respect to the latent space encoding.\n",
    "\n",
    "We define a pipeline to get images and labels of the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label(label_name):\n",
    "    \"\"\"Get image label.\"\"\"\n",
    "    return int(label_name[0])\n",
    "\n",
    "label = (\n",
    "    dt.Value(get_label, label_name=sources.label_name)\n",
    "    >> dt.Unsqueeze(0)\n",
    "    >> dt.pytorch.ToTensor(dtype=torch.float)\n",
    ")\n",
    "\n",
    "test_dataset = dt.pytorch.Dataset(pipeline & label, inputs=test_paths)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We encode the images of the test dataset into latent space representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu, test_labels = [], []\n",
    "for x, y in test_loader:\n",
    "    m, _ = vae.encode(x)\n",
    "    mu.append(m)\n",
    "    test_labels.append(y)\n",
    "mu = torch.cat(mu, dim=0).detach().numpy()\n",
    "test_labels = torch.cat(test_labels, dim=0).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot the latent-space representation, color coded according to the image label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "plt.scatter(mu[:, 0], mu[:, 1], c=test_labels, cmap=\"tab10\")\n",
    "plt.xlabel(\"mu[0]\"), plt.ylabel(\"mu[1]\"), plt.colorbar(), plt.axis('equal')\n",
    "plt.gca().add_patch(Rectangle((-3.1, -3.1), 6.2, 6.2, facecolor=\"none\", ec=\"k\", lw=2))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dt_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
