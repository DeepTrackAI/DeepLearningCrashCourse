{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaly detection\n",
    "\n",
    "We'll demonstrate how an autoencoder can be used to determine anomalies in electrocardiograms.\n",
    "\n",
    "_Disclaimer: This application is built for demonstration purposes only. It is not intended for use in any medical setting._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The ECG dataset\n",
    "\n",
    "I will use the ECG dataset, which contains 4998 examples of electrocardiogram (ECG) signals. The dataset is available for download as a csv file either at http://www.timeseriesclassification.com/description.php?Dataset=ECG5000 or via the Google cloud storage http://storage.googleapis.com/download.tensorflow.org/data/ecg.csv. \n",
    "I will use the latter direction to get the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  \n",
    "\n",
    "dataframe = pd.read_csv(\"ecg.csv\", header=None)\n",
    "raw_data = dataframe.values\n",
    "\n",
    "print(f\"Size = {raw_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each ECG trace has 140 points, plus one last datapoint that represents the label ('0' for anomalous and '1' for normal).\n",
    "I will separate the labels from the actual traces. I will cut out the beginning and the end of the traces so that they can be used with the autoencoder architecture that I will define later. \n",
    "Last, I will randomly split the dataset into training (70%) and test (30%). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "ecgs = raw_data[:, 2:-11]\n",
    "labels = raw_data[:, -1].astype(bool)\n",
    "\n",
    "train_ecgs, test_ecgs, train_labels, test_labels = train_test_split(\n",
    "    ecgs, labels, test_size=0.3,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the ECG dataset for anomaly detection purposes, I first need to select only the _normal_ traces and preprocess them. I will rescale them from the original range by subtracting the value corresponding to the minimum and dividing them by the range between the maximum and the minimum. In this way, the data points will be in the range $[0,1]$. Using global values to normalize the data helps to preserve relative information between traces. I thus define a pipeline that performs these procedures on randomly picked traces.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "normal_train_ecgs = train_ecgs[train_labels]\n",
    "normal_train_labels = train_labels[train_labels]\n",
    "\n",
    "min_normal_ecgs = np.min(normal_train_ecgs)\n",
    "max_normal_ecgs = np.max(normal_train_ecgs)\n",
    "\n",
    "\n",
    "normal_train_ecgs = ((normal_train_ecgs - min_normal_ecgs) \n",
    "                     / (max_normal_ecgs - min_normal_ecgs))\n",
    "test_ecgs = ((test_ecgs - min_normal_ecgs) \n",
    "             / (max_normal_ecgs - min_normal_ecgs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deeptrack as dt\n",
    "import torch\n",
    "\n",
    "def pick_random_ecg(ecgs):\n",
    "    \"\"\"Pick a random ECG.\"\"\"\n",
    "    return ecgs[np.random.randint(0, ecgs.shape[0])]\n",
    "\n",
    "train_pipeline = (\n",
    "    dt.Value(normal_train_ecgs)\n",
    "    >> pick_random_ecg\n",
    "    >> dt.Unsqueeze(0)\n",
    "    >> dt.pytorch.ToTensor(dtype=torch.float)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will visualize some of the traces of the dataset after preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "fig, axs = plt.subplots(1, 4, figsize=((10, 3)))\n",
    "fig.suptitle(\"Preprocessed normal ECG traces\", fontsize=15)\n",
    "for ax in axs.ravel():\n",
    "    train_pipeline.update()\n",
    "    ecg = train_pipeline()\n",
    "    ax.plot(ecg.squeeze())\n",
    "    ax.set_xlabel(\"timestep\")\n",
    "    ax.set_ylim([0, 1])\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoder\n",
    "I will train a convolutional autoencoder to reconstruct only normal ECG traces by minimizing the reconstruction error. \n",
    "The autoencoder's latent space is ideally kept at a low dimensionality to focus on capturing the most salient features while suppressing less relevant or noisy information.\n",
    "\n",
    "I implement a simple convolutional autoencoder in 1D using as a backbone the `ConvolutionalEncoderDecoder2d` of `deeplay`.\n",
    "<!-- A similar exercise, using a dense autoencoder can be found at \\href{https://www.tensorflow.org/tutorials/generative/autoencoder}. -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deeplay as dl\n",
    "from torch.nn import Identity\n",
    "import torch.nn as nn\n",
    "\n",
    "autoencoder = dl.ConvolutionalEncoderDecoder2d(\n",
    "    in_channels=1,\n",
    "    encoder_channels=[8, 8],\n",
    "    decoder_channels=[8, 8],\n",
    "    out_channels=1,\n",
    "    out_activation=Identity,\n",
    ")\n",
    "autoencoder.encoder.blocks.layer.configure(\n",
    "    nn.Conv1d, kernel_size=4, padding=\"same\"\n",
    ")\n",
    "autoencoder.encoder.blocks.pool[1:].configure(\n",
    "    nn.MaxPool1d, kernel_size=4, stride=4\n",
    ")\n",
    "autoencoder.decoder.blocks.layer.configure(\n",
    "    nn.Conv1d, kernel_size=4, padding=\"same\"\n",
    ")\n",
    "autoencoder.decoder.blocks.upsample[:-1].configure(\n",
    "    nn.ConvTranspose1d, kernel_size=4, stride=4\n",
    ")\n",
    "\n",
    "print(autoencoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will train the model by minimizing the reconstruction error through the L1 loss,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import L1Loss\n",
    "\n",
    "regressor_template = dl.Regressor(\n",
    "    model=autoencoder, loss=L1Loss(), optimizer=dl.Adam(),\n",
    ")\n",
    "ae = regressor_template.create()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the dataset and the dataloader. We train for 30 epochs with a batch size of 16. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataset = dt.pytorch.Dataset(train_pipeline & train_pipeline,\n",
    "                                   length=len(normal_train_ecgs))\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "autoencoder_trainer = dl.Trainer(max_epochs=30, accelerator=\"auto\")\n",
    "autoencoder_trainer.fit(ae, train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We show a few reconstructed traces together with the model input. The trained model has learned the main features of the normal traces, whereas is not able to generalize to anomalous traces, offering a poor reconstruction. It can also be observed that the reduced number of weights used by the model also produces reconstructed traces that are very similar among them, with only subtle differences. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 4, figsize=((10, 3)))\n",
    "fig.suptitle(\"Reconstructed ECG traces\", fontsize=15)\n",
    "for ax, idx in zip(axs.ravel(), np.random.choice( test_labels.shape[0], 5)):\n",
    "    test_ecg, test_label = test_ecgs[idx, :], test_labels[idx]\n",
    "    pred_ecg = ae(\n",
    "        torch.tensor(test_ecg).float().unsqueeze(0)\n",
    "    )\n",
    "    ax.plot(test_ecg, c=\"b\", label=\"Input ECG\")\n",
    "    ax.plot(pred_ecg.detach().squeeze(), c=\"r\", ls=\"--\", label=\"Reconstructed\")\n",
    "    [ax.set_title(\"Normal\") if test_label else ax.set_title(\"Anomalous\")]\n",
    "    ax.legend()\n",
    "    ax.set_xlabel(\"timestep\")\n",
    "    ax.set_ylim([0, 1])\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anomaly detection\n",
    "\n",
    "The distribution of the reconstruction metrics obtained for the training dataset can be used to calculate a threshold (e.g., the 90% quantile) to discriminate between normal and anomalous traces based on the reconstruction error. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import l1_loss\n",
    "\n",
    "normal_train_ecgs_tensor = torch.tensor(normal_train_ecgs).float().unsqueeze(1)\n",
    "normal_train_preds = ae(normal_train_ecgs_tensor).detach()\n",
    "normal_train_losses = [l1_loss(ecg, pred) for ecg, pred\n",
    "                       in zip(normal_train_ecgs_tensor, normal_train_preds)]\n",
    "\n",
    "threshold_losses = np.quantile(normal_train_losses, 0.9)\n",
    "\n",
    "plt.hist(normal_train_losses, bins=50, label=\"Training Dataset\")\n",
    "plt.axvline(x=threshold_losses, color=\"k\", linestyle=\":\", label=\"90% Quantile\")\n",
    "plt.xlabel(\"MAE\")\n",
    "plt.ylabel(\"ECG Number\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll calculate th reconstruction error on the test dataset. We observe a bimodal distribution with a second peak at values larger than the threshold we have calculated from the training dataset. These values should mainly correspond to anomalous traces that have been poorly reconstructed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ecgs_tensor = torch.tensor(test_ecgs).float().unsqueeze(1)\n",
    "test_preds = ae(test_ecgs_tensor).detach()\n",
    "test_losses = [l1_loss(ecg, pred) for ecg, pred \n",
    "               in zip(test_ecgs_tensor, test_preds)]\n",
    "\n",
    "plt.hist(test_losses, bins=50, label=\"Test Sataset\")\n",
    "plt.axvline(x=threshold_losses, color=\"k\", linestyle=\":\", label=\"90% Quantile\")\n",
    "plt.xlabel(\"MAE\")\n",
    "plt.ylabel(\"EGC Number\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will classify the traces of the test dataset based on the threshold value and calculate several classification metrics, such as the accuracy, the precision, and the recall. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "\n",
    "test_pred_labels = test_losses < threshold_losses\n",
    "\n",
    "print(f\"Accuracy = {accuracy_score(test_labels, test_pred_labels)}\")\n",
    "print(f\"Precision = {precision_score(test_labels, test_pred_labels)}\")\n",
    "print(f\"Recall = {recall_score(test_labels, test_pred_labels)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent space-based anomaly detection\n",
    "\n",
    "We apply the model to get the intermediate outputs of both the training and the test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_space_normal_train = ae.model.encoder(normal_train_ecgs_tensor).detach()\n",
    "latent_space_normal_train = (latent_space_normal_train\n",
    "                             .view(latent_space_normal_train.shape[0], -1))\n",
    "\n",
    "latent_space_test = ae.model.encoder(test_ecgs_tensor).detach()\n",
    "latent_space_test = latent_space_test.view(latent_space_test.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "n_neighbors = 4  # Number of nearest neighbors.\n",
    "\n",
    "neighbors = (NearestNeighbors(n_neighbors=n_neighbors + 1, \n",
    "                              algorithm=\"ball_tree\")\n",
    "             .fit(latent_space_normal_train))\n",
    "distances, _ = neighbors.kneighbors(latent_space_normal_train)\n",
    "distances = distances[:, 1:]\n",
    "mean_distance = np.mean(distances, 1)\n",
    "\n",
    "threshold_dist = np.quantile(mean_distance, 0.9)\n",
    "\n",
    "plt.hist(mean_distance, bins=50, label=\"Training Dataset\")\n",
    "plt.axvline(x=threshold_dist, color=\"k\", linestyle=\":\", label=\"90% Quantile\")\n",
    "plt.xlabel(\"Mean Distance\")\n",
    "plt.ylabel(\"ECG Number\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "mean_dist_test = pairwise_distances(latent_space_test, \n",
    "                                    latent_space_normal_train)\n",
    "mean_dist_test = np.mean(np.partition(mean_dist_test, n_neighbors, axis=1)[:, :n_neighbors], 1)\n",
    "\n",
    "plt.hist(mean_dist_test, bins=50, label=\"Test Dataset\")\n",
    "plt.axvline(x=threshold_dist, color=\"k\", linestyle=\":\", label=\"90% Quantile\")\n",
    "plt.xlabel(\"Mean Distance\")\n",
    "plt.ylabel(\"ECG Number\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.ones_like(test_labels)\n",
    "predictions[mean_dist_test > threshold_dist] = 0\n",
    "\n",
    "print(f\"Accuracy = {accuracy_score(test_labels, predictions)}\")\n",
    "print(f\"Precision = {precision_score(test_labels, predictions)}\")\n",
    "print(f\"Recall = {recall_score(test_labels, predictions)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dt_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
