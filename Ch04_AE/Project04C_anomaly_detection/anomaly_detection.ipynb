{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaly detection\n",
    "\n",
    "We'll demonstrate how an autoencoder can be used to determine anomalies in electrocardiograms.\n",
    "\n",
    "_Disclaimer: This application is built for demonstration purposes only. It is not intended for use in any medical setting._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The ECG dataset\n",
    "\n",
    "I will use the ECG dataset, which contains 4998 examples of electrocardiogram (ECG) signals. The dataset is available for download as a csv file either at http://www.timeseriesclassification.com/description.php?Dataset=ECG5000 or via the Google cloud storage http://storage.googleapis.com/download.tensorflow.org/data/ecg.csv. \n",
    "I will use the latter direction to get the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  \n",
    "\n",
    "dataframe = pd.read_csv(\"ecg.csv\", header=None)\n",
    "raw_data = dataframe.values\n",
    "\n",
    "print(f\"Size = {raw_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each ECG trace has 140 points, plus one last datapoint that represents the label ('0' for anomalous and '1' for normal).\n",
    "I will separate the labels from the actual traces. I will cut out the beginning and the end of the traces so that they can be used with the autoencoder architecture that I will define later. \n",
    "Last, I will randomly split the dataset into training (70%) and test (30%). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deeptrack as dt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "ecgs = raw_data[:, 2:-11]\n",
    "labels = raw_data[:, -1].astype(bool)\n",
    "\n",
    "# Create sources\n",
    "sources = dt.sources.Source(ecg=ecgs, is_normal=labels)\n",
    "# Split into train and test\n",
    "train_sources, test_sources = dt.sources.random_split(sources, [0.7, 0.3])\n",
    "# Extract normal ECGs from train set\n",
    "normal_sources = train_sources.filter(lambda ecg, is_normal: is_normal)\n",
    "\n",
    "print(f\"Number of normal ECGs = {len(normal_sources)} / {len(train_sources)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_normal_ecgs = np.min([source[\"ecg\"] for source in normal_sources])\n",
    "max_normal_ecgs = np.max([source[\"ecg\"] for source in normal_sources])\n",
    "\n",
    "ecg_pipeline = (\n",
    "    dt.Value(sources.ecg - min_normal_ecgs) / (max_normal_ecgs - min_normal_ecgs)\n",
    "    >> dt.Unsqueeze(axis=0)\n",
    "    >> dt.pytorch.ToTensor(dtype=torch.float)\n",
    ")\n",
    "\n",
    "label_pipeline = dt.Value(sources.is_normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "fig, axs = plt.subplots(1, 4, figsize=((10, 3)))\n",
    "fig.suptitle(\"Preprocessed normal ECG traces\", fontsize=15)\n",
    "for idx, ax in enumerate(axs.ravel()):\n",
    "    ecg = ecg_pipeline(normal_sources[idx])\n",
    "    ax.plot(ecg.squeeze())\n",
    "    ax.set_xlabel(\"timestep\")\n",
    "    ax.set_ylim([0, 1])\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will visualize some of the traces of the dataset after preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "fig, axs = plt.subplots(1, 4, figsize=((10, 3)))\n",
    "fig.suptitle(\"Preprocessed normal ECG traces\", fontsize=15)\n",
    "for idx, ax in enumerate(axs.ravel()):\n",
    "    ecg = ecg_pipeline(normal_sources[idx])\n",
    "    ax.plot(ecg.squeeze())\n",
    "    ax.set_xlabel(\"timestep\")\n",
    "    ax.set_ylim([0, 1])\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoder\n",
    "I will train a convolutional autoencoder to reconstruct only normal ECG traces by minimizing the reconstruction error. \n",
    "The autoencoder's latent space is ideally kept at a low dimensionality to focus on capturing the most salient features while suppressing less relevant or noisy information.\n",
    "\n",
    "I implement a simple convolutional autoencoder in 1D using as a backbone the `ConvolutionalEncoderDecoder2d` of `deeplay`.\n",
    "<!-- A similar exercise, using a dense autoencoder can be found at \\href{https://www.tensorflow.org/tutorials/generative/autoencoder}. -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deeplay as dl\n",
    "from torch.nn import Identity\n",
    "import torch.nn as nn\n",
    "\n",
    "### BM: we should add ConvolutionalEncoderDecoder1d instead of making ConvolutionalEncoderDecoder2d work with 1d data\n",
    "autoencoder = dl.ConvolutionalEncoderDecoder2d(\n",
    "    in_channels=1,\n",
    "    encoder_channels=[8, 8],\n",
    "    decoder_channels=[8, 8],\n",
    "    out_channels=1,\n",
    "    out_activation=Identity,\n",
    ")\n",
    "autoencoder[\"encoder|decoder\", ..., \"layer\"].configure(\n",
    "    nn.Conv1d, kernel_size=4, padding=\"same\"                                                        ### BM: padding=\"same\" with even kernel size gives warnings.\n",
    ")\n",
    "autoencoder[..., \"pool#1:\"].configure(\n",
    "    nn.MaxPool1d, kernel_size=4, stride=4\n",
    ")\n",
    "autoencoder[..., \"upsample#:-1\"].configure(\n",
    "    nn.ConvTranspose1d, kernel_size=4, stride=4\n",
    ")\n",
    "\n",
    "print(autoencoder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will train the model by minimizing the reconstruction error through the L1 loss,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import L1Loss\n",
    "\n",
    "regressor_template = dl.Regressor(\n",
    "    model=autoencoder, loss=L1Loss(), optimizer=dl.Adam(),\n",
    ")\n",
    "ae = regressor_template.create()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the dataset and the dataloader. We train for 30 epochs with a batch size of 16. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataset = dt.pytorch.Dataset(ecg_pipeline & ecg_pipeline, inputs=normal_sources)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "autoencoder_trainer = dl.Trainer(max_epochs=30, accelerator=\"auto\")\n",
    "autoencoder_trainer.fit(ae, train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We show a few reconstructed traces together with the model input. The trained model has learned the main features of the normal traces, whereas is not able to generalize to anomalous traces, offering a poor reconstruction. It can also be observed that the reduced number of weights used by the model also produces reconstructed traces that are very similar among them, with only subtle differences. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 4, figsize=((10, 3)))\n",
    "fig.suptitle(\"Reconstructed ECG traces\", fontsize=15)\n",
    "for ax, source in zip(axs.ravel(), np.random.choice(test_sources, 5)):\n",
    "    test_ecg, test_label = (ecg_pipeline & label_pipeline)(source)\n",
    "    pred_ecg = ae(test_ecg.float().unsqueeze(0))\n",
    "    ax.plot(test_ecg.squeeze(), c=\"b\", label=\"Input ECG\")\n",
    "    ax.plot(pred_ecg.detach().squeeze(), c=\"r\", ls=\"--\", label=\"Reconstructed\")\n",
    "    [ax.set_title(\"Normal\") if test_label else ax.set_title(\"Anomalous\")]\n",
    "    ax.legend()\n",
    "    ax.set_xlabel(\"timestep\")\n",
    "    ax.set_ylim([0, 1])\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anomaly detection\n",
    "\n",
    "The distribution of the reconstruction metrics obtained for the training dataset can be used to calculate a threshold (e.g., the 90% quantile) to discriminate between normal and anomalous traces based on the reconstruction error. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import l1_loss\n",
    "\n",
    "normal_train_ecgs_tensor = torch.stack([ecg_pipeline(source) for source in normal_sources])\n",
    "normal_train_preds = ae(normal_train_ecgs_tensor).detach()\n",
    "normal_train_losses = [l1_loss(ecg, pred) for ecg, pred\n",
    "                       in zip(normal_train_ecgs_tensor, normal_train_preds)]\n",
    "\n",
    "threshold_losses = np.quantile(normal_train_losses, 0.9)\n",
    "\n",
    "plt.hist(normal_train_losses, bins=50, label=\"Training Dataset\")\n",
    "plt.axvline(x=threshold_losses, color=\"k\", linestyle=\":\", label=\"90% Quantile\")\n",
    "plt.xlabel(\"MAE\")\n",
    "plt.ylabel(\"ECG Number\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll calculate th reconstruction error on the test dataset. We observe a bimodal distribution with a second peak at values larger than the threshold we have calculated from the training dataset. These values should mainly correspond to anomalous traces that have been poorly reconstructed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ecgs_tensor = torch.stack([ecg_pipeline(source) for source in test_sources])\n",
    "test_preds = ae(test_ecgs_tensor).detach()\n",
    "test_losses = [l1_loss(ecg, pred) for ecg, pred \n",
    "               in zip(test_ecgs_tensor, test_preds)]\n",
    "\n",
    "plt.hist(test_losses, bins=50, label=\"Test Sataset\")\n",
    "plt.axvline(x=threshold_losses, color=\"k\", linestyle=\":\", label=\"90% Quantile\")\n",
    "plt.xlabel(\"MAE\")\n",
    "plt.ylabel(\"EGC Number\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will classify the traces of the test dataset based on the threshold value and calculate several classification metrics, such as the accuracy, the precision, and the recall. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "\n",
    "test_labels = [label_pipeline(source) for source in test_sources]\n",
    "test_pred_labels = test_losses < threshold_losses\n",
    "\n",
    "print(f\"Accuracy = {accuracy_score(test_labels, test_pred_labels)}\")\n",
    "print(f\"Precision = {precision_score(test_labels, test_pred_labels)}\")\n",
    "print(f\"Recall = {recall_score(test_labels, test_pred_labels)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent space-based anomaly detection\n",
    "\n",
    "We apply the model to get the intermediate outputs of both the training and the test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_space_normal_train = ae.model.encoder(normal_train_ecgs_tensor).detach()\n",
    "latent_space_normal_train = (latent_space_normal_train\n",
    "                             .view(latent_space_normal_train.shape[0], -1))\n",
    "\n",
    "latent_space_test = ae.model.encoder(test_ecgs_tensor).detach()\n",
    "latent_space_test = latent_space_test.view(latent_space_test.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "n_neighbors = 4  # Number of nearest neighbors.\n",
    "\n",
    "neighbors = (NearestNeighbors(n_neighbors=n_neighbors + 1, \n",
    "                              algorithm=\"ball_tree\")\n",
    "             .fit(latent_space_normal_train))\n",
    "distances, _ = neighbors.kneighbors(latent_space_normal_train)\n",
    "distances = distances[:, 1:]\n",
    "mean_distance = np.mean(distances, 1)\n",
    "\n",
    "threshold_dist = np.quantile(mean_distance, 0.9)\n",
    "\n",
    "plt.hist(mean_distance, bins=50, label=\"Training Dataset\")\n",
    "plt.axvline(x=threshold_dist, color=\"k\", linestyle=\":\", label=\"90% Quantile\")\n",
    "plt.xlabel(\"Mean Distance\")\n",
    "plt.ylabel(\"ECG Number\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "mean_dist_test = pairwise_distances(latent_space_test, \n",
    "                                    latent_space_normal_train)\n",
    "mean_dist_test = np.mean(np.partition(mean_dist_test, n_neighbors, axis=1)[:, :n_neighbors], 1)\n",
    "\n",
    "plt.hist(mean_dist_test, bins=50, label=\"Test Dataset\")\n",
    "plt.axvline(x=threshold_dist, color=\"k\", linestyle=\":\", label=\"90% Quantile\")\n",
    "plt.xlabel(\"Mean Distance\")\n",
    "plt.ylabel(\"ECG Number\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.ones_like(test_labels)\n",
    "predictions[mean_dist_test > threshold_dist] = 0\n",
    "\n",
    "print(f\"Accuracy = {accuracy_score(test_labels, predictions)}\")\n",
    "print(f\"Precision = {precision_score(test_labels, predictions)}\")\n",
    "print(f\"Recall = {recall_score(test_labels, predictions)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dt_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
