{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Generation with Wasserstein Autoencoders\n",
    "\n",
    "We'll build a deterministic Wasserstein autoencoder (WAE) to generate images inspired by the Fashion MNIST dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Fashion MNIST dataset\n",
    "\n",
    "The Fashion-MNIST is a dataset of Zalando's article images, consisting of a training set of 60,000 examples and a test set of 10,000$ examples. Each example is a $28\\times28$ grayscale image, associated with a label from 10 classes. Fashion-MNIST is intended to serve as a direct drop-in replacement for the original MNIST dataset for benchmarking machine learning algorithms. It shares the same image size and structure of training and testing splits.\n",
    "\n",
    "We've organized these images in two folders named `train` and `test` in the GitHub repository https://github.com/DeepTrackAI/FashionMNIST_dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.path.exists(\"FashionMNIST_dataset\"):\n",
    "    os.system(\"git clone https://github.com/DeepTrackAI/FashionMNIST_dataset\")\n",
    "\n",
    "data_dir = os.path.join(\"FashionMNIST_dataset\", \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{len(os.listdir(data_dir))} training images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The labels of the classes are the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\", \"Sandal\", \n",
    "           \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the data using `Deeptrack2.0`. We will only use 6,000 images for training and 6,000 for test. We normalize the images in the range `[0, 1]`. We also define a pipeline to get images and labels of the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deeptrack as dt\n",
    "import torch\n",
    "\n",
    "files = dt.sources.ImageFolder(root=data_dir)\n",
    "train_files, test_files, _ = dt.sources.random_split(files, [0.1, 0.1, 0.8])\n",
    "\n",
    "train_pipeline = (\n",
    "    dt.LoadImage(train_files.path)\n",
    "    >> dt.NormalizeMinMax()\n",
    "    >> dt.MoveAxis(2, 0)\n",
    "    >> dt.pytorch.ToTensor(dtype=torch.float)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label(label_name):\n",
    "    \"\"\"Get image label.\"\"\"\n",
    "    return label_name[0]\n",
    "\n",
    "train_label = dt.Value(get_label, label_name=train_files.label_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the datasets and visualize some of the images in the test dataset with the corresponding label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "fig, axs = plt.subplots(3, 10, figsize=((10, 4)))\n",
    "for ax, train_file in zip(axs.ravel(), np.random.choice(train_files, axs.size)):\n",
    "    image, label = (train_pipeline & train_label)(train_file)\n",
    "    ax.imshow(image.squeeze(), cmap=\"gray\")\n",
    "    ax.set_title(f\"{int(label)} {classes[int(label)]}\", fontsize=9)    \n",
    "    ax.set_axis_off()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wasserstein autoencoder\n",
    "We define the Wasserstein autoencoder architecture with a deterministic latent space of 8 dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deeplay as dl\n",
    "\n",
    "latent_dimension = 8\n",
    "wae = dl.WassersteinAutoEncoder(latent_dim=latent_dimension).create()\n",
    "\n",
    "print(wae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the datasets, the dataloader and the trainer. We train the autoencder for `30` epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataset = dt.pytorch.Dataset(train_pipeline & train_pipeline, \n",
    "                                   inputs=train_files)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "wae_trainer = dl.Trainer(max_epochs=100, accelerator=\"auto\")\n",
    "wae_trainer.fit(wae, train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Reconstruction\n",
    "\n",
    "We check the WAE image reconstruction capability by comparing input images with autoencoder predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pipeline = (\n",
    "    dt.LoadImage(test_files.path)\n",
    "    >> dt.NormalizeMinMax()\n",
    "    >> dt.MoveAxis(2, 0)\n",
    "    >> dt.pytorch.ToTensor(dtype=torch.float)\n",
    ")\n",
    "test_label = dt.Value(get_label, label_name=test_files.label_name)\n",
    "#test_dataset = dt.pytorch.Dataset(test_pipeline & test_label, \n",
    "#                                  inputs=test_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 10, figsize=((10, 2)))\n",
    "for i, test_file in enumerate(np.random.choice(test_files, 10)):\n",
    "    image, label = (test_pipeline & test_label)(test_file)\n",
    "    axs[0, i].imshow(image.squeeze(), cmap=\"gray\")\n",
    "    axs[0, i].set_title(f\"{int(label)} {classes[int(label)]}\", fontsize=9)    \n",
    "    axs[0, i].set_axis_off()\n",
    "    \n",
    "    reconstructed_image, _ = wae(image.unsqueeze(0))\n",
    "    axs[1, i].imshow(reconstructed_image.detach().squeeze(), cmap=\"gray\")\n",
    "    axs[1, i].set_axis_off()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Generation\n",
    "We'll generate images by sampling a continuous distribution of latent representations and reconstructing the images using the trained WAEâ€™s decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = wae.decode(torch.randn(30, latent_dimension)).detach().squeeze()\n",
    "\n",
    "fig, axs = plt.subplots(3, 10, figsize=((10, 3)))\n",
    "for ax, image in zip(axs.ravel(), images):\n",
    "    ax.imshow(image, cmap=\"gray\")\n",
    "    ax.set_axis_off()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image interpolation\n",
    "We can use the latent space to interpolate between pairs of images by encoding them, linearly interpolate between their latent-space representations and decode the corresponding images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = 6\n",
    "\n",
    "fig, axs = plt.subplots(3, steps + 2, figsize=((10, 4)))\n",
    "for i, _ in enumerate(axs):\n",
    "    test_file_0, test_file_1 = np.random.choice(test_files, 2)\n",
    "    \n",
    "    image_0, label_0 = (test_pipeline & test_label)(test_file_0)\n",
    "    z_0 = wae.encode(image_0.unsqueeze(0))\n",
    "    \n",
    "    image_1, label_1 = (test_pipeline & test_label)(test_file_1)\n",
    "    z_1 = wae.encode(image_1.unsqueeze(0))\n",
    "    \n",
    "    axs[i, 0].imshow(image_0.squeeze(), cmap=\"gray\")\n",
    "    axs[i, 0].set_title(f\"{int(label_0)} {classes[int(label_0)]}\", fontsize=9)    \n",
    "    axs[i, 0].set_axis_off()\n",
    "    \n",
    "    for step in range(steps):\n",
    "        z_step = z_0 + (z_1 - z_0) * step / (steps - 1)\n",
    "        image_step = wae.decode(z_step).detach()\n",
    "        axs[i, step + 1].imshow(image_step.squeeze(), cmap=\"gray\")\n",
    "        axs[i, step + 1].set_axis_off()\n",
    "    \n",
    "    axs[i, -1].imshow(image_1.squeeze(), cmap=\"gray\")\n",
    "    axs[i, -1].set_title(f\"{int(label_1)} {classes[int(label_1)]}\", fontsize=9)    \n",
    "    axs[i, -1].set_axis_off()\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dt_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
