{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "train = datasets.MNIST(root=\"data\", train=True, download=True, \n",
    "                       transform=transforms.ToTensor())\n",
    "test = datasets.MNIST(root=\"data\", train=False, download=True, \n",
    "                      transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a ResNet18 backbone to train a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deeplay as dl\n",
    "\n",
    "backbone = dl.models.BackboneResnet18(in_channels=1, pool_output=True)\n",
    "head = dl.MultiLayerPerceptron(512, [], 10)\n",
    "\n",
    "classifier_net = dl.Sequential(backbone, head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we train a model on the full dataset as a baseline. We should see an accuracy between 99.0-99.5% on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nclassifier = dl.CategoricalClassifier(classifier_net.new(),\\n                                      optimizer=dl.Adam(lr=1e-3),\\n                                      num_classes=10,\\n                                      metrics=[accuracy]).build()\\n\\ntrainer = dl.Trainer(max_epochs=1)                                              ### trainer = dl.Trainer(max_epochs=30)\\ntrainer.fit(classifier, train_dataloader)\\nfull_results = trainer.test(classifier, test_dataloader)\\nprint(full_results[0])\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torchmetrics as tm\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train, batch_size=128, shuffle=True)\n",
    "test_dataloader = torch.utils.data.DataLoader(test, batch_size=1024, shuffle=False)\n",
    "\n",
    "accuracy = tm.Accuracy(task=\"multiclass\", num_classes=10)\n",
    "\n",
    "\"\"\"\n",
    "classifier = dl.CategoricalClassifier(classifier_net.new(),\n",
    "                                      optimizer=dl.Adam(lr=1e-3),\n",
    "                                      num_classes=10,\n",
    "                                      metrics=[accuracy]).build()\n",
    "\n",
    "trainer = dl.Trainer(max_epochs=1)                                              ### trainer = dl.Trainer(max_epochs=30)\n",
    "trainer.fit(classifier, train_dataloader)\n",
    "full_results = trainer.test(classifier, test_dataloader)\n",
    "print(full_results[0])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will test three active learning strategies. We will use the same ResNet18 model and train it on a small subset of the data. We will use the following strategies:\n",
    "- Random sampling (uniform)\n",
    "- Uncertainty sampling (Smallest margin)\n",
    "- Adversarial sampling \n",
    "\n",
    "The experiments will be repeated five times for statistical significance. We will compare the performance of the models on the test set and the number of samples required to reach a certain accuracy.\n",
    "\n",
    "First, we define the configurations of the experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "budget_per_iteration = 2                                                        ### budget_per_iteration = 120\n",
    "max_budget = 10                                                                 ### max_budget = 1800\n",
    "trials = 1                                                                      ### trials = 5\n",
    "\n",
    "# Number of rounds per trial\n",
    "rounds = max_budget // budget_per_iteration - 1\n",
    "\n",
    "uniform_experiment_accuracy = np.empty((trials, rounds))\n",
    "uncertainty_experiment_accuracy = np.empty((trials, rounds))\n",
    "adversarial_experiment_accuracy = np.empty((1, rounds))  ### only one trial for adversarial bc it's slow and stable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define a reusable active learning loop. This loop will be used to test the three strategies.\n",
    "In the loop, we will:\n",
    "1. Train the model on the current training set (trainer.fit)\n",
    "2. Evaluate the model on the test set (trainer.test)\n",
    "3. Use the active learning strategy to select the next samples (strategy.query_and_update)\n",
    "4. Reset the model to the starting state, such that each round of active learning starts training from scratch.\n",
    "\n",
    "An alternative formulation would omit the fourth step and continue training from the previous model state. This is useful if the traininig of the model is expensive. \n",
    "However, for this example, the training is relatively fast, so we will reset the model to the starting state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def active_learning_loop(strategy, epochs):\n",
    "    trainer = dl.Trainer(max_epochs=epochs, \n",
    "                            enable_checkpointing=False,\n",
    "                            enable_model_summary=False)\n",
    "    trainer.fit(strategy)\n",
    "\n",
    "    #test_results = trainer.test(strategy, test_dataloader)\n",
    "    #accuracy = test_results[0][\"testMulticlassAccuracy\"]\n",
    "    \n",
    "    print(\"1\")\n",
    "    strategy.query_and_update(budget_per_iteration)\n",
    "    print(\"2\")\n",
    "    \n",
    "    # Reset the model to the initial state.\n",
    "    strategy.reset_model()\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first strategy is uniform random sampling. To perform an active learning strategy, we first need to wrap the training data with a `ActiveLearningDataset` object. This object keeps track of the samples that have been annotated and the samples that are still unannotated. The `ActiveLearningDataset` object also provides a method to query the next samples to annotate. At the start, all data is assume to be unannotated.\n",
    "\n",
    "Then, we initialize the training dataset by randomly annotating a small subset of the data. This is required for all three active learning strategies we will test.\n",
    "\n",
    "Next, we create the strategy object, which contains the query strategy. It takes a model as input, together with the training data pool, the test set, a batch size, and a list of metrics.\n",
    "\n",
    "Finally, we run the active learning loop for `rounds` iterations, each round training for 40 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deeplay.activelearning as al"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "for trial in range(trials):\n",
    "\n",
    "    # Wrapper to remember which samples have been annotated\n",
    "    uniform_train_pool = al.ActiveLearningDataset(train)\n",
    "    \n",
    "    # Initialize the pool with random samples\n",
    "    uniform_train_pool.annotate_random(budget_per_iteration)\n",
    "\n",
    "    # Create a query strategy\n",
    "    uniform_strategy = al.UniformStrategy(classifier_net.new(), \n",
    "                                            uniform_train_pool, test=test, \n",
    "                                            batch_size=128,\n",
    "                                            test_metrics=[accuracy]).build()\n",
    "\n",
    "    for round in range(rounds):\n",
    "        uniform_experiment_accuracy[trial, round] = active_learning_loop(uniform_strategy, 1)       ### uniform_experiment_accuracy[trial, round] = active_learning_loop(uniform_strategy, 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try if margin uncertainty sampling can help us improve the model's performance."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "\n",
    "for trial in range(trials):\n",
    "    margin_train_pool = al.ActiveLearningDataset(train)\n",
    "    margin_train_pool.annotate_random(budget_per_iteration)\n",
    "\n",
    "    margin_strategy = al.UncertaintyStrategy(classifier_net.new(),\n",
    "                                             train_pool=margin_train_pool,\n",
    "                                             criterion=al.Margin(),\n",
    "                                             batch_size=128,\n",
    "                                             test_metrics=[accuracy]).build()\n",
    "\n",
    "    for i in range(rounds):\n",
    "        uncertainty_experiment_accuracy[trial, i] = active_learning_loop(margin_strategy, 1)       ### uncertainty_experiment_accuracy[trial, i] = active_learning_loop(margin_strategy, 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Margin does indeed work better than random sampling. However, neural networks are generally not very good at estimating their own uncertainty. More advanced methods try to mitigate this by using alternate means of estimating uncertainty. For example, ensamble methods, Monte Carlo dropout, or by estimating the loss of the model.\n",
    "\n",
    "Another issue with uncertainty sampling is that they can be biased towards outliers or datapoints with incomplete information. These are generally the datapoints that are the most uncertain, but they are not necessarily the most informative. There are a few other measures of informativeness that try to mitigate this issue. For example, the expected model change, diversity or representativeness of the data. In fact, a combination of these measures can perform better than any single measure. \n",
    "\n",
    "We'll explore a combination of uncertainty sampling and diversity sampling, using an adversarial approach. The idea is to adversarially train a discriminator to distinguish between the embeddings of images that have been annotated and those that have not. This has several advantages. First, the discriminator can indicate diversity. If the discriminator predicts that an unlabeled image is labeled, that means that the image is similar to already labeled images and might not be very informative. Second, by adversarially training the backbone to fool the discriminator, we are enforcing a structure to the embeddings using all the data in the dataset. This additional structure can help the model generalize better on small training sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([38067, 59485])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discriminator = dl.MultiLayerPerceptron(512, [512, 512], 1,\n",
    "                                        out_activation=torch.nn.Sigmoid())\n",
    "discriminator.initialize(dl.initializers.Kaiming())\n",
    "\n",
    "\n",
    "adversarial_train_pool = al.ActiveLearningDataset(train)\n",
    "adversarial_train_pool.annotate_random(budget_per_iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "adversarial_strategy = al.AdversarialStrategy(\n",
    "    backbone=backbone.new(),\n",
    "    classification_head=head.new(),\n",
    "    discriminator_head=discriminator.new(),\n",
    "    train_pool=adversarial_train_pool,\n",
    "    criterion=al.Margin(),\n",
    "    batch_size=128,\n",
    "    test_metrics=[accuracy]\n",
    ").build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/giovannivolpe/Documents/GitHub/DeepLearningCrashCourse/py_env_dlcc/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:75: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "/Users/giovannivolpe/Documents/GitHub/DeepLearningCrashCourse/py_env_dlcc/lib/python3.12/site-packages/lightning/pytorch/trainer/configuration_validator.py:74: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06fa447b2bf54e3392b0e46aff124851",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/giovannivolpe/Documents/GitHub/DeepLearningCrashCourse/py_env_dlcc/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=10` in the `DataLoader` to improve performance.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "for i in range(rounds):\n",
    "    adversarial_experiment_accuracy[0, i] = active_learning_loop(adversarial_strategy, 1)       ### adversarial_experiment_accuracy[0, i] = active_learning_loop(adversarial_strategy, 5)\n",
    "\"\"\"\n",
    "\n",
    "adversarial_experiment_accuracy = active_learning_loop(adversarial_strategy, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot the accuracy as a function of the number of annotated images. We find that the adversarial approach performs better than random sampling and margin sampling, and it is even competitive with the full dataset. Moreover, the adversarial approach is more stable than the other methods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = np.arange(budget_per_iteration, max_budget, budget_per_iteration)\n",
    "\n",
    "plt.plot(x, np.median(uniform_experiment_accuracy, 0), label=\"Uniform\", linestyle=\"--\")\n",
    "plt.plot(x, np.median(uncertainty_experiment_accuracy, 0), label=\"Uncertainty\", linestyle=\"-.\")\n",
    "plt.plot(x, adversarial_experiment_accuracy[0], label=\"Adversarial\", linestyle=\"-\")\n",
    "plt.axhline(full_results[0][\"testMulticlassAccuracy_epoch\"], label=\"Full Test Accuracy\", color=\"black\", linestyle=\":\")\n",
    "\n",
    "plt.xlabel(\"Number of Annotated Samples\")\n",
    "plt.ylabel(\"Test Accuracy\")\n",
    "plt.ylim(0.9, 1)\n",
    "plt.yticks([0.9, 0.95, full_results[0][\"testMulticlassAccuracy_epoch\"]])\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_levels = np.linspace(0.90, 1.0, 25)\n",
    "num_samples = np.arange(budget_per_iteration, max_budget, budget_per_iteration)\n",
    "\n",
    "average_samples_uniform = [num_samples[np.argmax(uniform_experiment_accuracy > level, axis=1)] for level in accuracy_levels]\n",
    "average_samples_uncertainty = [num_samples[np.argmax(uncertainty_experiment_accuracy > level, axis=1)] for level in accuracy_levels]\n",
    "average_samples_adversarial = [num_samples[np.argmax(adversarial_experiment_accuracy > level, axis=1)] for level in accuracy_levels]\n",
    "\n",
    "# if the accuracy is not reached, the number of samples is set to the maximum budget\n",
    "average_samples_uniform = [np.where(samples == budget_per_iteration, max_budget, samples).mean(-1) for samples in average_samples_uniform]\n",
    "average_samples_uncertainty = [np.where(samples == budget_per_iteration, max_budget, samples).mean(-1) for samples in average_samples_uncertainty]\n",
    "average_samples_adversarial = [np.where(samples == budget_per_iteration, max_budget, samples).mean(-1) for samples in average_samples_adversarial]\n",
    "\n",
    "# averag\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(accuracy_levels, average_samples_uniform, label=\"Uniform\", linestyle=\"--\")\n",
    "plt.plot(accuracy_levels, average_samples_uncertainty, label=\"Uncertainty\", linestyle=\"-.\")\n",
    "plt.plot(accuracy_levels, average_samples_adversarial, label=\"Adversarial\", linestyle=\"-\")\n",
    "plt.xlabel(\"Test Accuracy\")\n",
    "plt.ylabel(\"Average Number of Annotated Samples\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
