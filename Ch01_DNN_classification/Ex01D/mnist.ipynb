{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# MNIST Classification\n",
        "\n",
        "We'll build a dense neural network to classify images and apply it to the classical problem of the classification of the hand-written digits in the MNIST dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The MNIST dataset\n",
        "\n",
        "The MNIST dataset consists of grayscale images of hand-written digits from 0 to 9. Each image is 28 pixels by 28 pixels. There're 60,000 training images and 10,000 test images.\n",
        "\n",
        "We've organized these images in two folders named `train` and `test` in the GitHub repository https://github.com/DeepTrackAI/MNIST_dataset:\n",
        "\n",
        "> train/0_000000.png<br>\n",
        "> train/0_000001.png<br>\n",
        "> ...<br>\n",
        "> train/1_000000.png<br>\n",
        "> ...<br>\n",
        "\n",
        "> test/0_000000.png<br>\n",
        "> ...<br>\n",
        "> test/1_000000.png<br>\n",
        "> ...<br>\n",
        "\n",
        "The first digit in the filename is the label.\n",
        "\n",
        "The following code will download the MINST dataset repository only if the `MNIST_dataset` directory doesn't already exist."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "if not os.path.exists(\"MNIST_dataset\"):\n",
        "    os.system(\"git clone https://github.com/DeepTrackAI/MNIST_dataset\")\n",
        "\n",
        "train_path = os.path.join(\"MNIST_dataset\", \"mnist\", \"train\")\n",
        "train_images_paths = os.listdir(train_path)\n",
        "\n",
        "print(f\"{len(train_images_paths)} training images\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can use `matplotlib` to load the images into `numpy` arrays. Since the MNIST dataset is small enough to be held in memory, we can load all images at once. For larger datasets, we'd need to load the images as needed during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "train_images = []\n",
        "for path in train_images_paths:\n",
        "    image = plt.imread(os.path.join(train_path, path))\n",
        "    train_images.append(image)\n",
        "\n",
        "print(f\"{len(train_images)} training images with shape {train_images[0].shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Since the digit is encoded as the first character of the filename, we can extract the groundtruth labels from the filenames."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_digits = []\n",
        "for path in train_images_paths:\n",
        "    filename = os.path.basename(path)\n",
        "    digit = int(filename[0])\n",
        "    train_digits.append(digit)\n",
        "\n",
        "print(f\"{len(train_digits)} training ground-truth digits\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can now visualize some of the MNIST digits."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "idx_images_to_show = np.linspace(0, 60000, 18, dtype=int, endpoint=False)\n",
        "\n",
        "plt.figure(figsize=(15, 9))\n",
        "\n",
        "for i in range(18):\n",
        "    idx = idx_images_to_show[i]\n",
        "\n",
        "    plt.subplot(3, 6, i + 1)\n",
        "    plt.title(f\"Label: {train_digits[idx]}\", fontsize=20)\n",
        "    plt.imshow(train_images[idx].squeeze(), cmap=\"Greys\") \n",
        "    plt.axis(\"off\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## First Neural Network\n",
        "\n",
        "We will create, train, and evaluate a first version of the neural network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Neural Network Model\n",
        "\n",
        "We use the class `MultiLayerPerceptron` from `deeplay` to define a small _dense neural network_ (often called also _multi-layer perceptron_ or _fully connected neural network_) with 784 = 28 x 28 inputs (one for each pixel,`in_features=28 * 28`), two hidden layers with 32 neurons each (`hidden_features=[32, 32]`) with _sigmoid_ activation (`.blocks.activation.configure(Sigmoid)`), and an output layer with 10 neurons (`out_features=10`) with _sigmoid_ activation (one for each digit, `out_activation=Sigmoid`). We then call `.create()` to create the neural network.\n",
        "\n",
        "The output will be a vector of 10 numbers between 0 and 1, which can be loosely interpreted as probabilities, so that the predicted digit is the one with the highest output value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import deeplay as dl\n",
        "from torch.nn import Sigmoid\n",
        "\n",
        "mlp_template = dl.MultiLayerPerceptron(in_features=28 * 28, \n",
        "                                       hidden_features=[32, 32], \n",
        "                                       out_features=10, \n",
        "                                       out_activation=Sigmoid)\n",
        "mlp_template.blocks.activation.configure(Sigmoid)\n",
        "mlp_model = mlp_template.create()\n",
        "\n",
        "print(mlp_model)\n",
        "print(f\"{sum(p.numel() for p in mlp_model.parameters())} trainable parameters\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Application as Classifier\n",
        "\n",
        "We use the `Classifier` application class from `deeplay` to define what we want to do with the neural network (e.g., determining the training logic and adding some convenient metrics such as accuracy).\n",
        "\n",
        "We add to the classifier the model we have just created (`model=mlp_template`).  Then, we set the number of classes (`num_classes=10`), convert the groundtruth digits to one-hot vectors to match the output of the model (`make_targets_one_hot=True`), set _mean squared error_ as loss function (`loss=MSELoss()`), set _stochastic gradient descent_ as optimizer with learning rate 0.1 (`optimizer=dl.SGD(lr=.1)`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.nn import MSELoss\n",
        "\n",
        "classifier_template = dl.Classifier(\n",
        "    model=mlp_template,\n",
        "    num_classes=10,\n",
        "    make_targets_one_hot=True,\n",
        "    loss=MSELoss(),\n",
        "    optimizer=dl.SGD(lr=.1),\n",
        ")\n",
        "classifier = classifier_template.create()\n",
        "\n",
        "print(classifier)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Dataloader\n",
        "\n",
        "A `DataLoader` object manages the data to be passed to the neural network. The simplest way to create a dataloader from data already in memory is to create a list of `(sample, ground_truth)` tuples. This can easily be achieved using the Python native function `zip()`.\n",
        "\n",
        "For the training, it is important to shuffle the data, while this is not so relevant for the testing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_images_digits = list(zip(train_images, train_digits))\n",
        "train_dataloader = dl.DataLoader(train_images_digits, shuffle=True)\n",
        "\n",
        "print(f\"{len(train_dataloader)} train batches\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Trainer\n",
        "\n",
        "A `Trainer` object manages the training and evaluation, e.g., deciding what hardware to use, when to stop training, whether to utilize the GPU, and when to save the model, log the training, and evaluate the metrics. \n",
        "\n",
        "For now, we will create a trainer for a single epoch (an epoch is a single pass through the entire training set, `max_epochs=1`) and with automatic hardware acceleration (it will use a GPU if available, otherwise a CPU, `accelerator=\"auto\"`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "trainer = dl.Trainer(max_epochs=1, accelerator=\"auto\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "trainer.fit(classifier, train_dataloader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Testing\n",
        "\n",
        "Let's check the test set. We can reuse the trainer to test the model on the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_path = os.path.join(\"MNIST_dataset\", \"mnist\", \"test\")\n",
        "test_images_paths = os.listdir(test_path)\n",
        "\n",
        "print(f\"{len(test_images_paths)} test images\")\n",
        "\n",
        "test_images = []\n",
        "for path in test_images_paths:\n",
        "    image = plt.imread(os.path.join(test_path, path))\n",
        "    test_images.append(image)\n",
        "\n",
        "print(f\"{len(test_images)} test images with shape {test_images[0].shape}\")\n",
        "\n",
        "test_digits = []\n",
        "for path in test_images_paths:\n",
        "    filename = os.path.basename(path)\n",
        "    digit = int(filename[0])\n",
        "    test_digits.append(digit)\n",
        "\n",
        "print(f\"{len(test_digits)} test ground-truth digits\")\n",
        "\n",
        "test_images_digits = list(zip(test_images, test_digits))\n",
        "test_dataloader = dl.DataLoader(test_images_digits, shuffle=False)\n",
        "\n",
        "print(f\"{len(test_dataloader)} test batches\")\n",
        "\n",
        "trainer.test(classifier, test_dataloader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Confusion Matrix\n",
        "\n",
        "A _confusion matrix_ illustrates the performance of a classifier and to gain insights to improve its architecture and training hyperparameters. \n",
        "\n",
        "> A confusion matrix is a square matrix with a number of rows and columns equal to the number of classes in the classification problem (here, 10). Each row and each column corresponds to one class, ordered arbitrarily but equally for the two axes (here, we have a natural ordering of the classes, which is the order of the digits). An element $c_{i,j}$ of the confusion matrix represents the number of times the classifier assigned the predicted class $j$ to the actual class $i$. For example, in our case, $c_{3,5}$ corresponds to the number of times the neural network, when given an image depicting the digit $3$, classifies it as the digit $5$.\n",
        "\n",
        "We've assigned the `pred_digit` to the first axis (rows) and the `gt_digit` to the second axis (columns), but the opposite is also a valid (and comon) choice."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_confusion_matrix(classifier, trainer, dataloader):\n",
        "    from seaborn import heatmap, cubehelix_palette\n",
        "\n",
        "    confusion_matrix = np.zeros((10, 10), dtype=int)\n",
        "    for image, gt_digit in dataloader:\n",
        "        predictions = classifier(image)\n",
        "        max_prediction, pred_digit = predictions.max(dim=1)\n",
        "        np.add.at(confusion_matrix, (gt_digit, pred_digit), 1) \n",
        "\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    heatmap(confusion_matrix, annot=True, fmt=\".0f\", square=True, \n",
        "            cmap=cubehelix_palette(light=0.95, as_cmap=True), vmax=150)\n",
        "    plt.xlabel(\"Predicted digit\", fontsize=15)\n",
        "    plt.ylabel(\"Groundtruth digit\", fontsize=15)\n",
        "    plt.show()\n",
        "    \n",
        "plot_confusion_matrix(classifier, trainer, test_dataloader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Neural Network Improvements\n",
        "\n",
        "We now start a journey to improve our network and its training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Softmax Output Representation\n",
        "\n",
        "Currently each digit is assigned a value between 0 and 1 by the neural network. However, we know that only one classification is true, so we can normalize the output prediction such that their sums is one using a _softmax activation_."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.nn import Softmax\n",
        "\n",
        "classifier_template.model.configure(out_activation=Softmax(dim=-1))\n",
        "classifier_softmax = classifier_template.create()\n",
        "print(classifier_softmax)\n",
        "\n",
        "trainer_softmax = dl.Trainer(max_epochs=1, accelerator=\"auto\")\n",
        "trainer_softmax.fit(classifier_softmax, train_dataloader)\n",
        "trainer_softmax.test(classifier_softmax, test_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_confusion_matrix(classifier_softmax, trainer_softmax, test_dataloader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ReLU Activation Functions\n",
        "\n",
        "We now change the internal activation functions from sigmoidal to ReLU function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.nn import ReLU\n",
        "\n",
        "classifier_template.model.blocks[:].configure(\"activation\", ReLU)\n",
        "classifier_relu = classifier_template.create()\n",
        "print(classifier_relu)\n",
        "\n",
        "trainer_relu = dl.Trainer(max_epochs=1, accelerator=\"auto\")\n",
        "trainer_relu.fit(classifier_relu, train_dataloader)\n",
        "trainer_relu.test(classifier_relu, test_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_confusion_matrix(classifier_relu, trainer_relu, test_dataloader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Mini Batches and Optimizer\n",
        "\n",
        "We now introduce _mini-batches_ and, to better utilize the increased batch size, we use the `RMSprop` optimizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_dataloader_batch = dl.DataLoader(train_images_digits, shuffle=True, \n",
        "                                       batch_size=16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "classifier_template.configure(optimizer=dl.RMSprop(lr=0.001))\n",
        "classifier_rmsprop = classifier_template.create()\n",
        "print(classifier_rmsprop)\n",
        "\n",
        "trainer_rmsprop = dl.Trainer(max_epochs=10, accelerator=\"auto\")\n",
        "trainer_rmsprop.fit(classifier_rmsprop, train_dataloader_batch)\n",
        "trainer_rmsprop.test(classifier_rmsprop, test_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_confusion_matrix(classifier_rmsprop, trainer_rmsprop, test_dataloader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Failure Analysis\n",
        "\n",
        "Checking for which inputs the neural network fails can suggests potential ways to improve its performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "num_images_x_digit = 6\n",
        "\n",
        "plt.figure(figsize=(10, num_images_x_digit))\n",
        "\n",
        "num_failures_x_digit = np.zeros(10, int)\n",
        "for image, gt_digit in test_dataloader:\n",
        "    gt_digit = int(gt_digit)\n",
        "\n",
        "    if num_failures_x_digit[gt_digit] < num_images_x_digit:\n",
        "        predictions = classifier_rmsprop(image)\n",
        "        max_predition, pred_digit = predictions.max(dim=1)\n",
        "\n",
        "        if pred_digit != gt_digit:\n",
        "            num_failures_x_digit[gt_digit] += 1\n",
        "\n",
        "            plt.subplot(num_images_x_digit, 10, \n",
        "                        (num_failures_x_digit[gt_digit] - 1) * 10 + gt_digit + 1)\n",
        "            plt.imshow(image.squeeze(), cmap=\"Greys\")\n",
        "            plt.annotate(str(int(pred_digit)), (.8, 1), (1, 1), \n",
        "                         xycoords=\"axes fraction\", textcoords=\"offset points\", \n",
        "                         va=\"top\", ha=\"left\", fontsize=20, color=\"red\")\n",
        "            plt.axis(\"off\")\n",
        "            \n",
        "    if (num_failures_x_digit >= num_images_x_digit).all():\n",
        "        break\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
