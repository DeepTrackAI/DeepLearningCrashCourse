{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Introduction to Vision Transformer (ViT)**\n",
    "\n",
    "The [Vision Transformer](https://arxiv.org/abs/2010.11929) (ViT) represents an innovative breakthrough in how machines interpret images. Developed by Google Research, the ViT draws inspiration from the transformative impact of transformer models in Natural Language Processing (NLP). Traditionally, Convolutional Neural Networks (CNNs) dominated image analysis tasks. However, the introduction of ViT marked a paradigm shift, demonstrating that a model primarily designed for text could excel in visual domains as well. The ViT model has not only outperformed existing CNN-based methods across various image classification benchmarks but also shown remarkable scalability and generalization capabilities. Its success has led to its adoption in several leading-edge applications, including image segmentation in [Segment Anything](https://arxiv.org/abs/2304.02643) (SAM) and multimodal learning in [Contrastive Language-Image Pre-training](https://arxiv.org/pdf/2103.00020.pdf) (CLIP).\n",
    "\n",
    "This notebook provides an introduction to the Vision Transformer (ViT) model, explaining its architecture and how it can be implemented using PyTorch for image classification tasks.\n",
    "\n",
    "#### **CIFAR-10 Dataset**\n",
    "\n",
    "The CIFAR-10 dataset will serve as our primary playground for experimenting with ViT.\n",
    "\n",
    "Comprising 60,000 images, each 32x32 pixels and in full color, CIFAR-10 is categorized into 10 distinct classes, with each class containing 6,000 images. The dataset is divided into a training set of 50,000 images and a testing set of 10,000 images.\n",
    "\n",
    "Use the following code snippet to download and prepare the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms, datasets\n",
    "\n",
    "train_val_dataset = datasets.CIFAR10(root=\"./data\", train=True, download=True, \n",
    "                                     transform=transforms.ToTensor())\n",
    "test_dataset = datasets.CIFAR10(root=\"./data\", train=False, download=True, \n",
    "                                transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script initializes the CIFAR-10 dataset for training and testing purposes using PyTorch's `datasets.CIFAR10` class for seamless data handling. \n",
    "\n",
    "**(1)** To initiate the training dataset, the script specifies the root directory for storing the data, sets the `train` parameter to `True`, and enables the download option. Additionally, it uses the `transforms.ToTensor()` method to convert the images into PyTorch tensors.\n",
    "\n",
    "**(2)** To initialize the testing dataset, similar parameters are used except for the `train` parameter, which is set to `False`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize images from the CIFAR-10 dataset by defining the function `plot_class_examples`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_class_examples(dataset, n_images):\n",
    "    \"\"\"Plot the first images for each class in the dataset.\"\"\"\n",
    "    classes = dataset.classes\n",
    "    class_img_dict = {c: [] for c in range(len(classes))}\n",
    "    for i, (_, label) in enumerate(dataset):\n",
    "        if all(len(v) == n_images for v in class_img_dict.values()): \n",
    "            break\n",
    "        if len(class_img_dict[label]) < n_images: \n",
    "            class_img_dict[label].append(i)\n",
    "    \n",
    "    fig, axs = plt.subplots(n_images, len(classes), figsize=(len(classes), 3))\n",
    "    for class_idx, img_indices in class_img_dict.items():\n",
    "        for j, img_index in enumerate(img_indices):\n",
    "            img = dataset[img_index][0].permute(1, 2, 0)\n",
    "            axs[j, class_idx].imshow(img)\n",
    "            axs[j, class_idx].set(xticks=[], yticks=[])\n",
    "            if j == 0: \n",
    "                axs[j, class_idx].set_title(classes[class_idx], size=\"medium\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function displays the first  `n_images` examples for each class from the dataset.\n",
    "\n",
    "**(1)** The process starts by extracting the classes from the dataset and **(2)** initializing a dictionary to hold indices of images for each class. **(3-4)** Then, as it iterates over the dataset, the function checks whether enough images have been collected for each class. **(5-6)** If a class still needs more images, the current index is added to its list of indices. **(7)** Once the desired number of images for each class is reached, the function prepares a matplotlib figure with a grid of subplots. The subplots are arranged to have one row per class and one column per image. This ensures that the layout supports the visualization of all selected images. \n",
    "\n",
    "**(8-10)** For each class, the function iterates over its collected image indices, retrieves each image, permutes it for correct display formatting, and plots them in the corresponding subplot cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_class_examples(train_val_dataset, n_images=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Preparing the data**\n",
    "\n",
    "Preparing the data involves more than just loading it; we also need to ensure that our model trains effectively and generalizes well to unseen data. To this end, the CIFAR-10 dataset, initially comprising only training and testing subsets, requires further partitioning to create a separate validation set.\n",
    "\n",
    "For this purpose, we implement the `split_train_val` function, which splits the training set into training and validation subsets given a specified validation split ratio. This split allows us to independently evaluate the model's performance and make adjustments as needed without compromising the integrity of the test dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy, torch\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "indices = torch.randperm(len(train_val_dataset)).tolist()\n",
    "split = int(len(train_val_dataset) * 0.20)\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "train_dataset = Subset(copy.deepcopy(train_val_dataset), train_indices)\n",
    "val_dataset = Subset(copy.deepcopy(train_val_dataset), val_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `split_train_val` function takes a dataset and a validation split ratio as inputs, and returns the training and validation subsets. The function performs the following steps:\n",
    "\n",
    "**(1)** It generates random indices for the dataset.\n",
    "\n",
    "**(2)** It calculates the split index based on the validation split ratio.\n",
    "\n",
    "**(3)** It splits the indices into training and validation indices.\n",
    "\n",
    "**(4)** It creates the training subset using the `Subset` class, which allows us to create a subset of the dataset using the specified indices.\n",
    "\n",
    "**(5)** It creates the validation subset using the same method as step 4. To ensure that the training and validation subsets are independent of each other, the function uses the `copy.deepcopy` function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example, we will use a validation split ratio of 0.2, which means that 20% of the training data will be reserved for validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To enhance the model's ability to generalize and prevent overfitting, we will apply various data augmentation techniques to the training dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(), transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.49139968, 0.48215841, 0.44653091], \n",
    "                         std=[0.24703223, 0.24348513, 0.26158784]),\n",
    "])\n",
    "train_dataset.dataset.transform = train_transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These include **(1)** random cropping and padding, **(2)** horizontal flipping, and **(3)** normalization of the image tensors using the CIFAR-10 training set mean and standard deviation values for each channel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the validation dataset, however, we'll restrict the preprocessing to just conversion to tensors and normalization, avoiding augmentation to maintain the validity of the evaluation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.49139968, 0.48215841, 0.44653091], \n",
    "                         std=[0.24703223, 0.24348513, 0.26158784]),\n",
    "])\n",
    "val_dataset.dataset.transform = val_transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Defining data loaders**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will define data loaders that will feed the training, validation, and test data to the model during the training and evaluation processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Building a Vision Transformer model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having prepared our dataset, we define the Vision Transformer (ViT) model. \n",
    "\n",
    "**Figure 1** illustrates the ViT architecture. It is built around four pivotal components that work together to process and classify images.\n",
    "\n",
    "Firstly, **Image Patch Embeddings** play an essential role by slicing the input image into fixed-size, non-overlapping patches. These patches are linearly embedded, creating a series of flattened embeddings that serve as the input sequence for the transformer encoder (**Figure 1a**). To capture global information about the image, an additional learnable class token is integrated with these patch embeddings (**Figure 1b**). This class token, while not directly corresponding to any individual patch, symbolizes the collective image and its interactions with the embeddings, providing a holistic view for classification.\n",
    "\n",
    "Secondly, the model incorporates **Positional Encodings** to permeate the patch embeddings with spatial context (**Figure 1b**). Unlike sin/cos encoding, these positional encodings are initially randomized and subsequently refined through the learning process, ensuring the model comprehends the layout and position of each patch within the image.\n",
    "\n",
    "The **Transformer Encoder**, the third component, is the heart of the model (**Figure 1c**). It comprises several layers of transformer blocks that each feature a multi-head self-attention mechanism and a feedforward neural network. This structure allows the model to focus on different parts of the image simultaneously, facilitating a deeper understanding of the visual content.\n",
    "\n",
    "The **Classification Head** or **Dense Top** is the last component of ViT (**Figure 1d**). It takes the class token output from the transformer encoder and feeds it through a feedforward neural network to predict the image's class label.\n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"images/vit-image.png\" width=600 height=auto>\n",
    "  <br>\n",
    "  <b>Figure 1. Vision Transformer (ViT) Architecture</b>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now instantiate the ViT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deeplay as dl\n",
    "\n",
    "vit = dl.ViT(in_channels=3, image_size=32, patch_size=4, \n",
    "             hidden_features=[384,] * 7, out_features=10, num_heads=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Deeplay `ViT` class is responsible for managing the ViT (Vision Transformer) model architecture. The class constructor initializes the model components, including the patch embeddings, positional encodings, transformer encoder, and classification head. `ViT` accepts the following parameters:\n",
    "\n",
    "**(1)** `in_channels`: This parameter defines the number of input channels in the image. For the CIFAR-10 dataset, the number of channels is 3 (RGB).\n",
    "\n",
    "**(2)** `image_size`: This parameter defines the size of the input image, which is 32x32 for the CIFAR-10 dataset.\n",
    "\n",
    "**(3)** `patch_size`: This parameter defines the size of the image patches. By dividing the image into smaller patches, the model can analyze the image in segments and learn local features before integrating them for global understanding. This parameter is set to 4, resulting in a total of 64 patches of size 4x4.\n",
    "\n",
    "**(4)** `hidden_dim`: This parameter defines the dimension of the hidden layers in the transformer encoder. Seven transformer blocks (or layers) are defined in the model, each with a hidden size of 384 channels.\n",
    "\n",
    "**(5)** `out_channels`: This parameter defines the number of classes in the dataset, which is 10 for CIFAR-10.\n",
    "\n",
    "**(6)** `num_heads`: This parameter defines the number of attention heads in the multi-head self-attention mechanism. In this case, it is set to 12.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Training the model**\n",
    "\n",
    "The following create a `Trainer` object to train the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the Adam optimizer with a learning rate of `1e-3` and weight decay of `1e-5`, and `CrossEntropyLoss` as the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = dl.Classifier(\n",
    "    model=vit, num_classes=10,\n",
    "    optimizer=dl.Adam(lr=1e-3, weight_decay=5e-5, betas=(0.9, 0.999)),\n",
    ").build()\n",
    "trainer = dl.Trainer(max_epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(classifier, train_dataloader, val_dataloader)\n",
    "trainer.history.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the following code snippet to visualize the training and validation loss and accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ViT model achieves a validation accuracy of approximately 76% and reaches saturation at that point. While this is not a bad performance, it is still far from being comparable to the classification ability of Convolutional Neural Networks.\n",
    "\n",
    "On the test set, the model achieves a similar accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset.transform = val_transform\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test(classifier, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the ViT model can be further fine-tuned to improve its performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Improving the model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to CNNs, ViT models lack strong inductive biases. Inductive bias refers to the set of assumptions a model makes about the data it trains on to generalize from the training data to unseen data effectively. For instance, CNNs inherently assume spatial hierarchies in images, leveraging local patterns (e.g., edges and textures in early layers) and composing them into higher-order features (like objects) in deeper layers. This architectural  innatency towards capturing spatial locality and translation invariance directly supports learning from image data.\n",
    "\n",
    "Due to the absence of these biases, ViT requires a substantial amount of data to learn effectively. It treats the image as a sequence of patches and learns to interpret these without the innate advantage of assuming spatial correlations, relying instead on the data to dictate these relationships. Consequently, ViT models need larger datasets to achieve their impressive performance, as they must learn from scratch what CNNs get for free through their architecture.\n",
    "\n",
    "The CIFAR-10 dataset, containing only 60,000 images, is relatively small compared to the massive datasets where ViT models excel. Although 60,000 might seem substantial, it pales in comparison to the vast datasets like ImageNet with over 14 million images, which have been instrumental in training ViTs to their state-of-the-art performance.\n",
    "\n",
    "[CutMix](https://arxiv.org/abs/1905.04899) is a technique designed to address this challenge, among others. It works by forcing the network to learn local relationships more efficiently, effectively injecting the network with sufficient pathways to learn spatial insights without relying on vast amounts of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Implementing CutMix**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CutMix is a data augmentation strategy that seeks to enhance the model's ability to understand and integrate local features within images, thereby improving its generalization capabilities. Originating from the idea of [Mixup](https://arxiv.org/abs/1710.09412), where two images are blended together by averaging their pixels, CutMix takes this concept further by combining patches from two different images rather than mixing their entire content. This technique not only enriches the dataset but also introduces a more challenging and nuanced learning task for the model.\n",
    "\n",
    "In a typical CutMix  augmentation, a patch from one image is cut and pasted onto another image, and the labels are mixed proportionally to the area of the patches. This process creates a new training example that is a composite of features and labels from two distinct images. For the model, the task then becomes to predict a mixed label, reflecting the composition of the input image. The primary advantage of this method is its ability to force the model to focus on less dominant features of the image, promoting a deeper and more robust learning of spatial relationships and feature representations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following class `CutMix` implements the CutMix augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class CutMix(object):\n",
    "    \"\"\"CutMix.\"\"\"\n",
    "\n",
    "    def __init__(self, size, beta):\n",
    "        \"\"\"Initialize CutMix.\"\"\"\n",
    "        self.size, self.beta = size, beta\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        \"\"\"Execute CutMix.\"\"\"\n",
    "        imgs, labels = batch\n",
    "        rand_idx = torch.randperm(imgs.size(0))\n",
    "        rand_imgs, rand_labels = imgs[rand_idx], labels[rand_idx]\n",
    "        initial_mix_ratio = np.random.beta(self.beta, self.beta)\n",
    "        augmented_imgs = imgs.clone()\n",
    "        \n",
    "        r_x = np.random.uniform(0, self.size)\n",
    "        r_y = np.random.uniform(0, self.size)\n",
    "        r_w = self.size * np.sqrt(1 - initial_mix_ratio)\n",
    "        r_h = self.size * np.sqrt(1 - initial_mix_ratio)\n",
    "\n",
    "        x1 = np.clip(int(r_x - r_w // 2), a_min=0, a_max=self.size)\n",
    "        x2 = np.clip(int(r_x + r_w // 2), a_min=0, a_max=self.size)\n",
    "        y1 = np.clip(int(r_y - r_h // 2), a_min=0, a_max=self.size)\n",
    "        y2 = np.clip(int(r_y + r_h // 2), a_min=0, a_max=self.size)\n",
    "\n",
    "        augmented_imgs[:, :, y1:y2, x1:x2] = rand_imgs[:, :, y1:y2, x1:x2]\n",
    "        final_mix_ratio = 1 - ((x2 - x1) * (y2 - y1) / (self.size ** 2))\n",
    "        return augmented_imgs, labels, rand_labels, final_mix_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the CutMix augmentation by applying it to a random pair of images from the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from itertools import chain\n",
    "\n",
    "def create_batch(samples):\n",
    "    \"\"\"Create batch.\"\"\"\n",
    "    images, labels = zip(*samples)\n",
    "    return torch.stack(images), torch.tensor(labels)\n",
    "\n",
    "def normalize_image(image):\n",
    "    \"\"\"Normalize image.\"\"\"\n",
    "    mean = torch.tensor([0.49139968, 0.48215841, 0.44653091]).view(3, 1, 1)\n",
    "    std = torch.tensor([0.24703223, 0.24348513, 0.26158784]).view(3, 1, 1)\n",
    "    return image * std + mean\n",
    "\n",
    "samples = random.choices(train_dataset, k=2)\n",
    "batch = create_batch(samples)\n",
    "\n",
    "augmented_imgs, *_, mix_ratio = CutMix(size=32, beta=1.0)(batch)\n",
    "\n",
    "fig, axs = plt.subplots(1, 4, figsize=(12, 3))\n",
    "for i, img in enumerate(chain(batch[0], augmented_imgs)):\n",
    "    axs[i].imshow(normalize_image(img).permute(1, 2, 0))\n",
    "    axs[i].set_title(\"Original\" if i < 2 else f\"CutMix {mix_ratio:.2f}\")\n",
    "    axs[i].axis(\"off\")    \n",
    "plt.savefig(\"fig_08_C4.pdf\", bbox_inches=\"tight\")  ### plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code snippet **(1)** selects two random images from the dataset `train_dataset`. Next, **(2)** the `create_batch` function is used to create a batch of images and labels from the selected images. **(3)** The `CutMix` class is then applied to the batch to generate the augmented images and labels. Finally, **(4)** the original and augmented images are displayed side by side for comparison.\n",
    "\n",
    "*Please note that due to possibility of the shuffle operation resulting in the original order, there's a chance that the images might appear identical in certain instances. If this occurs, consider re-running the operation.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Training with CutMix**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train the model with CutMix, we define the `CutMixClassifier` class, which extends the `Classifier` to incorporate the CutMix augmentation."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "import torch.optim as optim\n",
    "import warmup_scheduler\n",
    "\n",
    "\n",
    "class CutMixClassifier(Classifier):\n",
    "    def __init__(self, model, size=32, beta=1.0, **kwargs):\n",
    "        super().__init__(model, **kwargs)\n",
    "\n",
    "        self.cutmix = CutMix(size=size, beta=beta)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "\n",
    "        augmented_img, labels, rand_labels, lam = self.cutmix(batch) # (1)\n",
    "\n",
    "        output = self.model(augmented_img) # (2)\n",
    "        loss = self.loss(output, labels) * lam + (1 - lam) * self.loss( # (3)\n",
    "            output, rand_labels\n",
    "        )\n",
    "\n",
    "        self.log(\n",
    "            \"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True\n",
    "        )\n",
    "        self.log_metrics(\n",
    "            \"train\",\n",
    "            output,\n",
    "            labels,\n",
    "            on_step=True,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "            logger=True,\n",
    "        )\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = super().configure_optimizers()\n",
    "\n",
    "        base_scheduler = optim.lr_scheduler.CosineAnnealingLR( # (4)\n",
    "            optimizer, T_max=200, eta_min=1e-5\n",
    "        )\n",
    "        scheduler = warmup_scheduler.GradualWarmupScheduler( # (5)\n",
    "            optimizer, multiplier=1.0, total_epoch=5, after_scheduler=base_scheduler\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": scheduler,\n",
    "                \"interval\": \"epoch\",\n",
    "            },\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from warmup_scheduler import GradualWarmupScheduler as GradualWarmup\n",
    "\n",
    "class CutMixClassifier(dl.Classifier):\n",
    "    \"\"\"Classifier with CutMix.\"\"\"\n",
    "\n",
    "    def __init__(self, model, size=32, beta=1.0, **kwargs):\n",
    "        \"\"\"Initialize classifier with CutMix.\"\"\"\n",
    "        super().__init__(model, **kwargs)\n",
    "        self.cutmix = CutMix(size=size, beta=beta)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \"\"\"Perform one training step.\"\"\"\n",
    "        augmented_imgs, labels, rand_labels, mix_ratio = self.cutmix(batch)\n",
    "        pred_labels = self.model(augmented_imgs)\n",
    "        loss = (mix_ratio * self.loss(pred_labels, labels) \n",
    "                + (1 - mix_ratio) * self.loss(pred_labels, rand_labels))\n",
    "\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True,\n",
    "                 prog_bar=True, logger=True)\n",
    "        self.log_metrics(\"train\", pred_labels, labels, on_step=True, \n",
    "                         on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"Configure optimizers.\"\"\"\n",
    "        optimizer = super().configure_optimizers()\n",
    "        base_scheduler = CosineAnnealingLR(optimizer, T_max=200, eta_min=1e-5)\n",
    "        scheduler = GradualWarmup(optimizer, multiplier=1.0, total_epoch=5, \n",
    "                                  after_scheduler=base_scheduler)\n",
    "        return {\"optimizer\": optimizer,\n",
    "                \"lr_scheduler\": {\"scheduler\": scheduler, \"interval\": \"epoch\"}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During training,  **(1)** `CutMixClassifier` first applies CutMix augmentation to the input batch, creating augmented images with their corresponding labels and random labels alongside the mixing coefficient `lam`. **(2)** The model processes the augmented images, and **(3)** the loss is calculated using a combination of the original and random labels weighted by `lam` and `1-lam`, respectively, to reflect the proportion of each image in the augmented mix. \n",
    "\n",
    "To further enhance the performance of the model,  `CutMixClassifier` **(4)** sets up a learning rate schedule starting with a `CosineAnnealingLR` scheduler to adjust the learning rate following a cosine curve, and then **(5)** employs a warm-up phase using `GradualWarmupScheduler`,  which gradually increases the learning rate over the first five epochs to the initial learning rate before transitioning to the cosine annealing schedule. This approach optimizes the learning process, improving training stability and model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we proceed to train the model using the `CutMixClassifier` class. \n",
    "\n",
    "The model is trained for 700 epochs to allow sufficient time for the model to learn from the augmented data and improve its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vit_cutmix = dl.ViT(in_channels=3, image_size=32, patch_size=4, \n",
    "                    hidden_features=[384,] * 7, out_features=10, num_heads=12)\n",
    "vit_cutmix[..., \"attention#-1\"].log_output(\"attention_output\")\n",
    "\n",
    "classifier_cutmix = CutMixClassifier(\n",
    "    model=vit_cutmix, num_classes=10,\n",
    "    optimizer=dl.Adam(lr=1e-3, weight_decay=5e-5, betas=(0.9, 0.999)),\n",
    ").build()\n",
    "trainer_cutmix = dl.Trainer(max_epochs=1)  ### trainer_cutmix = dl.Trainer(max_epochs=700)\n",
    "trainer_cutmix.fit(classifier_cutmix, train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(1)** the `log_output` method is used to log the output of the attention layer in the last transformer block. This will allow us to visualize the attention maps generated by the model during inference for interpretability purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the model has achieved a validation accuracy of approximately 90%, which is a significant improvement over the previous accuracy of 76%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_cutmix.test(classifier_cutmix, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_cutmix.logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Visualizing attention maps**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the model's performance by plotting test images, attention maps and predicted labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.transform import resize\n",
    "\n",
    "samples = random.choices(test_dataset, k=4)\n",
    "imgs, labels = create_batch(samples)\n",
    "\n",
    "classifier_cutmix.model.eval()\n",
    "output = classifier_cutmix(imgs)\n",
    "preds = torch.argmax(output, dim=1)\n",
    "\n",
    "attn_maps = \\\n",
    "    classifier_cutmix.logs[\"attention_output\"][1][:, 0, 1:].reshape(4, 8, 8)\n",
    "\n",
    "fig, axs = plt.subplots(2, 4, figsize=(12, 6))\n",
    "for i, (img, label, pred, attn_map) in enumerate(\n",
    "    zip(imgs, labels, preds, attn_maps)\n",
    "):\n",
    "    img = normalize_image(img).detach().permute(1, 2, 0)\n",
    "\n",
    "    axs[0, i].imshow(img)\n",
    "    axs[0, i].set_title(f\"Label: {test_dataset.classes[label]}\\n\"\n",
    "                        + f\"Pred: {test_dataset.classes[pred]}\")\n",
    "    axs[0, i].axis(\"off\")\n",
    "\n",
    "    resized_attn_map = resize(attn_map.detach().numpy(), (32, 32), \n",
    "                              anti_aliasing=True)\n",
    "    axs[1, i].imshow(img)\n",
    "    axs[1, i].imshow(resized_attn_map, cmap=\"hot\", alpha=0.5)\n",
    "    axs[1, i].set_title(\"Attention Map\")\n",
    "    axs[1, i].axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "raise KeyboardInterrupt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Using a pre-trained ViT model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model achieved a 90% accuracy, which is comparable to the performance levels of CNNs on the CIFAR-10 dataset. However, as the original ViT paper states, this model achieves its best performance when trained on large-scale datasets like ImageNet, which contains millions of images. In such cases, the model can leverage its capacity to learn from vast amounts of data and generalize effectively, outperforming CNNs significantly.\n",
    "\n",
    "To demonstrate this, we will use a pre-trained ViT model that has already learned rich representations from a diverse range of images in the ImageNet dataset. This model can perform well on a variety of tasks. \n",
    "\n",
    "The following loads the pre-trained ViT model and wrap it in a Torch Module that appends a classification head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTModel\n",
    "\n",
    "class PretrainedViTModel(torch.nn.Module):\n",
    "    \"\"\"Pretrained ViT model.\"\"\"\n",
    "\n",
    "    def __init__(self, pretrained_model, output_channels):\n",
    "        \"\"\"Initialize pretrained ViT model.\"\"\"\n",
    "        super().__init__()\n",
    "        self.backbone = ViTModel.from_pretrained(pretrained_model)\n",
    "        hidden_features = self.backbone.config.hidden_size\n",
    "        self.classifier = torch.nn.Linear(hidden_features, output_channels)\n",
    "\n",
    "    def forward(self, imgs):\n",
    "        \"\"\"Perform forward step.\"\"\"\n",
    "        features = self.backbone(imgs).last_hidden_state[:, 0]\n",
    "        return self.classifier(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Images from ImageNet are typically larger than those in CIFAR-10, so we need to resize the images to 224x224 before feeding them to the model. We also normalize the images using the mean and standard deviation values of the ImageNet dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as trans_pt\n",
    "from transformers import ViTImageProcessor\n",
    "\n",
    "pretrained_model = \"google/vit-base-patch16-224-in21k\"\n",
    "processor = ViTImageProcessor.from_pretrained(pretrained_model)\n",
    "\n",
    "image_mean, image_std = processor.image_mean, processor.image_std\n",
    "size = processor.size[\"height\"]\n",
    "\n",
    "train_trans_pt = [\n",
    "    trans_pt.Resize((size, size)), trans_pt.RandomCrop(size, padding=4),\n",
    "    trans_pt.RandomHorizontalFlip(), trans_pt.ToTensor(),\n",
    "    trans_pt.Normalize(image_mean, image_std),\n",
    "]\n",
    "\n",
    "val_trans_pt = [\n",
    "    trans_pt.Resize((size, size)), trans_pt.ToTensor(),\n",
    "    trans_pt.Normalize(image_mean, image_std),\n",
    "]\n",
    "\n",
    "train_dataset_pt = copy.deepcopy(train_dataset)\n",
    "train_dataset_pt.dataset.transform = transforms.Compose(train_trans_pt)\n",
    "val_dataset_pt = copy.deepcopy(val_dataset)\n",
    "val_dataset_pt.dataset.transform = transforms.Compose(val_trans_pt)\n",
    "test_dataset_pt = copy.deepcopy(test_dataset)\n",
    "test_dataset_pt.transform = transforms.Compose(val_trans_pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will redefine the data loaders to accommodate the resizing and normalization of the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader_pt = DataLoader(train_dataset_pt, batch_size=32, shuffle=True)\n",
    "val_dataloader_pt = DataLoader(val_dataset_pt, batch_size=32, shuffle=False)\n",
    "test_dataloader_pt = DataLoader(test_dataset_pt, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and train the model using the `Classifier` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_pt = dl.Classifier(\n",
    "    model=PretrainedViTModel(pretrained_model, output_channels=10), \n",
    "    num_classes=10, optimizer=dl.Adam(lr=2e-5), \n",
    ").create()\n",
    "trainer_pt = dl.Trainer(max_epochs=2)\n",
    "trainer_pt.fit(classifier_pt, train_dataloader_pt, val_dataloader_pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After only two epochs, the pre-trained ViT model achieves an accuracy of approximately 98% on the CIFAR-10 dataset, outperforming the performance of the model trained from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_pt.test(classifier_pt, test_dataloader_pt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py_env_book",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
