{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Attention\n",
    "\n",
    "This notebook provides a complete code example that demonstrates how the attention mechanism works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtaining a matrix representation of sentences\n",
    "\n",
    "Start by writing two sentences ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_sentence = \"The teacher praised a student because she improved\"\n",
    "key_sentence = \"A student asked the teacher to help her improving\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... tokenize the two sentences ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_tokens = query_sentence.split()\n",
    "key_tokens = key_sentence.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(query_tokens)\n",
    "print(key_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... download the GloVe embeddings ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torchvision.datasets.utils import download_url, extract_archive\n",
    "\n",
    "glove_folder = os.path.join(\".\", \".glove_cache\")\n",
    "zip_filepath = os.path.join(glove_folder, \"glove.42B.300d.zip\")\n",
    "if not os.path.exists(glove_folder):\n",
    "    os.makedirs(glove_folder, exist_ok=True)\n",
    "    url = \"https://nlp.stanford.edu/data/glove.42B.300d.zip\"\n",
    "    download_url(url, glove_folder)\n",
    "    extract_archive(zip_filepath, glove_folder)\n",
    "    os.remove(zip_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... implement a function to load the GloVe embeddings ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove_embeddings(glove_file):\n",
    "    \"\"\"Load GloVe embeddings.\"\"\"\n",
    "    glove_embeddings = {}\n",
    "    with open(glove_file, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            glove_embeddings[word] = np.round(\n",
    "                np.asarray(values[1:], dtype='float32'), decimals=6,\n",
    "            )\n",
    "    return glove_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... implement a function to get GloVe embeddings for a vocabulary ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_glove_embeddings(vocab, glove_embeddings, embed_dim):\n",
    "    \"\"\"Get GloVe embeddings for a vocabulary.\"\"\"\n",
    "    embeddings = torch.zeros((len(vocab), embed_dim), dtype=torch.float32)\n",
    "    for i, token in enumerate(vocab):\n",
    "        embedding = glove_embeddings.get(token)\n",
    "        if embedding is None:\n",
    "            embedding = glove_embeddings.get(token.lower())\n",
    "        if embedding is not None:\n",
    "            embeddings[i] = torch.tensor(embedding, dtype=torch.float32)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and calculate the embeddings of the query and key sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "glove_file = os.path.join(glove_folder, \"glove.42B.300d.txt\")\n",
    "glove_embed, embed_dim = load_glove_embeddings(glove_file), 300\n",
    "\n",
    "query_embeddings = get_glove_embeddings(query_tokens, glove_embed, embed_dim)\n",
    "key_embeddings = get_glove_embeddings(key_tokens, glove_embed, embed_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(query_embeddings)\n",
    "print(key_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Dot-Product Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a class to perform the dot-product attention ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deeplay as dl\n",
    "import torch\n",
    "\n",
    "class DotProductAttention(dl.DeeplayModule):\n",
    "    \"\"\"Dot-product attention.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize dot-product attention.\"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, queries, keys, values):\n",
    "        \"\"\"Calculate dot-product attention.\"\"\"\n",
    "        attn_scores = (torch.matmul(queries, keys.transpose(-2, -1))\n",
    "                       / (keys.size(-1) ** 0.5))\n",
    "        attn_matrix = torch.nn.functional.softmax(attn_scores, dim=-1)\n",
    "        attn_output = torch.matmul(attn_matrix, values)\n",
    "        return attn_output, attn_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... calculate attention matrix and attention output ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention = DotProductAttention() \n",
    "attn_output, attn_matrix = attention(\n",
    "    queries=query_embeddings, keys=key_embeddings, values=key_embeddings,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... implement a function to plot an attention matrix ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt \n",
    "from matplotlib.ticker import FixedLocator\n",
    "\n",
    "def plot_attention(query_tokens, key_tokens, attn_matrix):\n",
    "    \"\"\"Plot attention.\"\"\"\n",
    "    fig, ax = plt.subplots()\n",
    "    cax = ax.matshow(attn_matrix, cmap=\"bone\")\n",
    "    fig.colorbar(cax)\n",
    "    ax.xaxis.set_major_locator(FixedLocator(range(len(key_tokens))))\n",
    "    ax.yaxis.set_major_locator(FixedLocator(range(len(query_tokens))))\n",
    "    ax.set_xticklabels(key_tokens, rotation=90)\n",
    "    ax.set_yticklabels(query_tokens)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and use it to plot the attention matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_attention(query_tokens=query_tokens, key_tokens=key_tokens, \n",
    "               attn_matrix=attn_matrix.detach().squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making the Attention Mechanism Trainable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a class to perform a trainable dot-product attention ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deeplay as dl\n",
    "import torch\n",
    "\n",
    "class TrainableAttention(dl.DeeplayModule):\n",
    "    \"\"\"Trainable dot-product attention.\"\"\"\n",
    "    \n",
    "    def __init__(self, num_in_features=300, num_out_features=256):\n",
    "        \"\"\"Initialize trainable dot-product attention.\"\"\"\n",
    "        super().__init__()\n",
    "        self.Wq = torch.nn.Linear(num_in_features, num_out_features)\n",
    "        self.Wk = torch.nn.Linear(num_in_features, num_out_features)\n",
    "        self.Wv = torch.nn.Linear(num_in_features, num_out_features)\n",
    "\n",
    "    def forward(self, queries, keys, values):\n",
    "        \"\"\"Calculate dot-product attention with linear transformations.\"\"\"\n",
    "        Q, K, V = self.Wq(queries), self.Wk(keys), self.Wv(values)\n",
    "        attn_scores = (torch.matmul(Q, K.transpose(-2, -1))\n",
    "                       / (K.size(-1) ** 0.5))\n",
    "        attn_matrix = torch.nn.functional.softmax(attn_scores, dim=-1)\n",
    "        attn_output = torch.matmul(attn_matrix, V)\n",
    "        return attn_output, attn_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... calculate it ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainable_attention = TrainableAttention() \n",
    "trainable_attn_output, trainable_attn_matrix = trainable_attention(\n",
    "    queries=query_embeddings, keys=key_embeddings, values=key_embeddings,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and plot the resulting attention matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_attention(query_tokens=query_tokens, key_tokens=key_tokens, \n",
    "               attn_matrix=trainable_attn_matrix.detach().squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Additive Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a class to perform additive attention ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deeplay as dl\n",
    "import torch\n",
    "\n",
    "class AdditiveAttention(dl.DeeplayModule):\n",
    "    \"\"\"Additive dot-product attention.\"\"\"\n",
    "    \n",
    "    def __init__(self, num_in_features=300, num_out_features=256):\n",
    "        \"\"\"Initialize additive attention.\"\"\"\n",
    "        super().__init__()\n",
    "        self.Wq = torch.nn.Linear(num_in_features, num_out_features)\n",
    "        self.Wk = torch.nn.Linear(num_in_features, num_out_features)\n",
    "        self.Ws = torch.nn.Linear(num_out_features, 1)\n",
    "\n",
    "    def forward(self, queries, keys, values):\n",
    "        \"\"\"Calculate dot-product attention with linear transformations.\"\"\"\n",
    "        Q, K, V = self.Wq(queries), self.Wk(keys), values\n",
    "        attn_scores = self.Ws(torch.tanh(Q + K)).transpose(-2, -1)\n",
    "        attn_matrix = torch.nn.functional.softmax(attn_scores, dim=-1)\n",
    "        attn_output = torch.matmul(attn_matrix, V)\n",
    "        return attn_output, attn_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... calculate it ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "additive_attention = AdditiveAttention() \n",
    "additive_attn_output, additive_attn_matrix = trainable_attention(\n",
    "    queries=query_embeddings, keys=key_embeddings, values=key_embeddings,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and plot the resulting attention matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_attention(query_tokens=query_tokens, key_tokens=key_tokens, \n",
    "               attn_matrix=additive_attn_matrix.detach().squeeze())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py_env_book",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
