{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9183ecbc",
   "metadata": {},
   "source": [
    "# Classification of Anomalous Diffusion\n",
    "\n",
    "We'll develop a recurrent neural network to classify different kinds of motions obeying different anomalous diffusion models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8ad3b7",
   "metadata": {},
   "source": [
    "## Load Anomalous Diffusion Models\n",
    "\n",
    "We load the `datasets_theory()` class from the `andi_datasets` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c73683",
   "metadata": {},
   "outputs": [],
   "source": [
    "from andi_datasets.datasets_theory import datasets_theory\n",
    "\n",
    "AnDi = datasets_theory()\n",
    "model_names = AnDi.avail_models_name\n",
    "model_number = len(model_names)\n",
    "\n",
    "print(f\"{model_number} anomalous diffusion models: {model_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111cdc21",
   "metadata": {},
   "source": [
    "## Simulate Trajectories\n",
    "\n",
    "We now proceed to simulate the necessary trajectories for the dataset. This might take a few minutes. This will generate `N = 4000` trajectories for each anomalous diffusion model, each with random anomalous diffusion coefficients. Each trajectory contains `T = 100` time steps.\n",
    "\n",
    "The dataset contains rows with lenght `2 + D * T`, with the following structure:\n",
    "* First column: model label \n",
    "* Second column: value of the anomalous exponent\n",
    "* Following columns: trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d95e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import concatenate as cat\n",
    "from numpy.random import rand\n",
    "\n",
    "T = 100  # Time steps.\n",
    "N = 4000  # Trajectories per model.\n",
    "D = 2  # Dimensionality.\n",
    "\n",
    "dataset = cat(\n",
    "    (\n",
    "        AnDi.create_dataset(  # CARLO: should we use a create_noisy_diffusion_dataset?\n",
    "            # GIOVANNI: if we want to randomly control the exponent, we should do it in this way and add the 'noisy' part afterwords\n",
    "            T=T + 1,\n",
    "            N_models=1,\n",
    "            exponents=(rand(N) * 0.8) + 0.2,\n",
    "            models=[0, 1],  # attm, ctrw\n",
    "            dimension=D,\n",
    "        ),\n",
    "        AnDi.create_dataset(\n",
    "            T=T + 1,\n",
    "            N_models=1,\n",
    "            exponents=(rand(N) * 1.8) + 0.2,\n",
    "            models=[2, 4],  # fbm, sbm\n",
    "            dimension=D,\n",
    "        ),\n",
    "        AnDi.create_dataset(\n",
    "            T=T + 1,\n",
    "            N_models=1,\n",
    "            exponents=rand(N) + 1,\n",
    "            models=3,  # lw\n",
    "            dimension=D,\n",
    "        ),\n",
    "    ),\n",
    "    axis=0,\n",
    ")\n",
    "\n",
    "print(f\"{dataset.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0127ef57",
   "metadata": {},
   "source": [
    "## adding localization noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5332d2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = AnDi.create_noisy_diffusion_dataset(\n",
    "    dataset, diffusion_coefficients=1.0, T=T + 1, dimension=2\n",
    ")\n",
    "dataset = AnDi.create_noisy_localization_dataset(\n",
    "    dataset, T=T + 1, dimension=2, sigma=0.1\n",
    ")\n",
    "dataset = AnDi.create_noisy_diffusion_dataset(dataset, T=T + 1, dimension=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba83e421",
   "metadata": {},
   "source": [
    "We now shuffle the dataset and extract the model numbers, anomalous exponents, and the x and y components of the trajectories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d474593",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import shuffle\n",
    "\n",
    "shuffle(dataset)\n",
    "\n",
    "models = dataset[:, 0]\n",
    "exponents = dataset[:, 1]\n",
    "trajectories = dataset[:, 2:].reshape(-1, T + 1, D, order=\"F\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54946aa",
   "metadata": {},
   "source": [
    "## Visualize Trajectories\n",
    "We display trajectories corresponding to different models with a similar exponent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd8bdd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from numpy import where\n",
    "\n",
    "fig, axs = plt.subplots(1, model_number, figsize=((15, 3)), tight_layout=True)\n",
    "\n",
    "for m, ax in zip(range(model_number), axs):\n",
    "    idx = where((models == m) & (exponents > 0.95) & (exponents < 1.05))[0][0]\n",
    "    ax.plot(trajectories[idx, :, 0], trajectories[idx, :, 1], alpha=0.75)\n",
    "    ax.set_title(f\"{model_names[m]}, exponent =  {exponents[idx]:.1f}\")\n",
    "    ax.axis(\"equal\")\n",
    "    plt.setp(ax, xlabel=\"x\", ylabel=\"y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8b47c9",
   "metadata": {},
   "source": [
    "## Prepare Data\n",
    "\n",
    "We now prepare the data to be used by for the neural network training. We will use the trajectory displacements, i.e. the difference between positions at consecutive frames, standardized to have mean equal to zero and standard deviation equal to unit. \n",
    "We reshape the displacement time-series according to a time `window` to be used in the recurrent neural network training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28e8e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import diff\n",
    "\n",
    "window = 2\n",
    "n_timesteps = int(T / window)\n",
    "n_features = D * window\n",
    "\n",
    "displ = diff(trajectories, axis=1)\n",
    "displ -= displ.mean(axis=1, keepdims=True)\n",
    "displ /= displ.std(axis=1, keepdims=True)\n",
    "\n",
    "inputs = displ.reshape(displ.shape[0], n_timesteps, n_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390180ab",
   "metadata": {},
   "source": [
    "We then one-hot encode the targets. For this, we first implement two functions, that we add to `fnc_andi.py` ...\n",
    "\n",
    "```python\n",
    "```\n",
    "\n",
    "```python\n",
    "```\n",
    "\n",
    "... and then use them to one-hot encode the anomalous diffusion models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0897d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import argmax, zeros\n",
    "\n",
    "\n",
    "def digits_to_one_hot(digits, n_classes):\n",
    "    \"\"\"Convert an array of integer digits to a 2D one-hot encoded matrix.\"\"\"\n",
    "\n",
    "    one_hot_labels = zeros((digits.shape[0], n_classes))\n",
    "    for idx, digit in enumerate(digits):\n",
    "        one_hot_labels[idx, digit] = 1\n",
    "    return one_hot_labels\n",
    "\n",
    "\n",
    "def one_hot_to_digit(one_hot_labels):\n",
    "    \"\"\"Convert a 2D one-hot encoded matrix to a 1D array of integer digits.\"\"\"\n",
    "\n",
    "    return argmax(one_hot_labels, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3235fa7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from fnc_andi import digits_to_one_hot\n",
    "\n",
    "targets = digits_to_one_hot(models.astype(int), model_number)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0e0e6c",
   "metadata": {},
   "source": [
    "We then convert them into PyTorch tensors ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e9d2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "inputs = torch.tensor(inputs, dtype=torch.float32)\n",
    "targets = torch.tensor(targets, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521129a6",
   "metadata": {},
   "source": [
    "... split them into train, validation, and test datasets ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1578c13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split, TensorDataset\n",
    "\n",
    "[train_ds, val_ds, test_ds] = random_split(\n",
    "    TensorDataset(inputs, targets), [0.6, 0.2, 0.2]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0990fa36",
   "metadata": {},
   "source": [
    "... and create the necessary data loaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584356f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(test_ds, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2bfee10",
   "metadata": {},
   "source": [
    "## Use Simple RNN\n",
    "\n",
    "We start by using a simple RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385b21f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import deeplay as dl\n",
    "\n",
    "\n",
    "rnn = dl.RNN(\n",
    "    n_features,\n",
    "    hidden_features=[100],\n",
    "    out_features=model_number,\n",
    "    rnn_type=\"RNN\",\n",
    "    #    dropout=0.2, doesn't seem to have any effect, check it out\n",
    ")\n",
    "rnn.blocks[0].dropout.configure(p=0.2)\n",
    "# #GIOVANNI: I added blocks[0] otherwise you also change the dropout of the dense layer\n",
    "# Do you want to do that to? I didn't in the original example but no big deal\n",
    "\n",
    "model_rnn = dl.Classifier(\n",
    "    model=rnn,\n",
    "    loss=torch.nn.CrossEntropyLoss(),\n",
    "    optimizer=dl.Adam(lr=0.001),\n",
    ").create()\n",
    "\n",
    "print(model_rnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780fe0d4",
   "metadata": {},
   "source": [
    "We check that the dropout is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864a0c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_rnn.model.blocks[0].blocks[0].dropout.p)\n",
    "# this is just for diagnostic, it can be removed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee869cf",
   "metadata": {},
   "source": [
    "We now proceed to train the neural network ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5793c087",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_rnn = dl.Trainer(max_epochs=30, accelerator=\"auto\")\n",
    "trainer_rnn.fit(model_rnn, train_loader, val_dataloaders=val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f97775",
   "metadata": {},
   "source": [
    "... and test it on the test dataset ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f71e6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_rnn = trainer_rnn.predict(model_rnn, test_loader)\n",
    "preds_rnn = torch.cat(preds_rnn, dim=0).clone().detach()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c259c7f",
   "metadata": {},
   "source": [
    "... and evaluate its performance. For this, we write the `get_groundtruths()` and `accuracy()` functions, save them into `fnc_andi.py` ...\n",
    "\n",
    "```python\n",
    "```\n",
    "\n",
    "```python\n",
    "```\n",
    "\n",
    "... and use it to calculate the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17693d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def get_groundtruths(loader):\n",
    "    \"\"\"Returns the groundtruth targets.\"\"\"\n",
    "\n",
    "    groundtruths = []\n",
    "    for _, target in loader:\n",
    "        groundtruths.append(target)\n",
    "\n",
    "    groundtruths = torch.cat(groundtruths, dim=0)\n",
    "    groundtruths = torch.tensor(groundtruths)\n",
    "\n",
    "    return groundtruths\n",
    "\n",
    "\n",
    "def accuracy(predictions, groundtruths):\n",
    "    \"\"\"Calculate the accuracy.\"\"\"\n",
    "\n",
    "    predictions = one_hot_to_digit(predictions)\n",
    "    groundtruths = one_hot_to_digit(groundtruths)\n",
    "    correct = sum(predictions == groundtruths)\n",
    "    total = predictions.shape[0]\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c66ef5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from fnc_andi import accuracy, get_groundtruths\n",
    "\n",
    "groundtruths = get_groundtruths(test_loader)\n",
    "\n",
    "accuracy_rnn = accuracy(preds_rnn, groundtruths)\n",
    "print(f\"Accuracy Simple RNN: {accuracy_rnn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9296624c",
   "metadata": {},
   "source": [
    "We then plot the confusion matrix.\n",
    "\n",
    "```python\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e165a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from numpy import zeros\n",
    "from seaborn import heatmap, cubehelix_palette\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(predictions, groundtruths):\n",
    "    \"\"\"Calculate and plot confusion matrix.\"\"\"\n",
    "\n",
    "    confusion_matrix = zeros((5, 5))\n",
    "    predictions = one_hot_to_digit(predictions)\n",
    "    groundtruths = one_hot_to_digit(groundtruths)\n",
    "    for pred_idx, gt_idx in zip(predictions, groundtruths):\n",
    "        confusion_matrix[pred_idx, gt_idx] += 1\n",
    "    confusion_matrix *= 100 / confusion_matrix.sum(axis=0, keepdims=1)\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    heatmap(\n",
    "        confusion_matrix,\n",
    "        annot=True,\n",
    "        fmt=\".1f\",\n",
    "        square=True,\n",
    "        cmap=cubehelix_palette(light=0.95, as_cmap=True),\n",
    "        vmax=100,\n",
    "    )\n",
    "    plt.xlabel(\"Predicted model\", fontsize=15)\n",
    "    xlocs, _ = plt.xticks()\n",
    "    plt.xticks(ticks=xlocs, labels=model_names)\n",
    "    plt.ylabel(\"Ground truth model\", fontsize=15)\n",
    "    ylocs, _ = plt.yticks()\n",
    "    plt.yticks(ticks=ylocs, labels=model_names)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb103751",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from fnc_andi import plot_confusion_matrix\n",
    "\n",
    "plot_confusion_matrix(preds_rnn, groundtruths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db63249c",
   "metadata": {},
   "source": [
    "## Use GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3bc0e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn.blocks[0].layer.configure(torch.nn.GRU)\n",
    "model_gru = dl.Classifier(\n",
    "    rnn, loss=torch.nn.CrossEntropyLoss(), optimizer=dl.Adam(lr=0.001)\n",
    ").create()\n",
    "\n",
    "print(model_gru)\n",
    "\n",
    "trainer_gru = dl.Trainer(max_epochs=30, accelerator=\"auto\")\n",
    "trainer_gru.fit(model_gru, train_loader, val_dataloaders=val_loader)\n",
    "\n",
    "preds_gru = trainer_rnn.predict(model_gru, test_loader)\n",
    "preds_gru = torch.cat(preds_gru, dim=0).clone().detach()\n",
    "\n",
    "accuracy_gru = accuracy(preds_gru, groundtruths)\n",
    "print(f\"Accuracy GRU: {accuracy_gru}\")\n",
    "\n",
    "plot_confusion_matrix(preds_gru, groundtruths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05741f84",
   "metadata": {},
   "source": [
    "## Use LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef038bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn.blocks[0].layer.configure(torch.nn.LSTM)\n",
    "rnn.blocks[0].layer.configure(\n",
    "    hidden_size=50\n",
    ")  # To bring the total parameter count more in-line with a simple RNN.\n",
    "rnn.blocks[1].layer.configure(in_features=50)\n",
    "model_lstm = dl.Classifier(\n",
    "    rnn, loss=torch.nn.CrossEntropyLoss(), optimizer=dl.Adam(lr=0.01)\n",
    ").create()\n",
    "\n",
    "print(model_lstm)\n",
    "\n",
    "trainer_lstm = dl.Trainer(max_epochs=30, accelerator=\"auto\")\n",
    "trainer_lstm.fit(model_lstm, train_loader, val_dataloaders=val_loader)\n",
    "\n",
    "preds_lstm = trainer_rnn.predict(model_lstm, test_loader)\n",
    "preds_lstm = torch.cat(preds_lstm, dim=0).clone().detach()\n",
    "\n",
    "accuracy_lstm = accuracy(preds_lstm, groundtruths)\n",
    "print(f\"Accuracy LSTM: {accuracy_lstm}\")\n",
    "\n",
    "plot_confusion_matrix(preds_lstm, groundtruths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889fdc92",
   "metadata": {},
   "source": [
    "## Use Bidirectional LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569ed159",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = dl.RNN(\n",
    "    n_features,\n",
    "    hidden_features=[50],\n",
    "    out_features=model_number,\n",
    "    # dropout=0.2, doesn't produce any effect\n",
    "    rnn_type=\"LSTM\",\n",
    "    bidirectional=True,\n",
    ")\n",
    "rnn.blocks[0].dropout.configure(\n",
    "    p=0.2\n",
    ")  # HENRIK, BENJAMIN: I need to add this to see the dropout in the printout\n",
    "model_bi = dl.Classifier(\n",
    "    rnn, loss=torch.nn.CrossEntropyLoss(), optimizer=dl.Adam(lr=0.01)\n",
    ").create()\n",
    "\n",
    "print(model_bi)\n",
    "\n",
    "trainer_bi = dl.Trainer(max_epochs=30, accelerator=\"auto\")\n",
    "trainer_bi.fit(model_bi, train_loader, val_dataloaders=val_loader)\n",
    "\n",
    "preds_bi = trainer_rnn.predict(model_bi, test_loader)\n",
    "preds_bi = torch.cat(preds_bi, dim=0).clone().detach()\n",
    "\n",
    "accuracy_bi = accuracy(preds_bi, groundtruths)\n",
    "print(f\"Accuracy Bidirectional LSTM: {accuracy_bi}\")\n",
    "\n",
    "plot_confusion_matrix(preds_bi, groundtruths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd00267",
   "metadata": {},
   "source": [
    "## Adding layers to LSTM (overfitting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbecc6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = dl.RecurrentNeuralNetwork(  # HENRIK, BENJAMIN: why not RNN like above?\n",
    "    n_features,\n",
    "    hidden_features=[100],\n",
    "    out_features=30,\n",
    ")\n",
    "# GIOVANNI: RNN and RecurrentNeuralNetwork are two different things. RNN is a 'model' (a RNN + dense), RecurrentNeuralNetwork is a 'component' (only RNN), more general and more modifiable.\n",
    "\n",
    "\n",
    "rnn.blocks.dropout.configure(\n",
    "    p=0.2\n",
    ")  # HENRIK, BENJAMIN: I need to add this to see the dropout in the printout\n",
    "rnn.blocks.layer.configure(\n",
    "    torch.nn.LSTM\n",
    ")  # HENRIK, BENJAMIN: this shouldn't be needed because it's above, right?\n",
    "\n",
    "dense_top = dl.MultiLayerPerceptron(\n",
    "    in_features=None,\n",
    "    hidden_features=[15],\n",
    "    out_features=model_number,\n",
    ")\n",
    "dense_top.blocks.dropout.configure(\n",
    "    p=0.2\n",
    ")  # HENRIK, BENJAMIN: can this be put as an argument?\n",
    "# GIOVANNI: I'm not sure we really need the dropout here.\n",
    "\n",
    "dense_rnn = dl.Sequential(rnn, dense_top)\n",
    "model_overfit = dl.Classifier(\n",
    "    dense_rnn,\n",
    "    loss=torch.nn.CrossEntropyLoss(),\n",
    "    optimizer=dl.Adam(lr=0.01),\n",
    ").create()\n",
    "\n",
    "print(model_overfit)\n",
    "\n",
    "trainer_overfit = dl.Trainer(max_epochs=30, accelerator=\"auto\")\n",
    "trainer_overfit.fit(model_overfit, train_loader, val_dataloaders=val_loader)\n",
    "\n",
    "preds_overfit = trainer_rnn.predict(model_overfit, test_loader)\n",
    "preds_overfit = torch.cat(preds_overfit, dim=0).clone().detach()\n",
    "\n",
    "accuracy_overfit = accuracy(preds_overfit, groundtruths)\n",
    "print(f\"Accuracy Bidirectional LSTM: {accuracy_overfit}\")\n",
    "\n",
    "plot_confusion_matrix(preds_overfit, groundtruths)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
