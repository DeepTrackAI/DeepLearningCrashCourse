{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Temperatures using Recurrent Neural Networks\n",
    "\n",
    "This notebook provides you with a complete code example that uses different kinds of recurrent neural networks to predict the temperature in the Jena Climate Dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading the Jena Climate Dataset\n",
    "\n",
    "This dataset contains timeseries recorded at the weather station of the Max Planck Institute for Biogeochemistry in Jena, Germany. Itâ€™s made up of 14 different quantities recorded every 10 minutes over several years, from January 1st 2009 to December 31st 2016.\n",
    "\n",
    "Load the Jena Climante Dataset ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataframe = pd.read_csv(\"jena_climate_2009_2016.csv\", index_col=0)\n",
    "data = dataframe.values\n",
    "header = dataframe.columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and display its header and first few elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataframe.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "start, days, daily_samples = 0, 14, 144\n",
    "end = start + daily_samples * days\n",
    "\n",
    "fig, axes = plt.subplots(7, 2, figsize=(16, 12), sharex=True)\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    ax.plot(np.arange(start, end), data[start:end, i], label=header[i])\n",
    "    ax.set_xlim(start, end)\n",
    "    ax.tick_params(axis=\"both\", which=\"major\", labelsize=16)\n",
    "    ax.legend(fontsize=20)\n",
    "    \n",
    "    for day in range(1, days):\n",
    "        ax.axvline(x=start + daily_samples * day,\n",
    "                   color=\"gray\", linestyle=\"--\", linewidth=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the Data\n",
    "\n",
    "Reshape the data in a format ready to train a recurrent neural network ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples, n_features = data.shape[0], data.shape[1]\n",
    "past_seq = 2 * daily_samples\n",
    "lag = 72\n",
    "temp_idx = 1  # Temperature (Celsius) index.\n",
    "\n",
    "in_sequences, targets = [], []\n",
    "for i in np.random.permutation(range(0, n_samples - past_seq - lag, \n",
    "                                     daily_samples)):\n",
    "    in_sequences.append(data[i:i + past_seq, :])\n",
    "    targets.append(data[i + past_seq + lag:i + past_seq + lag + 1, temp_idx])\n",
    "in_sequences, targets = np.asarray(in_sequences), np.asarray(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... check the input shape ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(in_sequences.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... check the output shape ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(targets.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... splitting the data ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deeptrack as dt\n",
    "\n",
    "sources = dt.sources.Source(inputs=in_sequences, targets=targets)\n",
    "train_sources, val_sources = dt.sources.random_split(sources, [0.8, 0.2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... standardize the data ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "train_mean = np.mean([src[\"inputs\"] for src in train_sources], axis=(0, 1))\n",
    "train_std = np.std([src[\"inputs\"] for src in train_sources], axis=(0, 1))\n",
    "\n",
    "inputs_pipeline = (dt.Value(sources.inputs - train_mean) / train_std \n",
    "                   >> dt.pytorch.ToTensor(dtype=torch.float))\n",
    "targets_pipeline = (dt.Value(sources.targets - train_mean[temp_idx]) \n",
    "                    / train_std[temp_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and define the data loaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataset = dt.pytorch.Dataset(inputs_pipeline & targets_pipeline, \n",
    "                                   inputs=train_sources)\n",
    "val_dataset = dt.pytorch.Dataset(inputs_pipeline & targets_pipeline, \n",
    "                                 inputs=val_sources)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing a Common-Sense Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature = data[:, temp_idx]\n",
    "benchmark_celsius = np.mean(\n",
    "    np.abs(\n",
    "        temperature[daily_samples + lag :: daily_samples] \n",
    "        - temperature[lag : -(daily_samples - lag) : daily_samples]\n",
    "    )\n",
    ")\n",
    "benchmark = benchmark_celsius / train_std[temp_idx]\n",
    "\n",
    "print(f\"Benchmark Celsius: {benchmark_celsius}\")\n",
    "print(f\"Normalized Benchmark: {benchmark}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determining on Which Device to Perform the Computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device():\n",
    "    \"\"\"Select device where to perform computations.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda:0\")\n",
    "    elif torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\")\n",
    "    else:\n",
    "        return torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing a Simple Recurrent Neural Network\n",
    "\n",
    "Define a recurrent neural network in PyTorch ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "rnn = nn.RNN(input_size=in_sequences.shape[2], hidden_size=2, batch_first=True)\n",
    "fc = nn.Linear(in_features=2, out_features=1)\n",
    "rnn.to(device); fc.to(device);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... train and validate it ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.L1Loss()  # MAE Loss.\n",
    "parameter_list = list(rnn.parameters()) + list(fc.parameters())\n",
    "optimizer = torch.optim.Adam(parameter_list, lr=0.001)\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "train_losses, val_losses = [], []\n",
    "for epoch in range(epochs):\n",
    "    train_loss = 0.0\n",
    "    for in_sequences, targets in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        in_sequences, targets = in_sequences.to(device), targets.to(device)\n",
    "        hidden_sequences, _ = rnn(in_sequences)  # RNN layer.\n",
    "        last_hidden_states = hidden_sequences[:, -1, :]  # Last hidden states.\n",
    "\n",
    "        predictions = fc(last_hidden_states)  # Linear layer.\n",
    "        \n",
    "        loss = criterion(predictions, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    train_losses.append(train_loss / len(train_loader))\n",
    "    print(f\"Epoch {epoch} Training Loss: {train_losses[-1]:.4f}\")\n",
    "    \n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for in_sequences, targets in val_loader:\n",
    "            in_sequences, targets = in_sequences.to(device), targets.to(device)\n",
    "            rnn_outputs, _ = rnn(in_sequences)\n",
    "            rnn_outputs = rnn_outputs[:, -1, :]\n",
    "            predictions = fc(rnn_outputs)\n",
    "            \n",
    "            loss = criterion(predictions, targets)\n",
    "            val_loss += loss.item()\n",
    "    val_losses.append(val_loss / len(val_loader))\n",
    "    print(f\"Epoch {epoch} Validation Loss: {val_losses[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and plot the training and validation losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training(epochs, train_losses, val_losses, benchmark):\n",
    "    \"\"\"Plot the training and validation losses.\"\"\"\n",
    "    plt.plot(range(epochs), train_losses, label=\"Training Loss\")\n",
    "    plt.plot(range(epochs), val_losses, \"--\", label=\"Validation Loss\")\n",
    "    plt.plot([0, epochs - 1], [benchmark, benchmark], \":k\", label=\"Benchmark\")\n",
    "    plt.xlabel(\"Epoch\"), plt.xlim([0, epochs - 1])\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training(epochs, train_losses, val_losses, benchmark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing the Recurrent Neural Network in a More Compact Form with Deeplay\n",
    "\n",
    "Define a recurrent neural network in deeplay ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deeplay as dl\n",
    "\n",
    "rnn_dl = dl.RecurrentModel(\n",
    "    in_features=n_features, \n",
    "    hidden_features=[2],\n",
    "    out_features=1,\n",
    "    rnn_type=\"RNN\",\n",
    ")\n",
    "rnn_simple = dl.Regressor(rnn_dl, optimizer=dl.Adam(lr=0.001)).create()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = dl.Trainer(max_epochs=epochs, accelerator=\"auto\")\n",
    "trainer.fit(rnn_simple, train_loader, val_loader)\n",
    "\n",
    "train_losses = trainer.history.history[\"train_loss_epoch\"][\"value\"]\n",
    "val_losses = trainer.history.history[\"val_loss_epoch\"][\"value\"][1:]\n",
    "plot_training(epochs, train_losses, val_losses, benchmark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking Multiple Recurrent Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deeplay as dl\n",
    "\n",
    "rnn_dl = dl.RecurrentModel(\n",
    "    in_features=n_features, \n",
    "    hidden_features=[16, 16, 16],\n",
    "    out_features=1,\n",
    "    rnn_type=\"RNN\",\n",
    ")\n",
    "rnn_stacked = dl.Regressor(rnn_dl, optimizer=dl.Adam(lr=0.0001)).create()\n",
    "\n",
    "trainer = dl.Trainer(max_epochs=epochs, accelerator=\"auto\")\n",
    "trainer.fit(rnn_stacked, train_loader, val_loader)\n",
    "\n",
    "train_losses = trainer.history.history[\"train_loss_epoch\"][\"value\"]\n",
    "val_losses = trainer.history.history[\"val_loss_epoch\"][\"value\"][1:]\n",
    "plot_training(epochs, train_losses, val_losses, benchmark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Gated Recurrent Units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deeplay as dl\n",
    "\n",
    "gru_dl = dl.RecurrentModel(\n",
    "    in_features=n_features, \n",
    "    hidden_features=[8, 8, 8],\n",
    "    out_features=1,\n",
    "    rnn_type=\"GRU\",\n",
    "    dropout=0.2,\n",
    ")\n",
    "gru_stacked = dl.Regressor(gru_dl, optimizer=dl.Adam(lr=0.001)).create()\n",
    "\n",
    "trainer = dl.Trainer(max_epochs=epochs, accelerator=\"auto\")\n",
    "trainer.fit(gru_stacked, train_loader, val_loader)\n",
    "\n",
    "train_losses = trainer.history.history[\"train_loss_epoch\"][\"value\"]\n",
    "val_losses = trainer.history.history[\"val_loss_epoch\"][\"value\"][1:]\n",
    "plot_training(epochs, train_losses, val_losses, benchmark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Long Short-Term Memory Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deeplay as dl\n",
    "\n",
    "lstm_dl = dl.RecurrentModel(\n",
    "    in_features=n_features, \n",
    "    hidden_features=[8, 8, 8],\n",
    "    out_features=1,\n",
    "    rnn_type=\"LSTM\",\n",
    "    dropout=0.4,\n",
    ")\n",
    "lstm_stacked = dl.Regressor(lstm_dl, optimizer=dl.Adam(lr=0.001)).create()\n",
    "\n",
    "trainer = dl.Trainer(max_epochs=epochs, accelerator=\"auto\")\n",
    "trainer.fit(lstm_stacked, train_loader, val_loader)\n",
    "\n",
    "train_losses = trainer.history.history[\"train_loss_epoch\"][\"value\"]\n",
    "val_losses = trainer.history.history[\"val_loss_epoch\"][\"value\"][1:]\n",
    "plot_training(epochs, train_losses, val_losses, benchmark)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py_env_dlcc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
