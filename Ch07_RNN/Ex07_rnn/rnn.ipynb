{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Recurrent Neural Networks\n",
        "\n",
        "We'll introduce the concept of recurrent neural networks, which explicitly model the time-dependency of their inputs to facilitate the analysis of sequential data. We'll demonstrate this functionality using a weather forecasting dataset in a classic time-series analysis approach.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFVpPnIxlo4w"
      },
      "source": [
        "# Implement a Simple Recurrent Neural Network\n",
        "\n",
        "We start with an example implementing the feed-forward pass of a recurrent neural network in NumPy. To keep things simple, we assume that the signal is a scalar value containing a single feature, corresponding to a binary signal. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can use a recurrent relation to implement a feedback comb filter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "input_series = [0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
        "output_series = []\n",
        "\n",
        "state = 0\n",
        "U = 1 / 2\n",
        "V = 1 - U\n",
        "for input_data in input_series:\n",
        "    hidden = U * input_data + V * state\n",
        "    output_data = hidden\n",
        "    state = output_data\n",
        "    output_series.append(output_data)\n",
        "\n",
        "print(f\"Input Series: {[f'{x:.2f}' for x in input_series]}\")\n",
        "print(f\"Output Series: {[f'{x:.2f}' for x in output_series]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "\n",
        "plt.plot(input_series, label=\"input signal\")\n",
        "plt.plot(output_series, label=\"output signal\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We modify the previous code to implement a more general recurrent neuron."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def sigmoid(x):\n",
        "    \"\"\"Simple implementation of sigmoid function.\"\"\"\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "\n",
        "input_series = [0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0]\n",
        "output_series = []\n",
        "\n",
        "state = 0\n",
        "U, V, W, b = np.random.normal(size=4)\n",
        "for input_data in input_series:\n",
        "    hidden = sigmoid(U * input_data + V * state + b)\n",
        "    output_data = sigmoid(hidden * W)\n",
        "    state = output_data\n",
        "    output_series.append(output_data)\n",
        "\n",
        "print(f\"Input Series: {[f'{x:.2f}' for x in input_series]}\")\n",
        "print(f\"Output Series: {[f'{x:.2f}' for x in output_series]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Weather Dataset\n",
        "\n",
        "We download the uncompressed dataset of the Jena Climate Dataset from https://s3.amazonaws.com/keras-datasets/jena_climate_2009_2016.csv.zip.\n",
        "\n",
        "More information on this dataset can be found at https://www.bgc-jena.mpg.de/wetter/."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from torchvision.datasets.utils import download_url, _extract_zip\n",
        "\n",
        "# import ssl\n",
        "\n",
        "# ssl._create_default_https_context = ssl._create_unverified_context\n",
        "\n",
        "dataset_path = os.path.join(\".\", \"weather_dataset\")\n",
        "if not os.path.exists(dataset_path):\n",
        "    url = \"https://s3.amazonaws.com/keras-datasets/\" \"jena_climate_2009_2016.csv.zip\"\n",
        "    download_url(url, \".\")\n",
        "    _extract_zip(\"jena_climate_2009_2016.csv.zip\", dataset_path, None)\n",
        "    os.remove(\"jena_climate_2009_2016.csv.zip\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This dataset contains 14 weather measurements in a `.csv` file. Their meaning is shown in the header."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import csv\n",
        "# import numpy as np\n",
        "\n",
        "# filename = os.path.join(dataset_path, \"jena_climate_2009_2016.csv\")\n",
        "# with open(filename) as f:\n",
        "#     reader = csv.reader(f)\n",
        "#     header = next(reader)\n",
        "#     data = []\n",
        "#     for row in reader:\n",
        "#         data.append(row[1:])  # The first value (\"Date Time\") is excluded.\n",
        "#     data = np.asarray(data).astype(float)\n",
        "\n",
        "# print(header)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "filename = os.path.join(dataset_path, \"jena_climate_2009_2016.csv\")\n",
        "dataframe = pd.read_csv(filename, index_col=0)\n",
        "data = dataframe.values\n",
        "header = dataframe.columns.tolist()\n",
        "dataframe.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualize Data\n",
        "\n",
        "We now visualize the 14 measured features of the dataset. For this, we use the `plot_data()` function.\n",
        "\n",
        "```python\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def plot_data(data, header, start=0, samples_per_cycle=144, cycles=14):\n",
        "    \"\"\"Plot data highlighting periodic cycles.\"\"\"\n",
        "\n",
        "    fig, axes = plt.subplots(7, 2, figsize=(16, 12), sharex=True)\n",
        "\n",
        "    for i, ax in enumerate(axes.flatten()):\n",
        "        ax.plot(np.arange(start, start + samples_per_cycle * cycles),\n",
        "                data[start:start + samples_per_cycle * cycles, i], \n",
        "                label=header[i])\n",
        "        ax.legend()\n",
        "        ax.set_xlim(start, start + samples_per_cycle * cycles)\n",
        "        \n",
        "        for cycle in range(1, cycles):\n",
        "            ax.axvline(x=start + cycle * samples_per_cycle, \n",
        "                    color=\"gray\", linestyle=\"--\", linewidth=0.5)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from for_rnn import plot_data\n",
        "\n",
        "daily_samples = 144\n",
        "n_days = 14\n",
        "\n",
        "plot_data(data, header, samples_per_cycle=daily_samples, cycles=n_days)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preprocess Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We start by removing the mean from the data and normalizing them by their standard deviation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data -= data.mean(axis=0)\n",
        "data /= data.std(axis=0)\n",
        "\n",
        "plot_data(data, header, samples_per_cycle=daily_samples, cycles=n_days)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now preprocess the data into time series to input into the neural network model, that is, they need to be of shape (batch size, number of samples, number of feature)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "n_samples = data.shape[0]\n",
        "n_features = data.shape[1]\n",
        "pred_window = 288  # Length of the sequences to be fed to the RNN.\n",
        "pred_lag = pred_window + 72\n",
        "# How many time_steps ahead in time the RNN should predict temperature. 1 very easy. 18 used for course. 144 = 1 day\n",
        "temp_idx = 2  # Temperature index.\n",
        "\n",
        "inputs, targets = [], []\n",
        "for i in np.random.permutation(range(0, n_samples - pred_lag, pred_window)):\n",
        "    inputs.append(data[i : i + pred_window, :])\n",
        "    targets.append(data[i + pred_lag : i + pred_lag + 1, temp_idx])\n",
        "inputs = np.asarray(inputs)\n",
        "targets = np.asarray(targets)\n",
        "\n",
        "print(inputs.shape)\n",
        "print(targets.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We start by creating a dataset and the respective data loaders."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import random_split, TensorDataset, DataLoader\n",
        "\n",
        "# Convert to PyTorch tensors.\n",
        "inputs_tensor = torch.tensor(inputs, dtype=torch.float32)\n",
        "targets_tensor = torch.tensor(targets, dtype=torch.float32)\n",
        "\n",
        "# Splitting the dataset into training and validation\n",
        "dataset = TensorDataset(inputs_tensor, targets_tensor)\n",
        "train_dataset, val_dataset = random_split(dataset, [0.8, 0.2])\n",
        "\n",
        "# Creating data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=False)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Creating a Common-Sense Benchmark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We then create a baseline common-sense benchmark, generate benchmark for comparison, i.e., predict $T_t = T_{t-1}$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "temperature = data[:, temp_idx]\n",
        "benchmark = np.mean(\n",
        "    np.abs(\n",
        "        temperature[daily_samples::daily_samples]\n",
        "        - temperature[:-daily_samples:daily_samples]\n",
        "    )\n",
        ")\n",
        "\n",
        "print(benchmark)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5PsOxRzRt-W"
      },
      "source": [
        "## Implement PyTorch RNN Model\n",
        "\n",
        "This example introduces the `RNN` module in PyTorch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UH7b-1TGlkTm"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "# Define the RNN and Linear layers separately.\n",
        "rnn = nn.RNN(input_size=inputs.shape[2], hidden_size=2, batch_first=True)\n",
        "fc = nn.Linear(in_features=2, out_features=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now train the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "criterion = nn.L1Loss()  # MAE loss.\n",
        "optimizer = torch.optim.Adam(\n",
        "    list(rnn.parameters()) + list(fc.parameters()), lr=0.001\n",
        ")  # Optimizer.\n",
        "epochs = 100\n",
        "\n",
        "train_losses, val_losses = [], []\n",
        "for epoch in range(epochs):\n",
        "    # Training.\n",
        "    running_loss = 0\n",
        "    for inputs, targets in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        rnn_out, _ = rnn(inputs)  # RNN layer.\n",
        "        rnn_out = rnn_out[:, -1, :]  # Select the last output for each sequence.\n",
        "        outputs = fc(rnn_out)  # Linear layer.\n",
        "\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    train_losses.append(running_loss / len(train_loader))\n",
        "    print(f\"Epoch {epoch} Training Loss: {train_losses[-1]:.4f}\")\n",
        "\n",
        "    # Validation.\n",
        "    val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in val_loader:\n",
        "            rnn_out, _ = rnn(inputs)  # RNN layer.\n",
        "            rnn_out = rnn_out[:, -1, :]  # Selecting the last output for each sequence.\n",
        "            outputs = fc(rnn_out)  # Linear layer.\n",
        "\n",
        "            loss = criterion(outputs, targets)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    val_losses.append(val_loss / len(val_loader))\n",
        "    print(f\"Epoch {epoch} Validation Loss: {val_losses[-1]:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We then validate the model for which we write the `plot_training()` function.\n",
        "\n",
        "```python\n",
        "def plot_training(epochs, train_losses, val_losses, benchmark):\n",
        "    \"\"\"Plot the training and validation losses.\"\"\"\n",
        "    \n",
        "    plt.plot(range(epochs), train_losses, label=\"Training Loss\")\n",
        "    plt.plot(range(epochs), val_losses, label=\"Validation Loss\")\n",
        "    plt.plot([0, epochs - 1], [benchmark, benchmark], \n",
        "            linestyle=\"--\", color=\"k\", label=\"Benchmark\")\n",
        "    plt.legend()\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.xlim([0, epochs - 1])\n",
        "    plt.show()\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from for_rnn import plot_training\n",
        "\n",
        "plot_training(epochs, train_losses, val_losses, benchmark)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Implement the RNN in Deeplay\n",
        "\n",
        "We now implement the RNN in deeplay."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "class myRegressor(dl.Application):\n",
        "    model: torch.nn.Module\n",
        "    loss: torch.nn.Module\n",
        "    metrics: list\n",
        "    optimizer: dl.Optimizer\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        model: torch.nn.Module,\n",
        "        loss: torch.nn.Module = torch.nn.L1Loss(),\n",
        "        optimizer=None,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super().__init__(loss=loss, **kwargs)\n",
        "\n",
        "        self.model = model\n",
        "        self.optimizer = optimizer or Adam(lr=1e-3)\n",
        "\n",
        "        @self.optimizer.params\n",
        "        def params(self):\n",
        "            return self.model.parameters()\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        x, y = self.val_preprocess(batch)\n",
        "        y_hat = self(x)\n",
        "        loss = self.compute_loss(y_hat, y)\n",
        "        if not isinstance(loss, dict):\n",
        "            loss = {\"loss\": loss}\n",
        "\n",
        "        for name, v in loss.items():\n",
        "            self.log(\n",
        "                f\"val_{name}\",\n",
        "                v,\n",
        "                on_step=True,\n",
        "                on_epoch=True,\n",
        "                prog_bar=False,\n",
        "                logger=True,\n",
        "            )\n",
        "        self.log_metrics(\n",
        "            \"val\",\n",
        "            y_hat,\n",
        "            y,\n",
        "            on_step=True,\n",
        "            on_epoch=True,\n",
        "            prog_bar=False,\n",
        "            logger=True,\n",
        "        )\n",
        "        return sum(loss.values())\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "myRegressor(\n",
            "  (loss): L1Loss()\n",
            "  (train_metrics): MetricCollection,\n",
            "    prefix=train\n",
            "  )\n",
            "  (val_metrics): MetricCollection,\n",
            "    prefix=val\n",
            "  )\n",
            "  (test_metrics): MetricCollection,\n",
            "    prefix=test\n",
            "  )\n",
            "  (model): RecurrentModel(\n",
            "    (blocks): LayerList(\n",
            "      (0): RecurrentBlock(\n",
            "        (layer): RNN(14, 2, batch_first=True)\n",
            "        (activation): Identity()\n",
            "        (normalization): Identity()\n",
            "        (dropout): RecurrentDropout(\n",
            "          (dropout): Dropout(p=0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (1): LayerActivationNormalizationDropout(\n",
            "        (layer): Linear(in_features=2, out_features=1, bias=True)\n",
            "        (activation): Identity()\n",
            "        (normalization): Identity()\n",
            "        (dropout): Dropout(p=0, inplace=False)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (optimizer): Adam[Adam](lr=0.001)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "import deeplay as dl\n",
        "\n",
        "rnn = dl.RecurrentModel(\n",
        "    in_features=14,\n",
        "    hidden_features=[2],\n",
        "    out_features=1,\n",
        "    rnn_type=\"RNN\",\n",
        "    dropout=0,\n",
        ")\n",
        "rnn_simple = myRegressor(rnn, optimizer=dl.Adam(lr=0.001)).create()\n",
        "\n",
        "print(rnn_simple)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We write a `TrainingHistory` class to implement a custom callback for use with the PyTorch Lightning framework, designed to track and store the training and validation losses at the end of each epoch. We save also this one in `for_rnn.py`.\n",
        "\n",
        "```python\n",
        "class TrainingHistory(Callback):\n",
        "    \"\"\"Callback to record the training and validation losses.\"\"\"\n",
        "\n",
        "    def on_train_start(self, trainer, pl_module):\n",
        "        \"\"\"Initialize lists to store loss values.\"\"\"\n",
        "        self.train_losses = []\n",
        "        self.val_losses = []\n",
        "\n",
        "    def on_train_epoch_end(self, trainer, pl_module):\n",
        "        \"\"\"Store training and validation losses.\"\"\"\n",
        "\n",
        "        train_loss = trainer.callback_metrics.get(\n",
        "            \"train_loss\"\n",
        "        )  # Retrieve the training loss from the current epoch.\n",
        "        if train_loss is not None:\n",
        "            self.train_losses.append(train_loss.item())\n",
        "\n",
        "        val_loss = trainer.callback_metrics.get(\n",
        "            \"val_loss\"\n",
        "        )  # Retrieve the validation loss from the current epoch.\n",
        "        if val_loss is not None:\n",
        "            self.val_losses.append(val_loss.item()))\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "GPU available: False, used: False\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "\n",
            "  | Name          | Type             | Params\n",
            "---------------------------------------------------\n",
            "0 | loss          | L1Loss           | 0     \n",
            "1 | train_metrics | MetricCollection | 0     \n",
            "2 | val_metrics   | MetricCollection | 0     \n",
            "3 | test_metrics  | MetricCollection | 0     \n",
            "4 | model         | RecurrentModel   | 39    \n",
            "5 | optimizer     | Adam             | 0     \n",
            "---------------------------------------------------\n",
            "39        Trainable params\n",
            "0         Non-trainable params\n",
            "39        Total params\n",
            "0.000     Total estimated model params size (MB)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c5e61efbc5ea44d4b6b7852bcea75a30",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/841602/Documents/GitHub/Environments/deeplay_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
            "/Users/841602/Documents/GitHub/Environments/deeplay_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
            "/Users/841602/Documents/GitHub/Environments/deeplay_env/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (37) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "58a75c1424f14f2580ed5da5373ecd10",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d220819b60ed4bd5abd9dc84031e0db8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6dca064df06f466fa98354b339f06056",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "10e141a29d0b431e8f7cbcc84070bb0b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e04853a9d9ee413b89ad3fc6b53b3600",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e8cb6846535647ee97216598a47d08d2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e781b694086a4cc4ab2ad85867158bc6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b1739c18e7db4a92992418db5e35bf47",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4d168c9627cc48c1a06ed21d198bd654",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "228e81ee530b460f876741f99d3d85d1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "121b78756f0f4974b2ab6c2a8c30fb04",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1fa1979bd25945aca1b2205731d7293a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dce50dadc047456eba19a4eabc99a5b4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c0dce1ec83ed40729276dff52d068fd9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ba132fa4dbbb49a98cf7f5752eec5ee2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/841602/Documents/GitHub/Environments/deeplay_env/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py:54: Detected KeyboardInterrupt, attempting graceful shutdown...\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "x and y must have same first dimension, but have shapes (100,) and (14,)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[21], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m trainer \u001b[38;5;241m=\u001b[39m dl\u001b[38;5;241m.\u001b[39mTrainer(max_epochs\u001b[38;5;241m=\u001b[39mepochs, callbacks\u001b[38;5;241m=\u001b[39m[history], accelerator\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m trainer\u001b[38;5;241m.\u001b[39mfit(rnn_simple, train_loader, val_loader)\n\u001b[0;32m----> 7\u001b[0m \u001b[43mplot_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_losses\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mval_losses\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbenchmark\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Documents/GitHub/DeepLearningCrashCourse/Ch07_RNN/Ex07_rnn/fnc_rnn.py:37\u001b[0m, in \u001b[0;36mplot_training\u001b[0;34m(epochs, train_losses, val_losses, benchmark)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplot_training\u001b[39m(epochs, train_losses, val_losses, benchmark):\n\u001b[1;32m     35\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Plot the training and validation losses.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m     \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_losses\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTraining Loss\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m     plt\u001b[38;5;241m.\u001b[39mplot(\u001b[38;5;28mrange\u001b[39m(epochs), val_losses, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation Loss\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     39\u001b[0m     plt\u001b[38;5;241m.\u001b[39mplot(\n\u001b[1;32m     40\u001b[0m         [\u001b[38;5;241m0\u001b[39m, epochs \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m     41\u001b[0m         [benchmark, benchmark],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     44\u001b[0m         label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBenchmark\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     45\u001b[0m     )\n",
            "File \u001b[0;32m~/Documents/GitHub/Environments/deeplay_env/lib/python3.10/site-packages/matplotlib/pyplot.py:3578\u001b[0m, in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3570\u001b[0m \u001b[38;5;129m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[38;5;241m.\u001b[39mplot)\n\u001b[1;32m   3571\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplot\u001b[39m(\n\u001b[1;32m   3572\u001b[0m     \u001b[38;5;241m*\u001b[39margs: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m ArrayLike \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3576\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3577\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[Line2D]:\n\u001b[0;32m-> 3578\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgca\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3579\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3580\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscalex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscalex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3581\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscaley\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscaley\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3582\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m}\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3583\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3584\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Documents/GitHub/Environments/deeplay_env/lib/python3.10/site-packages/matplotlib/axes/_axes.py:1721\u001b[0m, in \u001b[0;36mAxes.plot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1478\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1479\u001b[0m \u001b[38;5;124;03mPlot y versus x as lines and/or markers.\u001b[39;00m\n\u001b[1;32m   1480\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1718\u001b[0m \u001b[38;5;124;03m(``'green'``) or hex strings (``'#008000'``).\u001b[39;00m\n\u001b[1;32m   1719\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1720\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m cbook\u001b[38;5;241m.\u001b[39mnormalize_kwargs(kwargs, mlines\u001b[38;5;241m.\u001b[39mLine2D)\n\u001b[0;32m-> 1721\u001b[0m lines \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_lines(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, data\u001b[38;5;241m=\u001b[39mdata, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)]\n\u001b[1;32m   1722\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m lines:\n\u001b[1;32m   1723\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_line(line)\n",
            "File \u001b[0;32m~/Documents/GitHub/Environments/deeplay_env/lib/python3.10/site-packages/matplotlib/axes/_base.py:303\u001b[0m, in \u001b[0;36m_process_plot_var_args.__call__\u001b[0;34m(self, axes, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m    301\u001b[0m     this \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    302\u001b[0m     args \u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m--> 303\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_plot_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    304\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mambiguous_fmt_datakey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mambiguous_fmt_datakey\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Documents/GitHub/Environments/deeplay_env/lib/python3.10/site-packages/matplotlib/axes/_base.py:499\u001b[0m, in \u001b[0;36m_process_plot_var_args._plot_args\u001b[0;34m(self, axes, tup, kwargs, return_kwargs, ambiguous_fmt_datakey)\u001b[0m\n\u001b[1;32m    496\u001b[0m     axes\u001b[38;5;241m.\u001b[39myaxis\u001b[38;5;241m.\u001b[39mupdate_units(y)\n\u001b[1;32m    498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n\u001b[0;32m--> 499\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx and y must have same first dimension, but \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    500\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhave shapes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    501\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m y\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m    502\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx and y can be no greater than 2D, but have \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    503\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshapes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (100,) and (14,)"
          ]
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAi4AAAGiCAYAAADA0E3hAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/SrBM8AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAcw0lEQVR4nO3db2zdVf3A8U/b0VsItEzn2m0WKyiiAhturBYkiKk2gUz3wDjBbHPhj+AkuEZlY7CK6DoRyKIrLkwQH6ibEDDGLUOsLgapWdjWBGSDwMBNYwsT184iLWu/vweG+qvrYLf0z077eiX3wY7n3O+5Hkbf3H8tyLIsCwCABBSO9QYAAI6VcAEAkiFcAIBkCBcAIBnCBQBIhnABAJIhXACAZAgXACAZwgUASIZwAQCSkXe4/OEPf4h58+bF9OnTo6CgIH75y1++5Zpt27bFRz7ykcjlcvG+970v7r///iFsFQCY6PIOl66urpg5c2Y0NTUd0/wXXnghLrvssrjkkkuitbU1vvrVr8ZVV10VjzzySN6bBQAmtoK380sWCwoK4uGHH4758+cfdc6NN94Ymzdvjqeeeqp/7POf/3wcPHgwtm7dOtRLAwAT0KSRvkBLS0vU1tYOGKurq4uvfvWrR13T3d0d3d3d/X/u6+uLV155Jd75zndGQUHBSG0VABhGWZbFoUOHYvr06VFYODxvqx3xcGlra4vy8vIBY+Xl5dHZ2Rn//ve/48QTTzxiTWNjY9x6660jvTUAYBTs378/3v3udw/LfY14uAzFihUror6+vv/PHR0dcdppp8X+/fujtLR0DHcGAByrzs7OqKysjFNOOWXY7nPEw6WioiLa29sHjLW3t0dpaemgz7ZERORyucjlckeMl5aWChcASMxwvs1jxL/HpaamJpqbmweMPfroo1FTUzPSlwYAxpm8w+Vf//pXtLa2Rmtra0T85+POra2tsW/fvoj4z8s8ixYt6p9/7bXXxt69e+Mb3/hG7NmzJ+6+++74xS9+EcuWLRueRwAATBh5h8sTTzwR5513Xpx33nkREVFfXx/nnXderFq1KiIi/v73v/dHTETEe9/73ti8eXM8+uijMXPmzLjzzjvjRz/6UdTV1Q3TQwAAJoq39T0uo6WzszPKysqio6PDe1wAIBEj8fPb7yoCAJIhXACAZAgXACAZwgUASIZwAQCSIVwAgGQIFwAgGcIFAEiGcAEAkiFcAIBkCBcAIBnCBQBIhnABAJIhXACAZAgXACAZwgUASIZwAQCSIVwAgGQIFwAgGcIFAEiGcAEAkiFcAIBkCBcAIBnCBQBIhnABAJIhXACAZAgXACAZwgUASIZwAQCSIVwAgGQIFwAgGcIFAEiGcAEAkiFcAIBkCBcAIBnCBQBIhnABAJIhXACAZAgXACAZwgUASIZwAQCSIVwAgGQIFwAgGcIFAEiGcAEAkiFcAIBkCBcAIBnCBQBIhnABAJIhXACAZAgXACAZwgUASIZwAQCSIVwAgGQIFwAgGcIFAEiGcAEAkiFcAIBkCBcAIBnCBQBIhnABAJIhXACAZAgXACAZQwqXpqamqKqqipKSkqiuro7t27e/6fy1a9fGBz7wgTjxxBOjsrIyli1bFq+99tqQNgwATFx5h8umTZuivr4+GhoaYufOnTFz5syoq6uLl156adD5P/vZz2L58uXR0NAQu3fvjnvvvTc2bdoUN91009vePAAwseQdLnfddVdcffXVsWTJkvjQhz4U69evj5NOOinuu+++Qec//vjjceGFF8YVV1wRVVVV8alPfSouv/zyt3yWBgDgf+UVLj09PbFjx46ora397x0UFkZtbW20tLQMuuaCCy6IHTt29IfK3r17Y8uWLXHppZce9Trd3d3R2dk54AYAMCmfyQcOHIje3t4oLy8fMF5eXh579uwZdM0VV1wRBw4ciI997GORZVkcPnw4rr322jd9qaixsTFuvfXWfLYGAEwAI/6pom3btsXq1avj7rvvjp07d8ZDDz0Umzdvjttuu+2oa1asWBEdHR39t/3794/0NgGABOT1jMuUKVOiqKgo2tvbB4y3t7dHRUXFoGtuueWWWLhwYVx11VUREXHOOedEV1dXXHPNNbFy5cooLDyynXK5XORyuXy2BgBMAHk941JcXByzZ8+O5ubm/rG+vr5obm6OmpqaQde8+uqrR8RJUVFRRERkWZbvfgGACSyvZ1wiIurr62Px4sUxZ86cmDt3bqxduza6urpiyZIlERGxaNGimDFjRjQ2NkZExLx58+Kuu+6K8847L6qrq+O5556LW265JebNm9cfMAAAxyLvcFmwYEG8/PLLsWrVqmhra4tZs2bF1q1b+9+wu2/fvgHPsNx8881RUFAQN998c/ztb3+Ld73rXTFv3rz4zne+M3yPAgCYEAqyBF6v6ezsjLKysujo6IjS0tKx3g4AcAxG4ue331UEACRDuAAAyRAuAEAyhAsAkAzhAgAkQ7gAAMkQLgBAMoQLAJAM4QIAJEO4AADJEC4AQDKECwCQDOECACRDuAAAyRAuAEAyhAsAkAzhAgAkQ7gAAMkQLgBAMoQLAJAM4QIAJEO4AADJEC4AQDKECwCQDOECACRDuAAAyRAuAEAyhAsAkAzhAgAkQ7gAAMkQLgBAMoQLAJAM4QIAJEO4AADJEC4AQDKECwCQDOECACRDuAAAyRAuAEAyhAsAkAzhAgAkQ7gAAMkQLgBAMoQLAJAM4QIAJEO4AADJEC4AQDKECwCQDOECACRDuAAAyRAuAEAyhAsAkAzhAgAkQ7gAAMkQLgBAMoQLAJAM4QIAJEO4AADJEC4AQDKECwCQDOECACRDuAAAyRAuAEAyhhQuTU1NUVVVFSUlJVFdXR3bt29/0/kHDx6MpUuXxrRp0yKXy8WZZ54ZW7ZsGdKGAYCJa1K+CzZt2hT19fWxfv36qK6ujrVr10ZdXV0888wzMXXq1CPm9/T0xCc/+cmYOnVqPPjggzFjxoz4y1/+Eqeeeupw7B8AmEAKsizL8llQXV0d559/fqxbty4iIvr6+qKysjKuv/76WL58+RHz169fH9/73vdiz549ccIJJwxpk52dnVFWVhYdHR1RWlo6pPsAAEbXSPz8zuulop6entixY0fU1tb+9w4KC6O2tjZaWloGXfOrX/0qampqYunSpVFeXh5nn312rF69Onp7e496ne7u7ujs7BxwAwDIK1wOHDgQvb29UV5ePmC8vLw82traBl2zd+/eePDBB6O3tze2bNkSt9xyS9x5553x7W9/+6jXaWxsjLKysv5bZWVlPtsEAMapEf9UUV9fX0ydOjXuueeemD17dixYsCBWrlwZ69evP+qaFStWREdHR/9t//79I71NACABeb05d8qUKVFUVBTt7e0Dxtvb26OiomLQNdOmTYsTTjghioqK+sc++MEPRltbW/T09ERxcfERa3K5XORyuXy2BgBMAHk941JcXByzZ8+O5ubm/rG+vr5obm6OmpqaQddceOGF8dxzz0VfX1//2LPPPhvTpk0bNFoAAI4m75eK6uvrY8OGDfGTn/wkdu/eHdddd110dXXFkiVLIiJi0aJFsWLFiv751113Xbzyyitxww03xLPPPhubN2+O1atXx9KlS4fvUQAAE0Le3+OyYMGCePnll2PVqlXR1tYWs2bNiq1bt/a/YXffvn1RWPjfHqqsrIxHHnkkli1bFueee27MmDEjbrjhhrjxxhuH71EAABNC3t/jMhZ8jwsApGfMv8cFAGAsCRcAIBnCBQBIhnABAJIhXACAZAgXACAZwgUASIZwAQCSIVwAgGQIFwAgGcIFAEiGcAEAkiFcAIBkCBcAIBnCBQBIhnABAJIhXACAZAgXACAZwgUASIZwAQCSIVwAgGQIFwAgGcIFAEiGcAEAkiFcAIBkCBcAIBnCBQBIhnABAJIhXACAZAgXACAZwgUASIZwAQCSIVwAgGQIFwAgGcIFAEiGcAEAkiFcAIBkCBcAIBnCBQBIhnABAJIhXACAZAgXACAZwgUASIZwAQCSIVwAgGQIFwAgGcIFAEiGcAEAkiFcAIBkCBcAIBnCBQBIhnABAJIhXACAZAgXACAZwgUASIZwAQCSIVwAgGQIFwAgGcIFAEiGcAEAkiFcAIBkCBcAIBnCBQBIxpDCpampKaqqqqKkpCSqq6tj+/btx7Ru48aNUVBQEPPnzx/KZQGACS7vcNm0aVPU19dHQ0ND7Ny5M2bOnBl1dXXx0ksvvem6F198Mb72ta/FRRddNOTNAgATW97hctddd8XVV18dS5YsiQ996EOxfv36OOmkk+K+++476pre3t74whe+ELfeemucfvrpb3mN7u7u6OzsHHADAMgrXHp6emLHjh1RW1v73zsoLIza2tpoaWk56rpvfetbMXXq1LjyyiuP6TqNjY1RVlbWf6usrMxnmwDAOJVXuBw4cCB6e3ujvLx8wHh5eXm0tbUNuuaxxx6Le++9NzZs2HDM11mxYkV0dHT03/bv35/PNgGAcWrSSN75oUOHYuHChbFhw4aYMmXKMa/L5XKRy+VGcGcAQIryCpcpU6ZEUVFRtLe3Dxhvb2+PioqKI+Y///zz8eKLL8a8efP6x/r6+v5z4UmT4plnnokzzjhjKPsGACagvF4qKi4ujtmzZ0dzc3P/WF9fXzQ3N0dNTc0R888666x48skno7W1tf/26U9/Oi655JJobW313hUAIC95v1RUX18fixcvjjlz5sTcuXNj7dq10dXVFUuWLImIiEWLFsWMGTOisbExSkpK4uyzzx6w/tRTT42IOGIcAOCt5B0uCxYsiJdffjlWrVoVbW1tMWvWrNi6dWv/G3b37dsXhYW+kBcAGH4FWZZlY72Jt9LZ2RllZWXR0dERpaWlY70dAOAYjMTPb0+NAADJEC4AQDKECwCQDOECACRDuAAAyRAuAEAyhAsAkAzhAgAkQ7gAAMkQLgBAMoQLAJAM4QIAJEO4AADJEC4AQDKECwCQDOECACRDuAAAyRAuAEAyhAsAkAzhAgAkQ7gAAMkQLgBAMoQLAJAM4QIAJEO4AADJEC4AQDKECwCQDOECACRDuAAAyRAuAEAyhAsAkAzhAgAkQ7gAAMkQLgBAMoQLAJAM4QIAJEO4AADJEC4AQDKECwCQDOECACRDuAAAyRAuAEAyhAsAkAzhAgAkQ7gAAMkQLgBAMoQLAJAM4QIAJEO4AADJEC4AQDKECwCQDOECACRDuAAAyRAuAEAyhAsAkAzhAgAkQ7gAAMkQLgBAMoQLAJAM4QIAJEO4AADJEC4AQDKECwCQjCGFS1NTU1RVVUVJSUlUV1fH9u3bjzp3w4YNcdFFF8XkyZNj8uTJUVtb+6bzAQCOJu9w2bRpU9TX10dDQ0Ps3LkzZs6cGXV1dfHSSy8NOn/btm1x+eWXx+9///toaWmJysrK+NSnPhV/+9vf3vbmAYCJpSDLsiyfBdXV1XH++efHunXrIiKir68vKisr4/rrr4/ly5e/5fre3t6YPHlyrFu3LhYtWjTonO7u7uju7u7/c2dnZ1RWVkZHR0eUlpbms10AYIx0dnZGWVnZsP78zusZl56entixY0fU1tb+9w4KC6O2tjZaWlqO6T5effXVeP311+Md73jHUec0NjZGWVlZ/62ysjKfbQIA41Re4XLgwIHo7e2N8vLyAePl5eXR1tZ2TPdx4403xvTp0wfEz/9asWJFdHR09N/279+fzzYBgHFq0mhebM2aNbFx48bYtm1blJSUHHVeLpeLXC43ijsDAFKQV7hMmTIlioqKor29fcB4e3t7VFRUvOnaO+64I9asWRO//e1v49xzz81/pwDAhJfXS0XFxcUxe/bsaG5u7h/r6+uL5ubmqKmpOeq622+/PW677bbYunVrzJkzZ+i7BQAmtLxfKqqvr4/FixfHnDlzYu7cubF27dro6uqKJUuWRETEokWLYsaMGdHY2BgREd/97ndj1apV8bOf/Syqqqr63wtz8sknx8knnzyMDwUAGO/yDpcFCxbEyy+/HKtWrYq2traYNWtWbN26tf8Nu/v27YvCwv8+kfPDH/4wenp64rOf/eyA+2loaIhvfvObb2/3AMCEkvf3uIyFkfgcOAAwssb8e1wAAMaScAEAkiFcAIBkCBcAIBnCBQBIhnABAJIhXACAZAgXACAZwgUASIZwAQCSIVwAgGQIFwAgGcIFAEiGcAEAkiFcAIBkCBcAIBnCBQBIhnABAJIhXACAZAgXACAZwgUASIZwAQCSIVwAgGQIFwAgGcIFAEiGcAEAkiFcAIBkCBcAIBnCBQBIhnABAJIhXACAZAgXACAZwgUASIZwAQCSIVwAgGQIFwAgGcIFAEiGcAEAkiFcAIBkCBcAIBnCBQBIhnABAJIhXACAZAgXACAZwgUASIZwAQCSIVwAgGQIFwAgGcIFAEiGcAEAkiFcAIBkCBcAIBnCBQBIhnABAJIhXACAZAgXACAZwgUASIZwAQCSIVwAgGQIFwAgGcIFAEiGcAEAkiFcAIBkDClcmpqaoqqqKkpKSqK6ujq2b9/+pvMfeOCBOOuss6KkpCTOOeec2LJly5A2CwBMbHmHy6ZNm6K+vj4aGhpi586dMXPmzKirq4uXXnpp0PmPP/54XH755XHllVfGrl27Yv78+TF//vx46qmn3vbmAYCJpSDLsiyfBdXV1XH++efHunXrIiKir68vKisr4/rrr4/ly5cfMX/BggXR1dUVv/71r/vHPvrRj8asWbNi/fr1g16ju7s7uru7+//c0dERp512Wuzfvz9KS0vz2S4AMEY6OzujsrIyDh48GGVlZcNyn5PymdzT0xM7duyIFStW9I8VFhZGbW1ttLS0DLqmpaUl6uvrB4zV1dXFL3/5y6Nep7GxMW699dYjxisrK/PZLgBwHPjHP/4xNuFy4MCB6O3tjfLy8gHj5eXlsWfPnkHXtLW1DTq/ra3tqNdZsWLFgNg5ePBgvOc974l9+/YN2wNnaN6oZ89+jT1ncfxwFscX53H8eOMVk3e84x3Ddp95hctoyeVykcvljhgvKyvzD+FxorS01FkcJ5zF8cNZHF+cx/GjsHD4PsSc1z1NmTIlioqKor29fcB4e3t7VFRUDLqmoqIir/kAAEeTV7gUFxfH7Nmzo7m5uX+sr68vmpubo6amZtA1NTU1A+ZHRDz66KNHnQ8AcDR5v1RUX18fixcvjjlz5sTcuXNj7dq10dXVFUuWLImIiEWLFsWMGTOisbExIiJuuOGGuPjii+POO++Myy67LDZu3BhPPPFE3HPPPcd8zVwuFw0NDYO+fMTochbHD2dx/HAWxxfncfwYibPI++PQERHr1q2L733ve9HW1hazZs2K73//+1FdXR0RER//+Mejqqoq7r///v75DzzwQNx8883x4osvxvvf//64/fbb49JLLx22BwEATAxDChcAgLHgdxUBAMkQLgBAMoQLAJAM4QIAJOO4CZempqaoqqqKkpKSqK6uju3bt7/p/AceeCDOOuusKCkpiXPOOSe2bNkySjsd//I5iw0bNsRFF10UkydPjsmTJ0dtbe1bnh3HLt+/F2/YuHFjFBQUxPz580d2gxNIvmdx8ODBWLp0aUybNi1yuVyceeaZ/j01TPI9i7Vr18YHPvCBOPHEE6OysjKWLVsWr7322ijtdvz6wx/+EPPmzYvp06dHQUHBm/4Owjds27YtPvKRj0Qul4v3ve99Az6BfMyy48DGjRuz4uLi7L777sv+/Oc/Z1dffXV26qmnZu3t7YPO/+Mf/5gVFRVlt99+e/b0009nN998c3bCCSdkTz755CjvfPzJ9yyuuOKKrKmpKdu1a1e2e/fu7Itf/GJWVlaW/fWvfx3lnY8/+Z7FG1544YVsxowZ2UUXXZR95jOfGZ3NjnP5nkV3d3c2Z86c7NJLL80ee+yx7IUXXsi2bduWtba2jvLOx598z+KnP/1plsvlsp/+9KfZCy+8kD3yyCPZtGnTsmXLlo3yzsefLVu2ZCtXrsweeuihLCKyhx9++E3n7927NzvppJOy+vr67Omnn85+8IMfZEVFRdnWrVvzuu5xES5z587Nli5d2v/n3t7ebPr06VljY+Og8z/3uc9ll1122YCx6urq7Etf+tKI7nMiyPcs/tfhw4ezU045JfvJT34yUlucMIZyFocPH84uuOCC7Ec/+lG2ePFi4TJM8j2LH/7wh9npp5+e9fT0jNYWJ4x8z2Lp0qXZJz7xiQFj9fX12YUXXjii+5xojiVcvvGNb2Qf/vCHB4wtWLAgq6ury+taY/5SUU9PT+zYsSNqa2v7xwoLC6O2tjZaWloGXdPS0jJgfkREXV3dUedzbIZyFv/r1Vdfjddff31YfxPoRDTUs/jWt74VU6dOjSuvvHI0tjkhDOUsfvWrX0VNTU0sXbo0ysvL4+yzz47Vq1dHb2/vaG17XBrKWVxwwQWxY8eO/peT9u7dG1u2bPElqGNguH52j/lvhz5w4ED09vZGeXn5gPHy8vLYs2fPoGva2toGnd/W1jZi+5wIhnIW/+vGG2+M6dOnH/EPJ/kZylk89thjce+990Zra+so7HDiGMpZ7N27N373u9/FF77whdiyZUs899xz8eUvfzlef/31aGhoGI1tj0tDOYsrrrgiDhw4EB/72Mciy7I4fPhwXHvttXHTTTeNxpb5f472s7uzszP+/e9/x4knnnhM9zPmz7gwfqxZsyY2btwYDz/8cJSUlIz1diaUQ4cOxcKFC2PDhg0xZcqUsd7OhNfX1xdTp06Ne+65J2bPnh0LFiyIlStXxvr168d6axPOtm3bYvXq1XH33XfHzp0746GHHorNmzfHbbfdNtZbY4jG/BmXKVOmRFFRUbS3tw8Yb29vj4qKikHXVFRU5DWfYzOUs3jDHXfcEWvWrInf/va3ce65547kNieEfM/i+eefjxdffDHmzZvXP9bX1xcREZMmTYpnnnkmzjjjjJHd9Dg1lL8X06ZNixNOOCGKior6xz74wQ9GW1tb9PT0RHFx8YjuebwaylnccsstsXDhwrjqqqsiIuKcc86Jrq6uuOaaa2LlypVRWOi/30fL0X52l5aWHvOzLRHHwTMuxcXFMXv27Ghubu4f6+vri+bm5qipqRl0TU1NzYD5ERGPPvroUedzbIZyFhERt99+e9x2222xdevWmDNnzmhsddzL9yzOOuusePLJJ6O1tbX/9ulPfzouueSSaG1tjcrKytHc/rgylL8XF154YTz33HP98RgR8eyzz8a0adNEy9swlLN49dVXj4iTN4Iy86v6RtWw/ezO733DI2Pjxo1ZLpfL7r///uzpp5/OrrnmmuzUU0/N2trasizLsoULF2bLly/vn//HP/4xmzRpUnbHHXdku3fvzhoaGnwcepjkexZr1qzJiouLswcffDD7+9//3n87dOjQWD2EcSPfs/hfPlU0fPI9i3379mWnnHJK9pWvfCV75plnsl//+tfZ1KlTs29/+9tj9RDGjXzPoqGhITvllFOyn//859nevXuz3/zmN9kZZ5yRfe5znxurhzBuHDp0KNu1a1e2a9euLCKyu+66K9u1a1f2l7/8JcuyLFu+fHm2cOHC/vlvfBz661//erZ79+6sqakp3Y9DZ1mW/eAHP8hOO+20rLi4OJs7d272pz/9qf9/u/jii7PFixcPmP+LX/wiO/PMM7Pi4uLswx/+cLZ58+ZR3vH4lc9ZvOc978ki4ohbQ0PD6G98HMr378X/J1yGV75n8fjjj2fV1dVZLpfLTj/99Ow73/lOdvjw4VHe9fiUz1m8/vrr2Te/+c3sjDPOyEpKSrLKysrsy1/+cvbPf/5z9Dc+zvz+978f9N//b/z/v3jx4uziiy8+Ys2sWbOy4uLi7PTTT89+/OMf533dgizzXBkAkIYxf48LAMCxEi4AQDKECwCQDOECACRDuAAAyRAuAEAyhAsAkAzhAgAkQ7gAAMkQLgBAMoQLAJCM/wM9kKRvAVrZIAAAAABJRU5ErkJggg==",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from for_rnn import TrainingHistory\n",
        "\n",
        "history = TrainingHistory()\n",
        "trainer = dl.Trainer(max_epochs=epochs, callbacks=[history], accelerator=\"auto\")\n",
        "\n",
        "trainer.fit(rnn_simple, train_loader, val_loader)\n",
        "plot_training(epochs, history.train_losses, history.val_losses, benchmark)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Stacked RNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TT6m6j4Rv800"
      },
      "outputs": [],
      "source": [
        "rnn = dl.RecurrentModel(\n",
        "    in_features=n_features,\n",
        "    hidden_features=[32, 32, 32, 32],\n",
        "    out_features=1,\n",
        "    rnn_type=\"RNN\",\n",
        ")\n",
        "stacked_rnn = dl.Regressor(rnn, optimizer=dl.Adam(lr=0.001)).create()\n",
        "\n",
        "print(stacked_rnn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "history = TrainingHistory()\n",
        "trainer = dl.Trainer(max_epochs=epochs, callbacks=[history], accelerator=\"auto\")\n",
        "trainer.fit(stacked_rnn, train_loader, val_dataloaders=val_loader)\n",
        "plot_training(epochs, history.train_losses, history.val_losses, benchmark)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Stacked GRU"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "id": "t3dmlkwZfxE9"
      },
      "source": [
        "rnn_gru = dl.RNN(\n",
        "    in_features=n_features,\n",
        "    hidden_features=[32, 32, 32, 32],\n",
        "    out_features=1,\n",
        "    #rnn_type=\"RNN\",  # HENRIK: Can this just be set up here as \"GRU\"?\n",
        ")\n",
        "rnn_gru.blocks[:-1].layer.configure(torch.nn.GRU)\n",
        "model_gru = dl.Regressor(rnn_gru).create()  # HENRIK: why no optimizer?\n",
        "\n",
        "print(model_gru)\n",
        "\n",
        "history_gru = TrainingHistory()\n",
        "\n",
        "trainer_gru = dl.Trainer(\n",
        "    max_epochs=epochs, \n",
        "    callbacks=[history_gru],\n",
        "    accelerator=\"auto\",\n",
        ")\n",
        "trainer_gru.fit(model_gru, train_loader, val_dataloaders=val_loader)\n",
        "\n",
        "plot_training(\n",
        "    epochs,\n",
        "    history_gru.train_losses, \n",
        "    history_gru.val_losses,\n",
        "    benchmark,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "rnn = dl.RecurrentModel(\n",
        "    in_features=n_features,\n",
        "    hidden_features=[32, 32, 32, 32],\n",
        "    out_features=1,\n",
        "    rnn_type=\"GRU\",\n",
        ")\n",
        "gru = dl.Regressor(rnn, optimizer=dl.Adam(lr=0.001)).create()\n",
        "\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "history = TrainingHistory()\n",
        "trainer = dl.Trainer(max_epochs=epochs, callbacks=[history], accelerator=\"auto\")\n",
        "trainer.fit(gru, train_loader, val_dataloaders=val_loader)\n",
        "plot_training(epochs, history.train_losses, history.val_losses, benchmark)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Stacked LSTM"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "id": "u-kH-Nf7hfSs"
      },
      "source": [
        "rnn_lstm = dl.RNN(\n",
        "    in_features=n_features,\n",
        "    hidden_features=[32, 32, 32, 32],\n",
        "    out_features=1,\n",
        "    rnn_type=\"RNN\",  # HENRIK: Can this just be set up here as \"LSTM\"?\n",
        ")\n",
        "rnn_lstm.blocks[:-1].layer.configure(torch.nn.LSTM)\n",
        "model_lstm = dl.Regressor(rnn_lstm).create()\n",
        "\n",
        "print(model_lstm)\n",
        "\n",
        "history_lstm = TrainingHistory()\n",
        "\n",
        "trainer_lstm = dl.Trainer(\n",
        "    max_epochs=epochs, \n",
        "    callbacks=[history_lstm],\n",
        "    accelerator=\"auto\",\n",
        ")\n",
        "trainer_lstm.fit(model_lstm, train_loader, val_dataloaders=val_loader)\n",
        "\n",
        "plot_training(\n",
        "    epochs,\n",
        "    history_lstm.train_losses, \n",
        "    history_lstm.val_losses,\n",
        "    benchmark,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "rnn = dl.RNN(\n",
        "    in_features=n_features,\n",
        "    hidden_features=[32, 32, 32, 32],\n",
        "    out_features=1,\n",
        "    rnn_type=\"LSTM\",\n",
        ")\n",
        "model = dl.Regressor(rnn, optimizer=dl.Adam(lr=0.001)).create()\n",
        "\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "history = TrainingHistory()\n",
        "trainer = dl.Trainer(max_epochs=epochs, callbacks=[history], accelerator=\"auto\")\n",
        "trainer.fit(model, train_loader, val_dataloaders=val_loader)\n",
        "plot_training(epochs, history.train_losses, history.val_losses, benchmark)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Introducing Dropout\n",
        "\n",
        "We now introduce dropout to prevent overfitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "rnn_dropout = dl.RNN(\n",
        "    in_features=n_features,\n",
        "    hidden_features=[32, 32, 32, 32],\n",
        "    out_features=1,\n",
        "    rnn_type=\"RNN\",  # HENRIK: Can this just be set up here as \"LSTM\"?\n",
        ")\n",
        "rnn_dropout.blocks[:-1].layer.configure(torch.nn.LSTM)\n",
        "rnn_dropout.blocks.dropout.configure(p=0.2)\n",
        "model_dropout = dl.Regressor(rnn_dropout).create()\n",
        "\n",
        "print(model_dropout)\n",
        "\n",
        "history_dropout = TrainingHistory()\n",
        "\n",
        "trainer_dropout = dl.Trainer(\n",
        "    max_epochs=epochs,\n",
        "    callbacks=[history_dropout],\n",
        "    accelerator=\"auto\",\n",
        ")\n",
        "trainer_dropout.fit(model_dropout, train_loader, val_dataloaders=val_loader)\n",
        "\n",
        "plot_training(\n",
        "    epochs,\n",
        "    history_dropout.train_losses,\n",
        "    history_dropout.val_losses,\n",
        "    benchmark,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Implementing a Preprocessor\n",
        "\n",
        "We now introduce a dense preprocessor in feature space."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5cxClS-5_Oto"
      },
      "outputs": [],
      "source": [
        "preprocessor = dl.MultiLayerPerceptron(\n",
        "    in_features=None, hidden_features=[32, 32], out_features=32\n",
        ")\n",
        "preprocessor.dropout.configure(p=0.2)\n",
        "\n",
        "\n",
        "class Reshape(nn.Module):  # HENRIK, shouldn't this be deeplay layer?\n",
        "    \"\"\"Custom reshape layer.\"\"\"\n",
        "\n",
        "    def __init__(self, new_shape):\n",
        "        \"\"\"Custom reshape layer.\"\"\"\n",
        "        super(Reshape, self).__init__()\n",
        "        self.new_shape = new_shape\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Define the forward pass for the reshape layer.\"\"\"\n",
        "        return x.view(self.new_shape)\n",
        "\n",
        "\n",
        "rnn_preprocessed = dl.Sequential(\n",
        "    torch.nn.Flatten(\n",
        "        start_dim=0, end_dim=1\n",
        "    ),  # HENRIK, shouldn't this be included in a deeplay layer?\n",
        "    preprocessor,\n",
        "    Reshape(\n",
        "        (-1, pred_window, 32)\n",
        "    ),  # HENRIK, shouldn't this be included in a deeplay layer?\n",
        "    dl.RNN(\n",
        "        in_features=32,\n",
        "        hidden_features=[32, 32, 32, 32],\n",
        "        out_features=1,\n",
        "        dropout=0.2,  # HENRIK: This doesn't seem to work. Please, double-check.\n",
        "        rnn_type=\"LSTM\",\n",
        "    ),\n",
        ")\n",
        "model_preprocessed = dl.Regressor(rnn_preprocessed).create()\n",
        "\n",
        "print(model_preprocessed)\n",
        "\n",
        "history_preprocessed = TrainingHistory()\n",
        "\n",
        "trainer_preprocessed = dl.Trainer(\n",
        "    max_epochs=epochs,\n",
        "    callbacks=[history_preprocessed],\n",
        "    accelerator=\"auto\",\n",
        ")\n",
        "trainer_preprocessed.fit(model_preprocessed, train_loader, val_dataloaders=val_loader)\n",
        "\n",
        "plot_training(\n",
        "    epochs,\n",
        "    history_preprocessed.train_losses,\n",
        "    history_preprocessed.val_losses,\n",
        "    benchmark,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
