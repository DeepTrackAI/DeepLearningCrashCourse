{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Recurrent Neural Networks\n",
        "\n",
        "We'll introduce the concept of recurrent neural networks, which explicitly model the time-dependency of their inputs to facilitate the analysis of sequential data. We'll demonstrate this functionality using a weather forecasting dataset in a classic time-series analysis approach.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFVpPnIxlo4w"
      },
      "source": [
        "# Implement a Simple Recurrent Neural Network\n",
        "\n",
        "We start with an example implementing the feed-forward pass of a recurrent neural network in NumPy. To keep things simple, we assume that the signal is a scalar value containing a single feature, corresponding to a binary signal. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can use a recurrent relation to implement a feedback \"comb\" filter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "input_series = [0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
        "output_series = []\n",
        "\n",
        "state = 0\n",
        "U = 1 / 2\n",
        "V = 1 - U\n",
        "for input_data in input_series:\n",
        "    hidden = U * input_data + V * state\n",
        "    output_data = hidden\n",
        "    state = output_data\n",
        "    output_series.append(output_data)\n",
        "\n",
        "print(f\"Input Series: {[f'{x:.2f}' for x in input_series]}\")\n",
        "print(f\"Output Series: {[f'{x:.2f}' for x in output_series]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "\n",
        "plt.plot(input_series, label=\"input signal\")\n",
        "plt.plot(output_series, label=\"output signal\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We modify the previous code to implement a more general recurrent neuron."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def sigmoid(x):\n",
        "    \"\"\"Simple implementation of sigmoid function.\"\"\"\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "\n",
        "input_series = [0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0]\n",
        "output_series = []\n",
        "\n",
        "state = 0\n",
        "U, V, W, b = np.random.normal(size=4)\n",
        "for input_data in input_series:\n",
        "    hidden = sigmoid(U * input_data + V * state + b)\n",
        "    output_data = sigmoid(hidden * W)\n",
        "    state = output_data\n",
        "    output_series.append(output_data)\n",
        "\n",
        "print(f\"Input Series: {[f'{x:.2f}' for x in input_series]}\")\n",
        "print(f\"Output Series: {[f'{x:.2f}' for x in output_series]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Weather Dataset\n",
        "\n",
        "We download the uncompressed dataset of the Jena Climate Dataset from https://s3.amazonaws.com/keras-datasets/jena_climate_2009_2016.csv.zip.\n",
        "\n",
        "More information on this dataset can be found at https://www.bgc-jena.mpg.de/wetter/."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from torchvision.datasets.utils import download_url, _extract_zip\n",
        "\n",
        "dataset_path = os.path.join(\".\", \"weather_dataset\")\n",
        "if not os.path.exists(dataset_path):\n",
        "    url = \"https://s3.amazonaws.com/keras-datasets/\" \"jena_climate_2009_2016.csv.zip\"\n",
        "    download_url(url, \".\")\n",
        "    _extract_zip(\"jena_climate_2009_2016.csv.zip\", dataset_path, None)\n",
        "    os.remove(\"jena_climate_2009_2016.csv.zip\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This dataset contains 14 weather measurements in a `.csv` file. Their meaning is shown in the header."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "filename = os.path.join(dataset_path, \"jena_climate_2009_2016.csv\")\n",
        "dataframe = pd.read_csv(filename, index_col=0)\n",
        "data = dataframe.values\n",
        "header = dataframe.columns.tolist()\n",
        "dataframe.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualize Data\n",
        "\n",
        "We now visualize the 14 measured features of the dataset. For this, we use the `plot_data()` function.\n",
        "\n",
        "```python\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def plot_data(data, header, start=0, samples_per_cycle=144, cycles=14):\n",
        "    \"\"\"Plot data highlighting periodic cycles.\"\"\"\n",
        "\n",
        "    fig, axes = plt.subplots(7, 2, figsize=(16, 12), sharex=True)\n",
        "\n",
        "    for i, ax in enumerate(axes.flatten()):\n",
        "        ax.plot(np.arange(start, start + samples_per_cycle * cycles),\n",
        "                data[start:start + samples_per_cycle * cycles, i], \n",
        "                label=header[i])\n",
        "        ax.legend()\n",
        "        ax.set_xlim(start, start + samples_per_cycle * cycles)\n",
        "        \n",
        "        for cycle in range(1, cycles):\n",
        "            ax.axvline(x=start + cycle * samples_per_cycle, \n",
        "                    color=\"gray\", linestyle=\"--\", linewidth=0.5)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from for_rnn import plot_data\n",
        "\n",
        "daily_samples = 144\n",
        "n_days = 14\n",
        "\n",
        "plot_data(data, header, samples_per_cycle=daily_samples, cycles=n_days)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preprocess Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now preprocess the data into time series to input into the neural network model, that is, they need to be of shape (batch size, number of samples, number of feature)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "n_samples = data.shape[0]\n",
        "n_features = data.shape[1]\n",
        "past_seq = 2 * daily_samples  # Length of the sequences to be fed to the RNN.\n",
        "lag = 71  # How many time_steps ahead in time the RNN should predict temperature.\n",
        "temp_idx = 1  # Temperature (Celsius) index.\n",
        "\n",
        "inputs, targets = [], []\n",
        "for i in np.random.permutation(range(0, n_samples - past_seq - lag, daily_samples)):\n",
        "    inputs.append(data[i : i + past_seq, :])\n",
        "    targets.append(data[i + past_seq + lag : i + past_seq + lag + 1, temp_idx])\n",
        "inputs = np.asarray(inputs)\n",
        "targets = np.asarray(targets)\n",
        "\n",
        "print(inputs.shape)\n",
        "print(targets.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "we use deeptrack to handle the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import deeptrack as dt\n",
        "\n",
        "sources = dt.sources.Source(inputs=inputs, targets=targets)\n",
        "train_sources, val_sources = dt.sources.random_split(sources, [0.8, 0.2])\n",
        "\n",
        "print(f\"Number of training inputs = {len(train_sources)} / {len(sources)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We start by removing the mean from the data and normalizing them by their standard deviation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "train_mean = np.mean([source[\"inputs\"] for source in train_sources], axis=(0, 1))\n",
        "train_std = np.std([source[\"inputs\"] for source in train_sources], axis=(0, 1))\n",
        "\n",
        "inputs_pl = dt.Value(sources.inputs - train_mean) / (train_std) >> dt.pytorch.ToTensor(\n",
        "    dtype=torch.float\n",
        ")\n",
        "\n",
        "targets_pl = dt.Value(sources.targets - train_mean[temp_idx]) / (train_std[temp_idx])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We start by creating a dataset and the respective data loaders."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_dataset = dt.pytorch.Dataset(inputs_pl & targets_pl, inputs=train_sources)\n",
        "val_dataset = dt.pytorch.Dataset(inputs_pl & targets_pl, inputs=val_sources)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Creating a Common-Sense Benchmark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We then create a baseline common-sense benchmark, generate benchmark for comparison, i.e., predict $T_t = T_{t-1}$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "temperature = data[:, temp_idx]\n",
        "benchmark = np.mean(\n",
        "    np.abs(\n",
        "        temperature[lag + daily_samples :: daily_samples]\n",
        "        - temperature[lag : -(daily_samples - lag) : daily_samples]\n",
        "    )\n",
        ")\n",
        "print(benchmark)\n",
        "std_bm = benchmark / (train_std[temp_idx])\n",
        "print(std_bm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5PsOxRzRt-W"
      },
      "source": [
        "## Implement PyTorch RNN Model\n",
        "\n",
        "This example introduces the `RNN` module in PyTorch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UH7b-1TGlkTm"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "# Define the RNN and Linear layers separately.\n",
        "rnn = nn.RNN(input_size=inputs.shape[2], hidden_size=2, batch_first=True)\n",
        "fc = nn.Linear(in_features=2, out_features=1)\n",
        "rnn.to(device)\n",
        "fc.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now train the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "criterion = nn.L1Loss()  # MAE loss.\n",
        "optimizer = torch.optim.Adam(\n",
        "    list(rnn.parameters()) + list(fc.parameters()), lr=0.001\n",
        ")  # Optimizer.\n",
        "epochs = 100\n",
        "\n",
        "train_losses, val_losses = [], []\n",
        "for epoch in range(epochs):\n",
        "\n",
        "    # Training.\n",
        "    running_loss = 0\n",
        "    for inputs, targets in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        inputs = inputs.to(device)\n",
        "        targets = targets.to(device)\n",
        "        rnn_out, _ = rnn(inputs)  # RNN layer.\n",
        "        rnn_out = rnn_out[:, -1, :]  # Select the last output for each sequence.\n",
        "        outputs = fc(rnn_out)  # Linear layer.\n",
        "\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    train_losses.append(running_loss / len(train_loader))\n",
        "    print(f\"Epoch {epoch} Training Loss: {train_losses[-1]:.4f}\")\n",
        "\n",
        "    # Validation.\n",
        "    val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in val_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            targets = targets.to(device)\n",
        "            rnn_out, _ = rnn(inputs)  # RNN layer.\n",
        "            rnn_out = rnn_out[:, -1, :]  # Selecting the last output for each sequence.\n",
        "            outputs = fc(rnn_out)  # Linear layer.\n",
        "\n",
        "            loss = criterion(outputs, targets)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    val_losses.append(val_loss / len(val_loader))\n",
        "    print(f\"Epoch {epoch} Validation Loss: {val_losses[-1]:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We then validate the model for which we write the `plot_training()` function.\n",
        "\n",
        "```python\n",
        "def plot_training(epochs, train_losses, val_losses, benchmark):\n",
        "    \"\"\"Plot the training and validation losses.\"\"\"\n",
        "    \n",
        "    plt.plot(range(epochs), train_losses, label=\"Training Loss\")\n",
        "    plt.plot(range(epochs), val_losses, label=\"Validation Loss\")\n",
        "    plt.plot([0, epochs - 1], [benchmark, benchmark], \n",
        "            linestyle=\"--\", color=\"k\", label=\"Benchmark\")\n",
        "    plt.legend()\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.xlim([0, epochs - 1])\n",
        "    plt.show()\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from for_rnn import plot_training\n",
        "\n",
        "plot_training(epochs, train_losses, val_losses, std_bm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Implement the RNN in Deeplay\n",
        "\n",
        "We now implement the RNN in deeplay."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import deeplay as dl\n",
        "\n",
        "rnn_dl = dl.RecurrentModel(\n",
        "    in_features=14,\n",
        "    hidden_features=[2],\n",
        "    out_features=1,\n",
        "    rnn_type=\"RNN\",\n",
        ")\n",
        "rnn_simple = dl.Regressor(rnn_dl, optimizer=dl.Adam(lr=0.001)).create()\n",
        "\n",
        "print(rnn_simple)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We define a trainer that saves training and validation losses at the end of each epoch. The learning curves are plotted together with the common-sense benchmark."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "trainer = dl.Trainer(max_epochs=epochs, accelerator=\"auto\")\n",
        "trainer.fit(rnn_simple, train_loader, val_loader)\n",
        "\n",
        "train_losses = trainer.history.history[\"train_loss_epoch\"][\"value\"]\n",
        "val_losses = trainer.history.history[\"val_loss_epoch\"][\"value\"][1:]\n",
        "\n",
        "plot_training(epochs, train_losses, val_losses, std_bm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Stacked RNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TT6m6j4Rv800"
      },
      "outputs": [],
      "source": [
        "rnn_dl = dl.RecurrentModel(\n",
        "    in_features=n_features,\n",
        "    hidden_features=[16, 16, 16],\n",
        "    out_features=1,\n",
        "    rnn_type=\"RNN\",\n",
        ")\n",
        "rnn_stacked = dl.Regressor(rnn_dl, optimizer=dl.Adam(lr=0.0001)).create()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "trainer = dl.Trainer(max_epochs=epochs)\n",
        "trainer.fit(rnn_stacked, train_loader, val_loader)\n",
        "\n",
        "train_losses = trainer.history.history[\"train_loss_epoch\"][\"value\"]\n",
        "val_losses = trainer.history.history[\"val_loss_epoch\"][\"value\"][1:]\n",
        "\n",
        "plot_training(epochs, train_losses, val_losses, std_bm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Stacked GRU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "gru_dl = dl.RecurrentModel(\n",
        "    in_features=n_features,\n",
        "    hidden_features=[8, 8, 8],\n",
        "    out_features=1,\n",
        "    rnn_type=\"GRU\",\n",
        "    dropout=0.2,\n",
        ")\n",
        "gru_stacked = dl.Regressor(gru_dl, optimizer=dl.Adam(lr=0.001)).create()\n",
        "\n",
        "print(gru_stacked)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "trainer = dl.Trainer(max_epochs=epochs)\n",
        "trainer.fit(gru_stacked, train_loader, val_dataloaders=val_loader)\n",
        "\n",
        "train_losses = trainer.history.history[\"train_loss_epoch\"][\"value\"]\n",
        "val_losses = trainer.history.history[\"val_loss_epoch\"][\"value\"][1:]\n",
        "\n",
        "plot_training(epochs, train_losses, val_losses, std_bm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Stacked LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "lstm_dl = dl.RecurrentModel(\n",
        "    in_features=n_features,\n",
        "    hidden_features=[8, 8, 8],\n",
        "    out_features=1,\n",
        "    rnn_type=\"LSTM\",\n",
        "    dropout=0.3,\n",
        ")\n",
        "lstm_stacked = dl.Regressor(lstm_dl, optimizer=dl.Adam(lr=0.001)).create()\n",
        "\n",
        "print(lstm_stacked)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "trainer = dl.Trainer(max_epochs=epochs)\n",
        "trainer.fit(lstm_stacked, train_loader, val_dataloaders=val_loader)\n",
        "\n",
        "train_losses = trainer.history.history[\"train_loss_epoch\"][\"value\"]\n",
        "val_losses = trainer.history.history[\"val_loss_epoch\"][\"value\"][1:]\n",
        "\n",
        "plot_training(epochs, train_losses, val_losses, std_bm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Implementing a Preprocessor\n",
        "\n",
        "We now introduce a dense preprocessor in feature space."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5cxClS-5_Oto"
      },
      "outputs": [],
      "source": [
        "preprocessor = dl.MultiLayerPerceptron(\n",
        "    in_features=None,\n",
        "    hidden_features=[32],\n",
        "    out_features=8,\n",
        ")\n",
        "preprocessor.dropout.configure(p=0.2)\n",
        "\n",
        "lstm_dl = dl.RecurrentModel(\n",
        "    in_features=8,\n",
        "    hidden_features=[8, 8, 8],\n",
        "    out_features=1,\n",
        "    dropout=0.2,\n",
        "    rnn_type=\"GRU\",\n",
        ")\n",
        "\n",
        "from deeplay import DeeplayModule\n",
        "\n",
        "\n",
        "class Reshape(DeeplayModule):\n",
        "    \"\"\"Custom reshape layer.\"\"\"\n",
        "\n",
        "    def __init__(self, new_shape):\n",
        "        \"\"\"Custom reshape layer.\"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.new_shape = new_shape\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Define the forward pass for the reshape layer.\"\"\"\n",
        "        return x.view(self.new_shape)\n",
        "\n",
        "\n",
        "lstm_pre_dl = dl.Sequential(\n",
        "    torch.nn.Flatten(\n",
        "        start_dim=0, end_dim=1\n",
        "    ),  # HENRIK, shouldn't this be included in a deeplay layer?\n",
        "    preprocessor,\n",
        "    Reshape((-1, pred_window, 8)),\n",
        "    lstm_dl,\n",
        ")\n",
        "lstm_pre = dl.Regressor(lstm_pre_dl).create()\n",
        "\n",
        "print(lstm_pre)\n",
        "\n",
        "trainer_pre = dl.Trainer(\n",
        "    max_epochs=epochs,\n",
        "    accelerator=\"cpu\",\n",
        ")\n",
        "trainer_pre.fit(lstm_pre, train_loader, val_loader)\n",
        "\n",
        "train_losses = trainer_pre.history.history[\"train_loss_epoch\"][\"value\"]\n",
        "val_losses = trainer_pre.history.history[\"val_loss_epoch\"][\"value\"][1:]\n",
        "plot_training(\n",
        "    epochs,\n",
        "    train_losses,\n",
        "    val_losses,\n",
        "    benchmark / std_train[temp_idx],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_training(epochs, train_losses, val_losses, std_bm)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
