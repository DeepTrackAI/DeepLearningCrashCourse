{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this guide, we delve into an engaging and intriguing application of recurrent sequence-to-sequence models by training a basic chatbot using film scripts from the Cornell Movie-Dialogs Corpus.\n",
    "\n",
    "Conversational models are currently a trending subject in the field of AI research. Chatbots are commonly used in various scenarios such as customer service platforms and online help desks. These bots typically utilize retrieval-based models that provide pre-set responses to specific types of questions. While these models might be adequate for highly specific domains like a company's IT helpdesk, they lack the robustness required for broader applications. However, the recent surge in deep learning, spearheaded recently by ChatGPT, has led to the development of potent multi-domain generative conversational models. In this guide, we will create one such model using the tools we have learnt so far.\n",
    "\n",
    "To begin, download the dialog dataset: \n",
    "\n",
    "https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html (should be replaced with direct data source for DLCC)\n",
    "\n",
    "and put in a ``data/`` directory under the current directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://zissou.infosci.cornell.edu/convokit/datasets/movie-corpus/movie-corpus.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# from io import open\n",
    "\n",
    "# import deeplay as dl\n",
    "\n",
    "# # Check for CUDA availability for PyTorch\n",
    "# USE_CUDA = torch.cuda.is_available()\n",
    "# device = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n",
    "\n",
    "# Mount Google Drive if using Google Colab\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive')\n",
    "# os.chdir(\"/content/gdrive/My Drive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing\n",
    "\n",
    "This step involves reorganizing our data file and loading the data into formats that are manageable.\n",
    "\n",
    "The [Cornell Movie-Dialogs Corpus](https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html) is a comprehensive dataset of dialogues from movie characters:\n",
    "\n",
    "- It contains 220,579 conversational exchanges between 10,292 pairs of movie characters.\n",
    "- It features 9,035 characters from 617 movies.\n",
    "- It has a total of 304,713 utterances.\n",
    "\n",
    "This dataset is vast and varied, with a wide range of language formality, time periods, sentiment, etc. We anticipate that this diversity will make our model capable of handling a variety of inputs and queries.\n",
    "\n",
    "Initially, we will examine some lines from our data file to understand the original format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Speakers: 359\n",
      "Number of Utterances: 163948\n",
      "Number of Conversations: 81974\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from convokit import download, Corpus\n",
    "\n",
    "import ssl\n",
    "\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "corpus_name = \"tennis-corpus\"  # \"movie-corpus\"\n",
    "if not os.path.exists(corpus_name):\n",
    "    download(corpus_name, data_dir=\"./\")\n",
    "\n",
    "corpus = Corpus(corpus_name)\n",
    "corpus.print_summary_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "an uninterrupted chain of spoken or written language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do you find this the most difficult of the Grand Slams? You have not had your best results here in terms of the depth, how far you have gone in the tournament. For you, is this the most difficult of the Grand Slams for you?\n",
      "How has your knee been? You talked about it earlier. Your dad said he was a little worried. How's your knee?\n",
      "I think Court2 is the slowest one. I had a chance to play on every court. I think 2 is the slowest court. Between 1 and Centre, there's no really big different.\n",
      "Yes. Especially the way the injury happened. I was leading 5Love in the first set, and all of a sudden  I knew, even though I won that match, I wouldn't be able to play the next one. Came back at home for 20 hours plane ride wasn't fun. Obviously it's happening. I just had to put that on the side of my mind and not too much think about it. But it was still there for a bit.\n",
      "Yeah, we are very good friends. We live close to each other in the States and we share the same coach, so, you know, it will be fun. I haven't played him in three years. You know, we'll -- we'll probably put -- like I said out there, we'll probably put dinner on it or something, so it's a win-win, at least.\n"
     ]
    }
   ],
   "source": [
    "for _ in range(5):\n",
    "    print(corpus.random_utterance().text)\n",
    "\n",
    "ex_text = corpus.random_utterance().text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "import re\n",
    "import contractions\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    standardized_text = contractions.fix(text)\n",
    "\n",
    "    standardized_text = (  # (1)\n",
    "        standardized_text.replace(\"’\", \"'\")\n",
    "        .replace(\"‘\", \"'\")\n",
    "        .replace(\"´\", \"'\")\n",
    "        .replace(\"“\", '\"')\n",
    "        .replace(\"”\", '\"')\n",
    "        .replace(\"´´\", '\"')\n",
    "    )\n",
    "\n",
    "    tokens = tokenizer(standardized_text)\n",
    "\n",
    "    filtered_tokens = [\n",
    "        token\n",
    "        for token in tokens\n",
    "        if re.match(\n",
    "            r\"^[a-zA-Z0-9.,!?]+(-[a-zA-Z0-9.,!?]+)*(_[a-zA-Z0-9.,!?]+)*$\", token\n",
    "        )\n",
    "    ]\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "\n",
    "def corpus_iterator(corpus):  # (1)\n",
    "    for utt_id in corpus.utterances:\n",
    "        utt = corpus.get_utterance(utt_id)\n",
    "        yield tokenize(utt.text)\n",
    "\n",
    "\n",
    "# Add EOS, SOS, and PAD to the specials list\n",
    "special_tokens = [\"<unk>\", \"<pad>\", \"<sos>\", \"<eos>\"]\n",
    "\n",
    "vocab = build_vocab_from_iterator(\n",
    "    corpus_iterator(corpus),  # (2)\n",
    "    specials=special_tokens,  # (3),\n",
    "    min_freq=5,  # (4)\n",
    ")\n",
    "vocab.set_default_index(vocab[\"<unk>\"])  # (5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9876"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'you'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.lookup_token(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of queries/responses: 15419\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "MAX_LENGTH = 30\n",
    "\n",
    "queries = []\n",
    "responses = []\n",
    "\n",
    "\n",
    "def all_words_in_vocab(sentence, vocab):\n",
    "    return all(word in vocab for word in sentence)\n",
    "\n",
    "\n",
    "def process_sentence(sequence, max_length=MAX_LENGTH):\n",
    "    processed = (\n",
    "        [\"<sos>\"] + sequence + [\"<eos>\"] + [\"<pad>\"] * (max_length - len(sequence))\n",
    "    )\n",
    "    return processed\n",
    "\n",
    "\n",
    "for conv_id in corpus.conversations:\n",
    "    conv = corpus.get_conversation(conv_id)\n",
    "    utt_ids = conv.get_utterance_ids()\n",
    "\n",
    "    range_end = len(utt_ids) - (len(utt_ids) % 2)\n",
    "    # Adjust range to exclude the last utterance in conversations with an odd number of utterances\n",
    "\n",
    "    for i in range(0, range_end, 2):\n",
    "\n",
    "        query = tokenize(corpus.get_utterance(utt_ids[i]).text)\n",
    "        response = tokenize(corpus.get_utterance(utt_ids[i + 1]).text)\n",
    "\n",
    "        if (\n",
    "            all_words_in_vocab(query + response, vocab)\n",
    "            and len(query) <= MAX_LENGTH\n",
    "            and len(response) <= MAX_LENGTH\n",
    "        ):\n",
    "            query = process_sentence(query)\n",
    "            response = process_sentence(response)\n",
    "\n",
    "            queries.append(vocab(query))\n",
    "            responses.append(vocab(response))\n",
    "\n",
    "queries = np.asarray(queries)\n",
    "responses = np.asarray(responses)\n",
    "\n",
    "print(f\"Number of queries/responses: {len(queries)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# queries = np.asarray(queries)\n",
    "# responses = np.asarray(responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:torchtext.vocab.vectors:Loading vectors from ./.vector_cache/glove.42B.300d.txt.pt\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Special tokens\n",
    "special_tokens = [\"<pad>\", \"<sos>\", \"<eos>\"]\n",
    "\n",
    "# Assuming the dimensionality of GloVe embeddings is 300\n",
    "embedding_dim = 300\n",
    "num_special_tokens = len(special_tokens)\n",
    "\n",
    "# Initialize a tensor to hold the embeddings for special tokens\n",
    "# Here, PAD is initialized to zeros, and SOS, EOS to random values\n",
    "special_embeddings = torch.zeros(num_special_tokens, embedding_dim)\n",
    "special_embeddings[1:] = (\n",
    "    torch.rand(num_special_tokens - 1, embedding_dim) * 0.01\n",
    ")  # Small random numbers for SOS and EOS\n",
    "\n",
    "from torchtext.vocab import GloVe\n",
    "\n",
    "# Load GloVe embeddings\n",
    "glove = GloVe(name=\"42B\", dim=300, cache=\"./.vector_cache\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get GloVe embeddings for the vocabulary tokens\n",
    "# Assuming 'vocab' is a list of vocabulary tokens including special tokens at the beginning\n",
    "glove_embeddings = glove.get_vecs_by_tokens(vocab.get_itos(), lower_case_backup=True)\n",
    "\n",
    "# Concatenate the special token embeddings with the GloVe embeddings\n",
    "extended_embeddings = torch.cat([special_embeddings, glove_embeddings], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import deeplay as dl\n",
    "# import torch.nn as nn\n",
    "\n",
    "# features_dim = 500\n",
    "# vocab_size = len(vocab)\n",
    "\n",
    "# embedding = dl.Layer(nn.Embedding, vocab_size, embedding_dim)\n",
    "# embedding.weight = extended_embeddings\n",
    "# embedding.weight.requires_grad = False\n",
    "\n",
    "# encoder = dl.RecurrentModel(\n",
    "#     in_features=embedding_dim,\n",
    "#     hidden_features=[features_dim, features_dim],\n",
    "#     out_features=embedding_dim,\n",
    "#     dropout=0.1,\n",
    "#     rnn_type=\"GRU\",\n",
    "# )\n",
    "# encoder.layer[2].configure(nn.Identity)\n",
    "\n",
    "# decoder = dl.RecurrentModel(\n",
    "#     in_features=embedding_dim,\n",
    "#     hidden_features=[features_dim, features_dim],\n",
    "#     out_features=embedding_dim,\n",
    "#     dropout=0.1,\n",
    "#     rnn_type=\"GRU\",\n",
    "# )\n",
    "# decoder.blocks[2].activation.configure(nn.Softmax)\n",
    "# seq2seq = dl.Sequential(\n",
    "#     embedding,\n",
    "#     encoder,\n",
    "#     decoder,\n",
    "# )\n",
    "\n",
    "# reg = dl.Regressor(\n",
    "#     model=seq2seq,\n",
    "#     loss=nn.CrossEntropyLoss(),\n",
    "#     optimizer=dl.Adam(),\n",
    "# )\n",
    "# seq2seq_reg = reg.create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from deeplay import DeeplayModule, Classifier\n",
    "\n",
    "\n",
    "class MyClassifier(Classifier):\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x1, x2 = batch\n",
    "        y = torch.cat([x2[1:], len(x2) - 1], dim=1).float()\n",
    "        y_hat = self(x1, x2)\n",
    "        loss = self.loss(y_hat, y)\n",
    "        self.log(\n",
    "            f\"train_{name}\",\n",
    "            loss,\n",
    "            on_step=True,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "            logger=True,\n",
    "        )\n",
    "\n",
    "        self.log_metrics(\n",
    "            \"train\", y_hat, y, on_step=True, on_epoch=True, prog_bar=True, logger=True\n",
    "        )\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "class Encoder(DeeplayModule):\n",
    "    def __init__(self, vocab_size, embedding_dim, lstm_units):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, lstm_units, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        outputs, (hidden, cell) = self.lstm(x)\n",
    "        return hidden, cell\n",
    "\n",
    "\n",
    "class Decoder(DeeplayModule):\n",
    "    def __init__(self, vocab_size, embedding_dim, lstm_units):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, lstm_units, batch_first=True)\n",
    "        self.dense = nn.Linear(lstm_units, vocab_size)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x, hidden, cell):\n",
    "        x = self.embedding(x)\n",
    "        outputs, (hidden, cell) = self.lstm(x, (hidden, cell))\n",
    "        outputs = self.dense(outputs)\n",
    "        outputs = self.softmax(outputs)\n",
    "        return outputs, hidden, cell\n",
    "\n",
    "\n",
    "class Seq2Seq(DeeplayModule):\n",
    "    def __init__(\n",
    "        self, vocab_size, embedding_dim, lstm_units, maxlen_questions, maxlen_answers\n",
    "    ):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = Encoder(vocab_size, embedding_dim, lstm_units)\n",
    "        self.decoder = Decoder(vocab_size, embedding_dim, lstm_units)\n",
    "        # self.maxlen_questions = maxlen_questions\n",
    "        # self.maxlen_answers = maxlen_answers\n",
    "        # self.vocab_size = vocab_size\n",
    "\n",
    "    def forward(self, encoder_input_data, decoder_input_data):\n",
    "        encoder_hidden, encoder_cell = self.encoder(encoder_input_data)\n",
    "        decoder_outputs, _, _ = self.decoder(\n",
    "            decoder_input_data, encoder_hidden, encoder_cell\n",
    "        )\n",
    "        return decoder_outputs\n",
    "\n",
    "\n",
    "seq2seq = Seq2Seq(len(vocab), 300, 200, 30, 30)\n",
    "\n",
    "classifier = MyClassifier(\n",
    "    model=seq2seq,\n",
    "    loss=nn.CrossEntropyLoss(),\n",
    "    optimizer=dl.Adam(),\n",
    ")\n",
    "seq2seq_classifier = classifier.create()\n",
    "\n",
    "classifier.model.encoder.embedding.weight.data = glove.get_vecs_by_tokens(\n",
    "    vocab.get_itos(), lower_case_backup=True  # (2)\n",
    ")\n",
    "classifier.model.encoder.embedding.weight.requires_grad = False  # (3)\n",
    "\n",
    "classifier.model.decoder.embedding.weight.data = glove.get_vecs_by_tokens(\n",
    "    vocab.get_itos(), lower_case_backup=True  # (2)\n",
    ")\n",
    "classifier.model.decoder.embedding.weight.requires_grad = False  # (3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MyClassifier(Classifier):\n",
    "\n",
    "#     def training_step(self, batch, batch_idx):\n",
    "#         x1, x2 = batch\n",
    "#         y = torch.cat([x2[1:], len(x2)-1], dim=1)\n",
    "#         y_hat = self(x1,x2)\n",
    "#         loss=self.loss(y_hat,y)\n",
    "#         self.log(\n",
    "#             f\"train_{name}\",\n",
    "#             loss,\n",
    "#             on_step=True,\n",
    "#             on_epoch=True,\n",
    "#             prog_bar=True,\n",
    "#             logger=True,\n",
    "#         )\n",
    "\n",
    "#     self.log_metrics(\n",
    "#         \"train\", y_hat, y, on_step=True, on_epoch=True, prog_bar=True, logger=True\n",
    "#     )\n",
    "\n",
    "#     return loss\n",
    "\n",
    "#     # def compute_loss(self, y_hat, y):\n",
    "#     #     if self.make_targets_one_hot:\n",
    "#     #         y = F.one_hot(y, num_classes=y_hat.size(1)).float()\n",
    "\n",
    "#     #    return self.loss(y_hat, y)\n",
    "\n",
    "#     # def forward(self, x):\n",
    "#     #     return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deeptrack as dt\n",
    "import torch\n",
    "\n",
    "sources = dt.sources.Source(inputs=queries, targets=responses)\n",
    "\n",
    "inputs_pl = dt.Value(sources.inputs) >> dt.pytorch.ToTensor(dtype=torch.int)\n",
    "targets_pl = dt.Value(sources.targets) >> dt.pytorch.ToTensor(dtype=torch.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataset = dt.pytorch.Dataset(inputs_pl & targets_pl, inputs=sources)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/841602/Documents/GitHub/Environments/deeplay_env/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "/Users/841602/Documents/GitHub/Environments/deeplay_env/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:74: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
       "┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name          </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type             </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span>┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span>│ loss          │ CrossEntropyLoss │      0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 </span>│ train_metrics │ MetricCollection │      0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2 </span>│ val_metrics   │ MetricCollection │      0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3 </span>│ test_metrics  │ MetricCollection │      0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 4 </span>│ model         │ Seq2Seq          │  8.7 M │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 5 </span>│ optimizer     │ Adam             │      0 │\n",
       "└───┴───────────────┴──────────────────┴────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
       "┃\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mName         \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mType            \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
       "│\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m│ loss          │ CrossEntropyLoss │      0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m1\u001b[0m\u001b[2m \u001b[0m│ train_metrics │ MetricCollection │      0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m2\u001b[0m\u001b[2m \u001b[0m│ val_metrics   │ MetricCollection │      0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m3\u001b[0m\u001b[2m \u001b[0m│ test_metrics  │ MetricCollection │      0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m4\u001b[0m\u001b[2m \u001b[0m│ model         │ Seq2Seq          │  8.7 M │\n",
       "│\u001b[2m \u001b[0m\u001b[2m5\u001b[0m\u001b[2m \u001b[0m│ optimizer     │ Adam             │      0 │\n",
       "└───┴───────────────┴──────────────────┴────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 8.7 M                                                                                            \n",
       "<span style=\"font-weight: bold\">Non-trainable params</span>: 0                                                                                            \n",
       "<span style=\"font-weight: bold\">Total params</span>: 8.7 M                                                                                                \n",
       "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 34                                                                         \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mTrainable params\u001b[0m: 8.7 M                                                                                            \n",
       "\u001b[1mNon-trainable params\u001b[0m: 0                                                                                            \n",
       "\u001b[1mTotal params\u001b[0m: 8.7 M                                                                                                \n",
       "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 34                                                                         \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a48042835c141aca989679900e23fd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/841602/Documents/GitHub/Environments/deeplay_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "expected Tensor as element 1 in argument 0, but got int",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[243], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m trainer \u001b[38;5;241m=\u001b[39m dl\u001b[38;5;241m.\u001b[39mTrainer(max_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, accelerator\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseq2seq_classifier\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/Environments/deeplay_env/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py:545\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    543\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[1;32m    544\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 545\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    546\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    547\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/Environments/deeplay_env/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py:44\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     43\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     47\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m~/Documents/GitHub/Environments/deeplay_env/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py:581\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    574\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    575\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    576\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    577\u001b[0m     ckpt_path,\n\u001b[1;32m    578\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    579\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    580\u001b[0m )\n\u001b[0;32m--> 581\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[1;32m    584\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/GitHub/Environments/deeplay_env/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py:990\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    985\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signal_connector\u001b[38;5;241m.\u001b[39mregister_signal_handlers()\n\u001b[1;32m    987\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    988\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    989\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 990\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    992\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    993\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    994\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    995\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/GitHub/Environments/deeplay_env/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py:1036\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1034\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_sanity_check()\n\u001b[1;32m   1035\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[0;32m-> 1036\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1037\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1038\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected state \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/GitHub/Environments/deeplay_env/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:202\u001b[0m, in \u001b[0;36m_FitLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start()\n\u001b[0;32m--> 202\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/GitHub/Environments/deeplay_env/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:359\u001b[0m, in \u001b[0;36m_FitLoop.advance\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_training_epoch\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    358\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_fetcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 359\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepoch_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_fetcher\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/Environments/deeplay_env/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py:136\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.run\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone:\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 136\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    137\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end(data_fetcher)\n\u001b[1;32m    138\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/GitHub/Environments/deeplay_env/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py:240\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.advance\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_training_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    238\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mlightning_module\u001b[38;5;241m.\u001b[39mautomatic_optimization:\n\u001b[1;32m    239\u001b[0m         \u001b[38;5;66;03m# in automatic optimization, there can only be one optimizer\u001b[39;00m\n\u001b[0;32m--> 240\u001b[0m         batch_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautomatic_optimization\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    242\u001b[0m         batch_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmanual_optimization\u001b[38;5;241m.\u001b[39mrun(kwargs)\n",
      "File \u001b[0;32m~/Documents/GitHub/Environments/deeplay_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:187\u001b[0m, in \u001b[0;36m_AutomaticOptimization.run\u001b[0;34m(self, optimizer, batch_idx, kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m         closure()\n\u001b[1;32m    182\u001b[0m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;66;03m# BACKWARD PASS\u001b[39;00m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;66;03m# gradient update with accumulated gradients\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 187\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_optimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    189\u001b[0m result \u001b[38;5;241m=\u001b[39m closure\u001b[38;5;241m.\u001b[39mconsume_result()\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result\u001b[38;5;241m.\u001b[39mloss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/GitHub/Environments/deeplay_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:265\u001b[0m, in \u001b[0;36m_AutomaticOptimization._optimizer_step\u001b[0;34m(self, batch_idx, train_step_and_backward_closure)\u001b[0m\n\u001b[1;32m    262\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptim_progress\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep\u001b[38;5;241m.\u001b[39mincrement_ready()\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# model hook\u001b[39;00m\n\u001b[0;32m--> 265\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_lightning_module_hook\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moptimizer_step\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_step_and_backward_closure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m should_accumulate:\n\u001b[1;32m    275\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptim_progress\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep\u001b[38;5;241m.\u001b[39mincrement_completed()\n",
      "File \u001b[0;32m~/Documents/GitHub/Environments/deeplay_env/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py:157\u001b[0m, in \u001b[0;36m_call_lightning_module_hook\u001b[0;34m(trainer, hook_name, pl_module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m hook_name\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[LightningModule]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpl_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 157\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    160\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m~/Documents/GitHub/Environments/deeplay_env/lib/python3.10/site-packages/lightning/pytorch/core/module.py:1282\u001b[0m, in \u001b[0;36mLightningModule.optimizer_step\u001b[0;34m(self, epoch, batch_idx, optimizer, optimizer_closure)\u001b[0m\n\u001b[1;32m   1243\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimizer_step\u001b[39m(\n\u001b[1;32m   1244\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1245\u001b[0m     epoch: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1248\u001b[0m     optimizer_closure: Optional[Callable[[], Any]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1249\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1250\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Override this method to adjust the default way the :class:`~lightning.pytorch.trainer.trainer.Trainer` calls\u001b[39;00m\n\u001b[1;32m   1251\u001b[0m \u001b[38;5;124;03m    the optimizer.\u001b[39;00m\n\u001b[1;32m   1252\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1280\u001b[0m \n\u001b[1;32m   1281\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1282\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer_closure\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/Environments/deeplay_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:151\u001b[0m, in \u001b[0;36mLightningOptimizer.step\u001b[0;34m(self, closure, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MisconfigurationException(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhen `optimizer.step(closure)` is called, the closure should be callable\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_strategy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 151\u001b[0m step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_strategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_on_after_step()\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m step_output\n",
      "File \u001b[0;32m~/Documents/GitHub/Environments/deeplay_env/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py:230\u001b[0m, in \u001b[0;36mStrategy.optimizer_step\u001b[0;34m(self, optimizer, closure, model, **kwargs)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;66;03m# TODO(fabric): remove assertion once strategy's optimizer_step typing is fixed\u001b[39;00m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, pl\u001b[38;5;241m.\u001b[39mLightningModule)\n\u001b[0;32m--> 230\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprecision_plugin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/Environments/deeplay_env/lib/python3.10/site-packages/lightning/pytorch/plugins/precision/precision_plugin.py:117\u001b[0m, in \u001b[0;36mPrecisionPlugin.optimizer_step\u001b[0;34m(self, optimizer, model, closure, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Hook to run the optimizer step.\"\"\"\u001b[39;00m\n\u001b[1;32m    116\u001b[0m closure \u001b[38;5;241m=\u001b[39m partial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrap_closure, model, optimizer, closure)\n\u001b[0;32m--> 117\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/Environments/deeplay_env/lib/python3.10/site-packages/torch/optim/optimizer.py:385\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    381\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    382\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    383\u001b[0m             )\n\u001b[0;32m--> 385\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    388\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/GitHub/Environments/deeplay_env/lib/python3.10/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/Documents/GitHub/Environments/deeplay_env/lib/python3.10/site-packages/torch/optim/adam.py:146\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m closure \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39menable_grad():\n\u001b[0;32m--> 146\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_groups:\n\u001b[1;32m    149\u001b[0m     params_with_grad \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/Documents/GitHub/Environments/deeplay_env/lib/python3.10/site-packages/lightning/pytorch/plugins/precision/precision_plugin.py:104\u001b[0m, in \u001b[0;36mPrecisionPlugin._wrap_closure\u001b[0;34m(self, model, optimizer, closure)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_wrap_closure\u001b[39m(\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     93\u001b[0m     model: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpl.LightningModule\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     94\u001b[0m     optimizer: Optimizer,\n\u001b[1;32m     95\u001b[0m     closure: Callable[[], Any],\n\u001b[1;32m     96\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m     97\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"This double-closure allows makes sure the ``closure`` is executed before the ``on_before_optimizer_step``\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;124;03m    hook is called.\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    102\u001b[0m \n\u001b[1;32m    103\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 104\u001b[0m     closure_result \u001b[38;5;241m=\u001b[39m \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_after_closure(model, optimizer)\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m closure_result\n",
      "File \u001b[0;32m~/Documents/GitHub/Environments/deeplay_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:140\u001b[0m, in \u001b[0;36mClosure.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[Tensor]:\n\u001b[0;32m--> 140\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\u001b[38;5;241m.\u001b[39mloss\n",
      "File \u001b[0;32m~/Documents/GitHub/Environments/deeplay_env/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/Environments/deeplay_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:126\u001b[0m, in \u001b[0;36mClosure.closure\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39menable_grad()\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclosure\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ClosureResult:\n\u001b[0;32m--> 126\u001b[0m     step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_step_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m step_output\u001b[38;5;241m.\u001b[39mclosure_loss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    129\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwarning_cache\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`training_step` returned `None`. If this was on purpose, ignore this warning...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/GitHub/Environments/deeplay_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:315\u001b[0m, in \u001b[0;36m_AutomaticOptimization._training_step\u001b[0;34m(self, kwargs)\u001b[0m\n\u001b[1;32m    312\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\n\u001b[1;32m    314\u001b[0m \u001b[38;5;66;03m# manually capture logged metrics\u001b[39;00m\n\u001b[0;32m--> 315\u001b[0m training_step_output \u001b[38;5;241m=\u001b[39m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtraining_step\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mpost_training_step()  \u001b[38;5;66;03m# unused hook - call anyway for backward compatibility\u001b[39;00m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_result_cls\u001b[38;5;241m.\u001b[39mfrom_training_step_output(training_step_output, trainer\u001b[38;5;241m.\u001b[39maccumulate_grad_batches)\n",
      "File \u001b[0;32m~/Documents/GitHub/Environments/deeplay_env/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py:309\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[0;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    306\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 309\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    312\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m~/Documents/GitHub/Environments/deeplay_env/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py:382\u001b[0m, in \u001b[0;36mStrategy.training_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module:\n\u001b[1;32m    381\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_redirection(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining_step\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 382\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlightning_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[240], line 11\u001b[0m, in \u001b[0;36mMyClassifier.training_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtraining_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch, batch_idx):\n\u001b[1;32m     10\u001b[0m     x1, x2 \u001b[38;5;241m=\u001b[39m batch\n\u001b[0;32m---> 11\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx2\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx2\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m     12\u001b[0m     y_hat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m(x1, x2)\n\u001b[1;32m     13\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss(y_hat, y)\n",
      "\u001b[0;31mTypeError\u001b[0m: expected Tensor as element 1 in argument 0, but got int"
     ]
    }
   ],
   "source": [
    "trainer = dl.Trainer(max_epochs=100, accelerator=\"cpu\")\n",
    "trainer.fit(seq2seq_classifier, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 32])"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Embedding(9879, 300)\n",
      "  (1): RecurrentModel(\n",
      "    (blocks): LayerList(\n",
      "      (0): RecurrentBlock(\n",
      "        (layer): GRU(300, 500, batch_first=True)\n",
      "        (activation): Identity()\n",
      "        (normalization): Identity()\n",
      "        (dropout): RecurrentDropout(\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (1): RecurrentBlock(\n",
      "        (layer): GRU(500, 500, batch_first=True)\n",
      "        (activation): Identity()\n",
      "        (normalization): Identity()\n",
      "        (dropout): RecurrentDropout(\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (2): LayerActivationNormalizationDropout(\n",
      "        (layer): Linear(in_features=500, out_features=1, bias=True)\n",
      "        (activation): Identity()\n",
      "        (normalization): Identity()\n",
      "        (dropout): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (2): RecurrentModel(\n",
      "    (blocks): LayerList(\n",
      "      (0): RecurrentBlock(\n",
      "        (layer): GRU(8, 500, batch_first=True)\n",
      "        (activation): Identity()\n",
      "        (normalization): Identity()\n",
      "        (dropout): RecurrentDropout(\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (1): RecurrentBlock(\n",
      "        (layer): GRU(500, 500, batch_first=True)\n",
      "        (activation): Identity()\n",
      "        (normalization): Identity()\n",
      "        (dropout): RecurrentDropout(\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (2): LayerActivationNormalizationDropout(\n",
      "        (layer): Linear(in_features=500, out_features=1, bias=True)\n",
      "        (activation): Identity()\n",
      "        (normalization): Identity()\n",
      "        (dropout): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(seq2seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Formatted Data File\n",
    "\n",
    "For ease of use, we will generate a well-structured data file where each line comprises a tab-separated pair of a *query sentence* and a *response sentence*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pairs = []\n",
    "\n",
    "# for conv_id in corpus.conversations:\n",
    "#     conv = corpus.get_conversation(conv_id)\n",
    "#     utt_ids = conv.get_utterance_ids()\n",
    "\n",
    "#     num_utt = len(utt_ids)\n",
    "#     range_end = num_utt - 2 if num_utt % 2 != 0 else num_utt - 1\n",
    "\n",
    "#     for i in range(range_end):\n",
    "#         query = corpus.get_utterance(utt_ids[i]).text.strip()\n",
    "#         response = corpus.get_utterance(utt_ids[i + 1]).text.strip()\n",
    "#         if query and response:\n",
    "#             pairs.append(f\"{query}|-->|{response}\")\n",
    "\n",
    "# print(f\"Number of pairs: {len(pairs)}\")\n",
    "\n",
    "# # filename = os.path.join(corpus_name, \"formatted_movie_lines.txt\")\n",
    "\n",
    "# # with open(filename, \"w\", encoding=\"utf-8\") as file:\n",
    "# #     for pair in pairs:\n",
    "# #         file.write(pair + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "convert all letters to lowercase \n",
    "\n",
    "trim all non-letter characters except for basic punctuation\n",
    "\n",
    "filter out sentences with length greater than the MAX_LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "\n",
    "\n",
    "# def clean_text(text):\n",
    "#     text = text.strip().lower()\n",
    "#     text = re.sub(r\"[^a-z0-9.!?]+\", r\" \", text)\n",
    "#     text = re.sub(r\"([.!?])\", r\" \\1 \", text)\n",
    "#     text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "#     return text\n",
    "\n",
    "\n",
    "# MAX_LENGTH = 30\n",
    "# proc_pairs = []\n",
    "\n",
    "# for pair in pairs:\n",
    "#     query, response = pair.strip().split(\"|-->|\")\n",
    "#     clean_query = clean_text(query)\n",
    "#     clean_response = clean_text(response)\n",
    "#     if len(clean_query) <= MAX_LENGTH and len(clean_response) <= MAX_LENGTH:\n",
    "#         proc_pairs.append(f\"{clean_query}|-->|{clean_response}\")\n",
    "\n",
    "# print(f\"Number of pairs: {len(proc_pairs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create a vocabulary\n",
    "\n",
    "Remove words below a certain count threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading and Trimming\n",
    "\n",
    "The next step involves creating a vocabulary and loading query/response sentence pairs into memory.\n",
    "\n",
    "Keep in mind that we are working with sequences of **words**, which do not inherently map to a discrete numerical space. Therefore, we need to create such a mapping by associating each unique word we encounter in our dataset with an index value.\n",
    "\n",
    "To achieve this, we define a `Vocabulary` class, which maintains a mapping from words to indexes, a reverse mapping from indexes to words, a count of each word, and a total word count. The class offers methods for adding a word to the vocabulary (`add_word`), adding all words in a sentence (`add_sentence`), and trimming infrequently seen words (`trim`). We will discuss trimming in more detail later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# sentences = [\n",
    "#     sentence.strip() for pair in proc_pairs for sentence in pair.strip().split(\"|-->|\")\n",
    "# ]\n",
    "\n",
    "# MIN_COUNT = 5\n",
    "\n",
    "# # Initialize CountVectorizer\n",
    "# vectorizer = CountVectorizer(\n",
    "#     min_df=MIN_COUNT, tokenizer=lambda txt: txt.strip().split(\" \")\n",
    "# )\n",
    "# X = vectorizer.fit_transform(sentences)\n",
    "\n",
    "# # Get the feature names to build a vocab dictionary\n",
    "# vocab = vectorizer.get_feature_names_out()\n",
    "\n",
    "# print(f\"Vocabulary size: {len(vocab)}\")\n",
    "# # print(f\"Encoded sentence example: {encoded_sentences[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep_pairs = []\n",
    "# for pair in proc_pairs:\n",
    "#     query, response = pair.strip().split(\"|-->|\")\n",
    "\n",
    "#     keep_query = True\n",
    "#     keep_response = True\n",
    "#     # Check input sentence\n",
    "#     for word in query.strip().split(\" \"):\n",
    "#         if word not in vocab:\n",
    "#             keep_query = False\n",
    "#             break\n",
    "#     # Check output sentence\n",
    "#     for word in response.strip().split(\" \"):\n",
    "#         if word not in vocab:\n",
    "#             keep_response = False\n",
    "#             break\n",
    "\n",
    "#     if keep_query and keep_response:\n",
    "#         keep_pairs.append(pair)\n",
    "# print(f\"Number of pairs: {len(keep_pairs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PAD, SOS,EOS token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Mapping words to indices (+3 offset for special tokens)\n",
    "# word_to_idx = {word: i + 3 for i, word in enumerate(vocab)}\n",
    "\n",
    "# # Adding special tokens to the dictionary\n",
    "# word_to_idx[\"<PAD>\"] = 0  # PAD\n",
    "# word_to_idx[\"<SOS>\"] = 1  # SOS\n",
    "# word_to_idx[\"<EOS>\"] = 2  # EOS\n",
    "\n",
    "\n",
    "# # Example of encoding a sentence with SOS, EOS, and converting to indices\n",
    "# def encode_sentence(sentence, word_to_idx, max_len=MAX_LENGTH):\n",
    "#     # Tokenize the sentence\n",
    "#     tokens = sentence.split()\n",
    "\n",
    "#     # Add SOS and EOS tokens\n",
    "#     tokens = [\"<SOS>\"] + tokens + [\"<EOS>\"]\n",
    "\n",
    "#     # Convert tokens to indices\n",
    "#     indices = [word_to_idx.get(token, word_to_idx[\"<EOS>\"]) for token in tokens]\n",
    "\n",
    "#     # Pad the sequence to max_len\n",
    "#     padded_sequence = indices + [word_to_idx[\"<PAD>\"]] * (max_len - len(indices))\n",
    "#     return padded_sequence[:max_len]\n",
    "\n",
    "\n",
    "# sentences = [\n",
    "#     sentence.strip() for pair in keep_pairs for sentence in pair.strip().split(\"|-->|\")\n",
    "# ]\n",
    "# # Example: encoding the first sentence\n",
    "# # max_len = max(len(s.split()) for s in sentences) + 2  # +2 for SOS and EOS tokens\n",
    "# encoded_sentences = [encode_sentence(s, word_to_idx) for s in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"Encoded sentence example: {encoded_sentences[10000]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idx_to_word = {idx: word for word, idx in word_to_idx.items()}\n",
    "\n",
    "\n",
    "# def decode_sentence(encoded_sentence, idx_to_word):\n",
    "#     # Convert indices back to tokens, ignoring special tokens for padding, start, and end\n",
    "#     tokens = [\n",
    "#         idx_to_word.get(idx)\n",
    "#         for idx in encoded_sentence\n",
    "#         if idx in idx_to_word and idx > 2\n",
    "#     ]\n",
    "\n",
    "#     # Join the tokens back into a single string\n",
    "#     sentence = \" \".join(tokens)\n",
    "#     return sentence\n",
    "\n",
    "\n",
    "# # Example: decoding the first encoded sentence\n",
    "# decoded_sentences = [\n",
    "#     decode_sentence(encoded, idx_to_word) for encoded in encoded_sentences\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"Decoded sentence example: {decoded_sentences[0:20]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now compile our vocabulary and query/response sentence pairs. However, before we can utilize this data, we need to carry out some preprocessing steps.\n",
    "\n",
    "Initially, we need to transform the Unicode strings into ASCII using `unicodeToAscii`. Subsequently, we should convert all characters to lowercase and remove all non-letter characters, excluding basic punctuation (`normalize_string`). Lastly, to facilitate training convergence, we will exclude sentences exceeding the `MAX_LENGTH` threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another tactic that is beneficial to achieving faster convergence during\n",
    "training is trimming rarely used words out of our vocabulary. Decreasing\n",
    "the feature space will also soften the difficulty of the function that\n",
    "the model must learn to approximate. We will do this as a two-step\n",
    "process:\n",
    "\n",
    "1) Trim words used under ``MIN_COUNT`` threshold using the ``voc.trim``\n",
    "   function.\n",
    "\n",
    "2) Filter out pairs with trimmed words.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation for Models\n",
    "\n",
    "Despite our extensive efforts to curate and process our data into a convenient vocabulary object and list of sentence pairs, our models will ultimately require numerical torch tensors as inputs.  \n",
    "\n",
    " To accommodate sentences of different sizes in the same batch, we will create our batched input tensor of shape (max_length, batch_size), where sentences shorter than the max_length are zero padded after an EOS_token.\n",
    "\n",
    "If we simply convert our English sentences to tensors by converting words to their indexes and zero-pad, our tensor would have shape (batch_size, max_length) and indexing the first dimension would return a full sequence across all time-steps. However, we need to be able to index our batch along time, and across all sequences in the batch. Therefore, we transpose our input batch shape to (max_length, batch_size), so that indexing across the first dimension returns a time step across all sentences in the batch.\n",
    "\n",
    "The output function palso returns a binary mask tensor and a maximum target sentence length. The binary mask tensor has the same shape as the output target tensor, but every element that is a PAD_token is 0 and all others are 1.\n",
    "\n",
    "`batch_to_train_data` simply takes a bunch of pairs and returns the input and target tensors using the aforementioned functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cny8pPMnIrQC"
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import torch\n",
    "import random\n",
    "\n",
    "# Assuming PAD_token and EOS_token are defined with their respective integral values\n",
    "\n",
    "\n",
    "def indexes_from_sentence(vocabulary, sentence):\n",
    "    \"\"\"\n",
    "    Convert sentence to a list of indexes, appending the EOS token at the end.\n",
    "    \"\"\"\n",
    "    return [vocabulary.word_to_index[word] for word in sentence.split(\" \")] + [\n",
    "        EOS_TOKEN\n",
    "    ]\n",
    "\n",
    "\n",
    "def binary_matrix(l, value=PAD_TOKEN):\n",
    "    \"\"\"\n",
    "    Create a binary matrix representing the padding of sentences.\n",
    "    \"\"\"\n",
    "    return [[0 if token == value else 1 for token in seq] for seq in l]\n",
    "\n",
    "\n",
    "def batch_to_train_data(vocabulary, pair_batch):\n",
    "    \"\"\"\n",
    "    Prepare the batch for training: sort by input length, create tensors for input/target variables.\n",
    "    \"\"\"\n",
    "    pair_batch.sort(key=lambda x: len(x[0].split(\" \")), reverse=True)\n",
    "    input_batch, output_batch = zip(*pair_batch)\n",
    "    input_indexes = [\n",
    "        indexes_from_sentence(vocabulary, sentence) for sentence in input_batch\n",
    "    ]\n",
    "    input_lengths = torch.tensor([len(indexes) for indexes in input_indexes])\n",
    "    input_padded = torch.LongTensor(\n",
    "        list(itertools.zip_longest(*input_indexes, fillvalue=PAD_TOKEN))\n",
    "    )\n",
    "\n",
    "    output_indexes = [\n",
    "        indexes_from_sentence(vocabulary, sentence) for sentence in output_batch\n",
    "    ]\n",
    "    output_padded = torch.LongTensor(\n",
    "        list(itertools.zip_longest(*output_indexes, fillvalue=PAD_TOKEN))\n",
    "    )\n",
    "    output_mask = torch.BoolTensor(binary_matrix(output_padded))\n",
    "    max_target_len = max(len(indexes) for indexes in output_indexes)\n",
    "\n",
    "    return input_padded, input_lengths, output_padded, output_mask, max_target_len\n",
    "\n",
    "\n",
    "# Example for validation\n",
    "small_batch_size = 5\n",
    "batches = batch_to_train_data(\n",
    "    voc, [random.choice(pairs) for _ in range(small_batch_size)]\n",
    ")\n",
    "input_variable, lengths, target_variable, mask, max_target_len = batches\n",
    "\n",
    "print(\"input_variable:\", input_variable)\n",
    "print(\"lengths:\", lengths)\n",
    "print(\"target_variable:\", target_variable)\n",
    "print(\"mask:\", mask)\n",
    "print(\"max_target_len:\", max_target_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Procedure Definition\n",
    "\n",
    "### Loss with Masking\n",
    "\n",
    "Given that we're working with batches of padded sequences, we can't compute loss using all tensor elements. We establish `mask_nll_loss` to compute our loss based on the decoder's output tensor, the target tensor, and a binary mask tensor that indicates the padding of the target tensor. This loss function computes the average negative log likelihood of the elements that align with a *1* in the mask tensor.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KSjDpLkxI8VX"
   },
   "outputs": [],
   "source": [
    "def mask_nll_loss(inp, target, mask, device):\n",
    "    \"\"\"\n",
    "    Calculate the negative log likelihood loss with a mask over the lengths of target sequences.\n",
    "    \"\"\"\n",
    "    n_total = mask.sum()\n",
    "    cross_entropy = -torch.log(torch.gather(inp, 1, target.view(-1, 1)).squeeze(1))\n",
    "    loss = cross_entropy.masked_select(mask).mean()\n",
    "    loss = loss.to(device)\n",
    "    return loss, n_total.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Training Iteration Procedure\n",
    "\n",
    "The `train` function encapsulates the process for a single training iteration (a single batch of inputs).\n",
    "\n",
    "We employ two strategies to aid convergence:\n",
    "\n",
    "-  **Teacher forcing**: At a probability determined by `teacher_forcing_ratio`, we use the current target word as the decoder’s next input instead of the decoder’s current guess. This helps in efficient training but can cause instability during inference. Hence, the `teacher_forcing_ratio` must be set carefully.\n",
    "\n",
    "-  **Gradient clipping**: This technique counters the \"exploding gradient\" problem by capping gradients to a maximum value, preventing them from growing exponentially and causing overflow or overshooting steep cost function cliffs.\n",
    "\n",
    "**Procedure:**\n",
    "\n",
    "   1) Pass the entire input batch through the encoder.\n",
    "   2) Initialize decoder inputs as SOS_token, and hidden state as the encoder's final hidden state.\n",
    "   3) Pass the input batch sequence through the decoder one time step at a time.\n",
    "   4) If teacher forcing: set next decoder input as the current target; else: set next decoder input as current decoder output.\n",
    "   5) Calculate and accumulate loss.\n",
    "   6) Perform backpropagation.\n",
    "   7) Clip gradients.\n",
    "   8) Update encoder and decoder model parameters.\n",
    "\n",
    "Note: PyTorch’s RNN modules (`RNN`, `LSTM`, `GRU`) can be used like any other non-recurrent layers by passing them the entire input sequence. We use the `GRU` layer like this in the `encoder`. However, you can also run these modules one time-step at a time, as we do for the `decoder` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NF5Kt7PBI9qq"
   },
   "outputs": [],
   "source": [
    "def train(\n",
    "    input_variable,\n",
    "    lengths,\n",
    "    target_variable,\n",
    "    mask,\n",
    "    max_target_len,\n",
    "    encoder,\n",
    "    decoder,\n",
    "    embedding,\n",
    "    encoder_optimizer,\n",
    "    decoder_optimizer,\n",
    "    batch_size,\n",
    "    clip,\n",
    "    max_length=MAX_LENGTH,\n",
    "):\n",
    "\n",
    "    # Zero gradients\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    # Set device options\n",
    "    input_variable = input_variable.to(device)\n",
    "    target_variable = target_variable.to(device)\n",
    "    mask = mask.to(device)\n",
    "    # Lengths for RNN packing should always be on the CPU\n",
    "    lengths = lengths.to(\"cpu\")\n",
    "\n",
    "    # Initialize variables\n",
    "    loss = 0\n",
    "    print_losses = []\n",
    "    n_totals = 0\n",
    "\n",
    "    # Forward pass through encoder\n",
    "    encoder_outputs, encoder_hidden = encoder(input_variable, lengths)\n",
    "\n",
    "    # Create initial decoder input (start with SOS tokens for each sentence)\n",
    "    decoder_input = torch.LongTensor([[SOS_TOKEN for _ in range(batch_size)]])\n",
    "    decoder_input = decoder_input.to(device)\n",
    "\n",
    "    # Set initial decoder hidden state to the encoder's final hidden state\n",
    "    decoder_hidden = encoder_hidden[: decoder.n_layers]\n",
    "\n",
    "    # Determine if we are using teacher forcing this iteration\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    # Forward batch of sequences through decoder one time step at a time\n",
    "    if use_teacher_forcing:\n",
    "        for t in range(max_target_len):\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "            # Teacher forcing: next input is current target\n",
    "            decoder_input = target_variable[t].view(1, -1)\n",
    "            # Calculate and accumulate loss\n",
    "            mask_loss, nTotal = mask_nll_loss(\n",
    "                decoder_output, target_variable[t], mask[t], device\n",
    "            )\n",
    "            loss += mask_loss\n",
    "            print_losses.append(mask_loss.item() * nTotal)\n",
    "            n_totals += nTotal\n",
    "    else:\n",
    "        for t in range(max_target_len):\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "            # No teacher forcing: next input is decoder's own current output\n",
    "            _, topi = decoder_output.topk(1)\n",
    "            decoder_input = torch.LongTensor([[topi[i][0] for i in range(batch_size)]])\n",
    "            decoder_input = decoder_input.to(device)\n",
    "            # Calculate and accumulate loss\n",
    "            mask_loss, nTotal = mask_nll_loss(\n",
    "                decoder_output, target_variable[t], mask[t], device\n",
    "            )\n",
    "            loss += mask_loss\n",
    "            print_losses.append(mask_loss.item() * nTotal)\n",
    "            n_totals += nTotal\n",
    "\n",
    "    # Perform backpropagation\n",
    "    loss.backward()\n",
    "\n",
    "    # Clip gradients: gradients are modified in place\n",
    "    _ = nn.utils.clip_grad_norm_(encoder.parameters(), clip)\n",
    "    _ = nn.utils.clip_grad_norm_(decoder.parameters(), clip)\n",
    "\n",
    "    # Adjust model weights\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return sum(print_losses) / n_totals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Iterations\n",
    "\n",
    "Now we can integrate the complete training procedure with the data. The `train_iters` function executes `n_iterations` of training using the provided models, optimizers, data, etc. Most of the complex work is handled by the `train` function.\n",
    "\n",
    "It's important to note that when we save our model, we store a tarball that includes the encoder and decoder `state_dicts` (parameters), the optimizers' `state_dicts`, the loss, the iteration, etc. Saving the model in this way provides maximum flexibility with the checkpoint. After loading a checkpoint, we can either use the model parameters to run inference or continue training from where we left off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S7BAQWthJAxf"
   },
   "outputs": [],
   "source": [
    "def train_iters(\n",
    "    model_name,\n",
    "    voc,\n",
    "    pairs,\n",
    "    encoder,\n",
    "    decoder,\n",
    "    encoder_optimizer,\n",
    "    decoder_optimizer,\n",
    "    embedding,\n",
    "    encoder_n_layers,\n",
    "    decoder_n_layers,\n",
    "    save_dir,\n",
    "    n_iteration,\n",
    "    batch_size,\n",
    "    print_every,\n",
    "    save_every,\n",
    "    clip,\n",
    "    corpus_name,\n",
    "):\n",
    "    \"\"\"\n",
    "    Run training for a set number of iterations.\n",
    "    \"\"\"\n",
    "    # Load batches for each iteration\n",
    "    training_batches = [\n",
    "        batch_to_train_data(voc, [random.choice(pairs) for _ in range(batch_size)])\n",
    "        for _ in range(n_iteration)\n",
    "    ]\n",
    "\n",
    "    print(\"Initializing ...\")\n",
    "    start_iteration = 1\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "\n",
    "    print(\"Training...\")\n",
    "    for iteration in range(start_iteration, n_iteration + 1):\n",
    "        training_batch = training_batches[iteration - 1]\n",
    "        input_variable, lengths, target_variable, mask, max_target_len = training_batch\n",
    "\n",
    "        # Run a training iteration with batch\n",
    "        loss = train(\n",
    "            input_variable,\n",
    "            lengths,\n",
    "            target_variable,\n",
    "            mask,\n",
    "            max_target_len,\n",
    "            encoder,\n",
    "            decoder,\n",
    "            embedding,\n",
    "            encoder_optimizer,\n",
    "            decoder_optimizer,\n",
    "            batch_size,\n",
    "            clip,\n",
    "        )\n",
    "        print_loss_total += loss\n",
    "\n",
    "        # Print progress\n",
    "        if iteration % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print(\n",
    "                f\"Iteration: {iteration}; Percent complete: {iteration / n_iteration * 100:.1f}%; Average loss: {print_loss_avg:.4f}\"\n",
    "            )\n",
    "            print_loss_total = 0\n",
    "\n",
    "        # Save checkpoint\n",
    "        if iteration % save_every == 0:\n",
    "            directory = os.path.join(\n",
    "                save_dir,\n",
    "                model_name,\n",
    "                corpus_name,\n",
    "                f\"{encoder_n_layers}-{decoder_n_layers}_{hidden_features}\",\n",
    "            )\n",
    "            if not os.path.exists(directory):\n",
    "                os.makedirs(directory)\n",
    "            torch.save(\n",
    "                {\n",
    "                    \"iteration\": iteration,\n",
    "                    \"en\": encoder.state_dict(),\n",
    "                    \"de\": decoder.state_dict(),\n",
    "                    \"en_opt\": encoder_optimizer.state_dict(),\n",
    "                    \"de_opt\": decoder_optimizer.state_dict(),\n",
    "                    \"loss\": loss,\n",
    "                    \"voc_dict\": voc.__dict__,\n",
    "                    \"embedding\": embedding.state_dict(),\n",
    "                },\n",
    "                os.path.join(directory, f\"{iteration}_checkpoint.tar\"),\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "After training the model, we want to interact with the bot. We need to define how the model decodes the encoded input.\n",
    "\n",
    "### Greedy Decoding\n",
    "\n",
    "Greedy decoding is used during training when we're not using teacher forcing. At each time step, we choose the word from `decoder_output` with the highest softmax value. This method is optimal at a single time-step level.\n",
    "\n",
    "We define a `GreedySearchDecoder` class to perform greedy decoding. The input sentence is evaluated as follows:\n",
    "\n",
    "**Computation Steps:**\n",
    "\n",
    "   1) Pass input through the encoder model.\n",
    "   2) Prepare the encoder's final hidden layer to be the first hidden input to the decoder.\n",
    "   3) Initialize the decoder's first input as SOS_token.\n",
    "   4) Initialize tensors to append decoded words to.\n",
    "   5) Iteratively decode one word token at a time:\n",
    "       a) Pass through the decoder.\n",
    "       b) Obtain the most likely word token and its softmax score.\n",
    "       c) Record the token and score.\n",
    "       d) Prepare the current token to be the next decoder input.\n",
    "   6) Return collections of word tokens and scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eGzjE-ggJCcZ"
   },
   "outputs": [],
   "source": [
    "class GreedySearchDecoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        \"\"\"\n",
    "        Greedy decoding module initialization.\n",
    "        \"\"\"\n",
    "        super(GreedySearchDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, input_seq, input_length, max_length):\n",
    "        \"\"\"\n",
    "        Forward propagation of the input to produce a sequence of tokens.\n",
    "        \"\"\"\n",
    "        # Forward input through encoder model\n",
    "        encoder_outputs, encoder_hidden = self.encoder(input_seq, input_length)\n",
    "\n",
    "        # Prepare encoder's final hidden layer to be first hidden input to the decoder\n",
    "        decoder_hidden = encoder_hidden[: self.decoder.n_layers]\n",
    "\n",
    "        # Initialize decoder input with SOS_token\n",
    "        decoder_input = torch.tensor([[SOS_TOKEN]], device=device, dtype=torch.long)\n",
    "\n",
    "        # Initialize tensors to append decoded words to\n",
    "        all_tokens = torch.zeros(0, dtype=torch.long, device=device)\n",
    "        all_scores = torch.zeros(0, device=device)\n",
    "\n",
    "        # Iteratively decode one word token at a time\n",
    "        for _ in range(max_length):\n",
    "            # Forward pass through decoder\n",
    "            decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden)\n",
    "\n",
    "            # Obtain most likely word token and its softmax score\n",
    "            decoder_scores, decoder_input = torch.max(decoder_output, dim=1)\n",
    "\n",
    "            # Record token and score\n",
    "            all_tokens = torch.cat((all_tokens, decoder_input), dim=0)\n",
    "            all_scores = torch.cat((all_scores, decoder_scores), dim=0)\n",
    "\n",
    "            # Prepare current token to be next decoder input (add a dimension)\n",
    "            decoder_input = torch.unsqueeze(decoder_input, 0)\n",
    "\n",
    "        # Return collections of word tokens and scores\n",
    "        return all_tokens, all_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Evaluation\n",
    "\n",
    "With our decoding method defined, we can create functions to evaluate a string input sentence. The `evaluate` function handles the input sentence, it formats the sentence as an input batch of word indexes with *batch_size==1*. This is done by converting the sentence words to their corresponding indexes and transposing the dimensions to prepare the tensor for our models. A `lengths` tensor is also created which contains the length of our input sentence. The decoded response sentence tensor is obtained using our `GreedySearchDecoder` object (`searcher`). Finally, the response’s indexes are converted to words and the list of decoded words is returned.\n",
    "\n",
    "`evaluate_input` serves as the user interface for our chatbot. It prompts an input text field where we can enter our query sentence. After entering our input sentence and pressing *Enter*, our text is normalized like our training data, and is fed to the `evaluate` function to obtain a decoded output sentence. This process is looped for continuous interaction with our bot until we enter either “q” or “quit”.\n",
    "\n",
    "If a sentence is entered that contains a word not in the vocabulary, an error message is printed and the user is prompted to enter another sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7LY0AotNJEzk"
   },
   "outputs": [],
   "source": [
    "def evaluate(searcher, voc, sentence, max_length=MAX_LENGTH):\n",
    "    \"\"\"\n",
    "    Evaluate a sentence using the encoder, decoder, and searcher provided.\n",
    "    \"\"\"\n",
    "    # Prepare the input sentence as a batch of word indexes\n",
    "    indexes_batch = [indexes_from_sentence(voc, sentence)]\n",
    "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
    "    input_batch = torch.LongTensor(indexes_batch).transpose(0, 1).to(device)\n",
    "    lengths = lengths.to(\"cpu\")  # Lengths need to be on CPU for pack_padded_sequence\n",
    "\n",
    "    # Decode the sentence with the searcher\n",
    "    tokens, scores = searcher(input_batch, lengths, max_length)\n",
    "    decoded_words = [voc.index_to_word[token.item()] for token in tokens]\n",
    "\n",
    "    return decoded_words\n",
    "\n",
    "\n",
    "def evaluate_input(encoder, decoder, searcher, voc):\n",
    "    \"\"\"\n",
    "    Interactively evaluate input from the user.\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        try:\n",
    "            input_sentence = input(\"> \")\n",
    "            if input_sentence in (\"q\", \"quit\"):\n",
    "                break\n",
    "            print(input_sentence)\n",
    "            input_sentence = normalize_string(input_sentence)\n",
    "            output_words = evaluate(searcher, voc, input_sentence)\n",
    "            output_words = [word for word in output_words if word not in (\"EOS\", \"PAD\")]\n",
    "            print(\"Bot:\", \" \".join(output_words))\n",
    "\n",
    "        except KeyError:\n",
    "            print(\"Error: Encountered unknown word.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Models Overview\n",
    "\n",
    "### Seq2Seq Model\n",
    "\n",
    "Our chatbot uses a sequence-to-sequence (seq2seq) model, which takes a variable-length sequence as input and returns a variable-length sequence as output. This is achieved by using two separate recurrent neural nets (RNNs): an **encoder** and a **decoder**. The encoder encodes the input sequence into a fixed-length context vector, which theoretically contains semantic information about the input sentence. The decoder takes an input word and the context vector, and returns a guess for the next word in the sequence and a hidden state for the next iteration.\n",
    "\n",
    "### Encoder\n",
    "\n",
    "The encoder RNN iterates through the input sentence one token at a time, outputting an \"output\" vector and a \"hidden state\" vector at each time step. The hidden state vector is passed to the next time step, while the output vector is recorded. The encoder uses a multi-layered Gated Recurrent Unit (GRU) and a bidirectional variant of the GRU. An `embedding` layer is used to encode our word indices in an arbitrarily sized feature space. \n",
    "\n",
    "### Decoder\n",
    "\n",
    "The decoder RNN generates the response sentence in a token-by-token fashion. It uses the encoder’s context vectors, and internal hidden states to generate the next word in the sequence. To avoid information loss, especially when dealing with long input sequences, an \"attention mechanism\" is typically used that allows the decoder to pay attention to certain parts of the input sequence, rather than using the entire fixed context at every step. However, in this tutorial, we only consider the use of standard RNNs, which will limit the performance of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set configuration parameters for the model\n",
    "model_name = \"cb_model\"\n",
    "hidden_features = 500\n",
    "in_features = hidden_features\n",
    "encoder_n_layers = 2\n",
    "decoder_n_layers = 2\n",
    "rnn_type = \"GRU\"\n",
    "dropout = 0.1\n",
    "batch_size = 64\n",
    "\n",
    "# Set training and optimization parameters\n",
    "clip = 50.0\n",
    "teacher_forcing_ratio = 1.0\n",
    "learning_rate = 0.0001\n",
    "decoder_learning_ratio = 5.0\n",
    "n_iteration = 4000\n",
    "print_every = 1\n",
    "save_every = 500\n",
    "\n",
    "embedding = nn.Embedding(voc.num_words, hidden_features)\n",
    "# Initialize the EncoderRNN\n",
    "encoder = dl.EncoderRNN(\n",
    "    in_features=in_features,\n",
    "    hidden_features=hidden_features,\n",
    "    num_layers=encoder_n_layers,\n",
    "    embedding=embedding,\n",
    "    rnn_type=rnn_type,\n",
    "    dropout=dropout,\n",
    ").to(device)\n",
    "\n",
    "\n",
    "encoder.build()\n",
    "print(encoder)\n",
    "\n",
    "decoder = dl.DecoderRNN(\n",
    "    in_features=in_features,\n",
    "    hidden_features=hidden_features,\n",
    "    out_features=voc.num_words,\n",
    "    num_layers=decoder_n_layers,\n",
    "    embedding=embedding,\n",
    "    rnn_type=rnn_type,\n",
    "    dropout=dropout,\n",
    ").to(device)\n",
    "\n",
    "\n",
    "encoder.build()\n",
    "print(decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define and train model\n",
    "\n",
    "Now, we're ready to run our model!\n",
    "\n",
    "Whether we're training or testing the chatbot model, we need to initialize the encoder and decoder models. In the next block, we configure the parameters, and construct and initialize the models. You're encouraged to experiment with different model configurations to enhance performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize word embeddings and encoder & decoder models\n",
    "# Set models to training mode\n",
    "encoder.train()\n",
    "decoder.train()\n",
    "\n",
    "# Initialize optimizers for the encoder and decoder\n",
    "encoder_optimizer = torch.optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = torch.optim.Adam(\n",
    "    decoder.parameters(), lr=learning_rate * decoder_learning_ratio\n",
    ")\n",
    "\n",
    "# Move optimizer states to GPU if necessary\n",
    "if torch.cuda.is_available():\n",
    "    for state in encoder_optimizer.state.values():\n",
    "        for k, v in state.items():\n",
    "            if isinstance(v, torch.Tensor):\n",
    "                state[k] = v.cuda()\n",
    "\n",
    "    for state in decoder_optimizer.state.values():\n",
    "        for k, v in state.items():\n",
    "            if isinstance(v, torch.Tensor):\n",
    "                state[k] = v.cuda()\n",
    "\n",
    "# Begin the training process\n",
    "train_iters(\n",
    "    model_name,\n",
    "    voc,\n",
    "    pairs,\n",
    "    encoder,\n",
    "    decoder,\n",
    "    encoder_optimizer,\n",
    "    decoder_optimizer,\n",
    "    embedding,\n",
    "    encoder_n_layers,\n",
    "    decoder_n_layers,\n",
    "    save_dir,\n",
    "    n_iteration,\n",
    "    batch_size,\n",
    "    print_every,\n",
    "    save_every,\n",
    "    clip,\n",
    "    corpus_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1OSKoKwg87mR"
   },
   "outputs": [],
   "source": [
    "# Set dropout layers to ``eval`` mode\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "\n",
    "# Initialize search module\n",
    "searcher = GreedySearchDecoder(encoder, decoder)\n",
    "\n",
    "# Begin chatting (uncomment and run the following line to begin)\n",
    "evaluate_input(encoder, decoder, searcher, voc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's review the changes made to incorporate conversation history into your chatbot model training and evaluation:\n",
    "### Changes for Training with Conversation History\n",
    "\n",
    "create_history_pairs Function: This function creates training pairs that include the history of the conversation. For each exchange in the pairs, it appends a certain number of previous exchanges (up to MAX_HISTORY) separated by the <EOS> token. These pairs are then used for training.\n",
    "\n",
    "Batch Preparation (batch_to_train_data): The function for preparing a batch of training data remains largely the same. The only difference is that it now handles input sequences that include conversation history.\n",
    "\n",
    "Training Function (train):\n",
    "    The core training logic remains unchanged.\n",
    "    The function receives input sequences that now contain conversation history.\n",
    "    The forward pass through the encoder and the decoding steps are performed as usual, without any specific changes needed to accommodate the conversation history.\n",
    "\n",
    "Data Preparation for Training:\n",
    "    The history_pairs are generated using the create_history_pairs function.\n",
    "    These pairs are then used throughout the training process.\n",
    "\n",
    "### Changes for Evaluation with Conversation History\n",
    "\n",
    "evaluate Function:\n",
    "    The conversation history is now considered when evaluating a new input.\n",
    "    The history is concatenated with the current input sentence, separated by spaces.\n",
    "    The concatenated string is then processed and fed into the model for generating a response.\n",
    "\n",
    "evaluate_input Function:\n",
    "    Manages interactive evaluation with the user.\n",
    "    Maintains a conversation_history list, appending each user input and the model's response to it.\n",
    "    Passes the accumulated conversation history to the evaluate function for each new input.\n",
    "\n",
    "### Training and Evaluation Process\n",
    "\n",
    "The model is trained with input sequences that include conversation history, allowing it to learn the context of the conversation.\n",
    "During evaluation, the model uses the accumulated conversation history to generate more context-aware responses.\n",
    "\n",
    "### General Setup\n",
    "\n",
    "Model, optimizer, and training configurations are set according to your specifications.\n",
    "The training process (train_iters) follows the standard approach but uses the modified input pairs with conversation history.\n",
    "\n",
    "### Important Notes\n",
    "\n",
    "The effectiveness of including conversation history in the model depends on the depth of context the model can understand and how well it can handle longer input sequences.\n",
    "Fine-tuning and experimenting with the MAX_HISTORY parameter and the MAX_LENGTH of sequences may be necessary to achieve optimal performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9BnEtHHxUHC7"
   },
   "outputs": [],
   "source": [
    "MAX_LENGTH = 10  # Maximum length of a single sentence, adjust as necessary\n",
    "MAX_HISTORY = 5  # Number of previous exchanges to include in the history\n",
    "\n",
    "\n",
    "def create_history_pairs(pairs, max_history=MAX_HISTORY):\n",
    "    history_pairs = []\n",
    "    for i in range(len(pairs)):\n",
    "        dialogue_history = \" EOS \".join(\n",
    "            [pairs[j][0] for j in range(max(0, i - max_history), i)]\n",
    "        )\n",
    "        history_pairs.append([dialogue_history, pairs[i][1]])\n",
    "    return history_pairs\n",
    "\n",
    "\n",
    "def zero_padding(l, fillvalue=0):\n",
    "    return list(itertools.zip_longest(*l, fillvalue=fillvalue))\n",
    "\n",
    "\n",
    "def binary_matrix(l, value=0):\n",
    "    m = []\n",
    "    for i, seq in enumerate(l):\n",
    "        m.append([])\n",
    "        for token in seq:\n",
    "            if token == value:\n",
    "                m[i].append(0)\n",
    "            else:\n",
    "                m[i].append(1)\n",
    "    return m\n",
    "\n",
    "\n",
    "def output_var(l, voc):\n",
    "    indexes_batch = [indexes_from_sentence(voc, sentence) for sentence in l]\n",
    "    max_target_len = max([len(indexes) for indexes in indexes_batch])\n",
    "    pad_list = zero_padding(indexes_batch)\n",
    "    mask = binary_matrix(pad_list)\n",
    "    mask = torch.BoolTensor(mask)\n",
    "    pad_var = torch.LongTensor(pad_list)\n",
    "    return pad_var, mask, max_target_len\n",
    "\n",
    "\n",
    "def indexes_from_sentence(voc, sentence):\n",
    "    # return [voc.word_to_index[word] for word in sentence.split(' ')] + [EOS_TOKEN]\n",
    "    return [voc.word_to_index[word] for word in sentence.split()] + [EOS_TOKEN]\n",
    "\n",
    "\n",
    "def batch_to_train_data(voc, pair_batch):\n",
    "    pair_batch.sort(key=lambda p: len(p[0].split()), reverse=True)\n",
    "    input_batch, output_batch = [], []\n",
    "    for pair in pair_batch:\n",
    "        input_batch.append(pair[0])\n",
    "        output_batch.append(pair[1])\n",
    "    inp, lengths = input_var(input_batch, voc)\n",
    "    output, mask, max_target_len = output_var(output_batch, voc)\n",
    "    return inp, lengths, output, mask, max_target_len\n",
    "\n",
    "\n",
    "# Input_var function needs to calculate the correct lengths for packed sequences\n",
    "def input_var(l, voc):\n",
    "    indexes_batch = [indexes_from_sentence(voc, sentence) for sentence in l]\n",
    "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
    "    pad_list = zero_padding(indexes_batch)\n",
    "    pad_var = torch.LongTensor(pad_list)\n",
    "    return pad_var, lengths\n",
    "\n",
    "\n",
    "# Prepare the data with history\n",
    "history_pairs = create_history_pairs(pairs)\n",
    "\n",
    "\n",
    "# Modify the training function to handle the dialogue history in the input sequences\n",
    "def train(\n",
    "    input_variable,\n",
    "    lengths,\n",
    "    target_variable,\n",
    "    mask,\n",
    "    max_target_len,\n",
    "    encoder,\n",
    "    decoder,\n",
    "    embedding,\n",
    "    encoder_optimizer,\n",
    "    decoder_optimizer,\n",
    "    batch_size,\n",
    "    clip,\n",
    "    max_length=MAX_LENGTH,\n",
    "):\n",
    "\n",
    "    # Zero gradients\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    # Set device options\n",
    "    input_variable = input_variable.to(device)\n",
    "    target_variable = target_variable.to(device)\n",
    "    mask = mask.to(device)\n",
    "    # Lengths for RNN packing should always be on the CPU\n",
    "    lengths = lengths.to(\"cpu\")\n",
    "\n",
    "    # Initialize variables\n",
    "    loss = 0\n",
    "    print_losses = []\n",
    "    n_totals = 0\n",
    "\n",
    "    # Forward pass through encoder\n",
    "    encoder_outputs, encoder_hidden = encoder(input_variable, lengths)\n",
    "\n",
    "    # Create initial decoder input (start with SOS tokens for each sentence)\n",
    "    decoder_input = torch.LongTensor([[SOS_TOKEN for _ in range(batch_size)]])\n",
    "    decoder_input = decoder_input.to(device)\n",
    "\n",
    "    # Set initial decoder hidden state to the encoder's final hidden state\n",
    "    decoder_hidden = encoder_hidden[: decoder.n_layers]\n",
    "\n",
    "    # Determine if we are using teacher forcing this iteration\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    # Forward batch of sequences through decoder one time step at a time\n",
    "    if use_teacher_forcing:\n",
    "        for t in range(max_target_len):\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "            # Teacher forcing: next input is current target\n",
    "            decoder_input = target_variable[t].view(1, -1)\n",
    "            # Calculate and accumulate loss\n",
    "            mask_loss, nTotal = mask_nll_loss(\n",
    "                decoder_output, target_variable[t], mask[t], device\n",
    "            )\n",
    "            loss += mask_loss\n",
    "            print_losses.append(mask_loss.item() * nTotal)\n",
    "            n_totals += nTotal\n",
    "    else:\n",
    "        for t in range(max_target_len):\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "            # No teacher forcing: next input is decoder's own current output\n",
    "            _, topi = decoder_output.topk(1)\n",
    "            decoder_input = torch.LongTensor([[topi[i][0] for i in range(batch_size)]])\n",
    "            decoder_input = decoder_input.to(device)\n",
    "            # Calculate and accumulate loss\n",
    "            mask_loss, nTotal = mask_nll_loss(\n",
    "                decoder_output, target_variable[t], mask[t], device\n",
    "            )\n",
    "            loss += mask_loss\n",
    "            print_losses.append(mask_loss.item() * nTotal)\n",
    "            n_totals += nTotal\n",
    "\n",
    "    # Perform backpropagation\n",
    "    loss.backward()\n",
    "\n",
    "    # Clip gradients: gradients are modified in place\n",
    "    _ = nn.utils.clip_grad_norm_(encoder.parameters(), clip)\n",
    "    _ = nn.utils.clip_grad_norm_(decoder.parameters(), clip)\n",
    "\n",
    "    # Adjust model weights\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return sum(print_losses) / n_totals\n",
    "\n",
    "\n",
    "def train_iters(\n",
    "    model_name,\n",
    "    voc,\n",
    "    history_pairs,\n",
    "    encoder,\n",
    "    decoder,\n",
    "    encoder_optimizer,\n",
    "    decoder_optimizer,\n",
    "    embedding,\n",
    "    encoder_n_layers,\n",
    "    decoder_n_layers,\n",
    "    save_dir,\n",
    "    n_iteration,\n",
    "    batch_size,\n",
    "    print_every,\n",
    "    save_every,\n",
    "    clip,\n",
    "    corpus_name,\n",
    "):\n",
    "    \"\"\"\n",
    "    Run training for a set number of iterations.\n",
    "    \"\"\"\n",
    "    # Load batches for each iteration\n",
    "    training_batches = [\n",
    "        batch_to_train_data(\n",
    "            voc, [random.choice(history_pairs) for _ in range(batch_size)]\n",
    "        )\n",
    "        for _ in range(n_iteration)\n",
    "    ]\n",
    "\n",
    "    print(\"Initializing ...\")\n",
    "    start_iteration = 1\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "\n",
    "    print(\"Training...\")\n",
    "    for iteration in range(start_iteration, n_iteration + 1):\n",
    "        training_batch = training_batches[iteration - 1]\n",
    "        input_variable, lengths, target_variable, mask, max_target_len = training_batch\n",
    "\n",
    "        # Run a training iteration with batch\n",
    "        loss = train(\n",
    "            input_variable,\n",
    "            lengths,\n",
    "            target_variable,\n",
    "            mask,\n",
    "            max_target_len,\n",
    "            encoder,\n",
    "            decoder,\n",
    "            embedding,\n",
    "            encoder_optimizer,\n",
    "            decoder_optimizer,\n",
    "            batch_size,\n",
    "            clip,\n",
    "        )\n",
    "        print_loss_total += loss\n",
    "\n",
    "        # Print progress\n",
    "        if iteration % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print(\n",
    "                f\"Iteration: {iteration}; Percent complete: {iteration / n_iteration * 100:.1f}%; Average loss: {print_loss_avg:.4f}\"\n",
    "            )\n",
    "            print_loss_total = 0\n",
    "\n",
    "        # Save checkpoint\n",
    "        if iteration % save_every == 0:\n",
    "            directory = os.path.join(\n",
    "                save_dir,\n",
    "                model_name,\n",
    "                corpus_name,\n",
    "                f\"{encoder_n_layers}-{decoder_n_layers}_{hidden_features}\",\n",
    "            )\n",
    "            if not os.path.exists(directory):\n",
    "                os.makedirs(directory)\n",
    "            torch.save(\n",
    "                {\n",
    "                    \"iteration\": iteration,\n",
    "                    \"en\": encoder.state_dict(),\n",
    "                    \"de\": decoder.state_dict(),\n",
    "                    \"en_opt\": encoder_optimizer.state_dict(),\n",
    "                    \"de_opt\": decoder_optimizer.state_dict(),\n",
    "                    \"loss\": loss,\n",
    "                    \"voc_dict\": voc.__dict__,\n",
    "                    \"embedding\": embedding.state_dict(),\n",
    "                },\n",
    "                os.path.join(directory, f\"{iteration}_checkpoint.tar\"),\n",
    "            )\n",
    "\n",
    "\n",
    "def evaluate(searcher, voc, conversation_history, max_length=MAX_LENGTH):\n",
    "    \"\"\"\n",
    "    Evaluate a conversation history using the encoder, decoder, and searcher provided.\n",
    "    \"\"\"\n",
    "    # Join the conversation history into a single input and normalize\n",
    "    input_sentence = \" \".join(conversation_history)\n",
    "    input_sentence = normalize_string(input_sentence)\n",
    "\n",
    "    # Prepare the input sentence as a batch of word indexes\n",
    "    indexes_batch = [indexes_from_sentence(voc, input_sentence)]\n",
    "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
    "    input_batch = torch.LongTensor(indexes_batch).transpose(0, 1).to(device)\n",
    "    lengths = lengths.to(\"cpu\")  # Lengths need to be on CPU for pack_padded_sequence\n",
    "\n",
    "    # Decode the sentence with the searcher\n",
    "    tokens, scores = searcher(input_batch, lengths, max_length)\n",
    "    decoded_words = [voc.index_to_word[token.item()] for token in tokens]\n",
    "\n",
    "    return decoded_words\n",
    "\n",
    "\n",
    "def evaluate_input(searcher, voc):\n",
    "    \"\"\"\n",
    "    Interactively evaluate input from the user, considering the entire conversation history.\n",
    "    \"\"\"\n",
    "    conversation_history = []\n",
    "    while True:\n",
    "        try:\n",
    "            input_sentence = input(\"> \")\n",
    "            if input_sentence in (\"q\", \"quit\"):\n",
    "                break\n",
    "\n",
    "            # Normalize and add the user's input to the conversation history\n",
    "            input_sentence = normalize_string(input_sentence)\n",
    "            conversation_history.append(input_sentence)\n",
    "\n",
    "            # Evaluate the conversation history\n",
    "            output_words = evaluate(searcher, voc, conversation_history)\n",
    "            output_words = [word for word in output_words if word not in (\"EOS\", \"PAD\")]\n",
    "            print(\"Bot:\", \" \".join(output_words))\n",
    "\n",
    "            # Add the bot's response to the conversation history\n",
    "            conversation_history.extend(output_words)\n",
    "            conversation_history = conversation_history[-MAX_HISTORY * MAX_LENGTH :]\n",
    "\n",
    "        except KeyError:\n",
    "            print(\"Error: Encountered unknown word.\")\n",
    "\n",
    "\n",
    "# Set configuration parameters for the model\n",
    "model_name = \"cb_model\"\n",
    "hidden_features = 2000\n",
    "in_features = hidden_features\n",
    "encoder_n_layers = 4\n",
    "decoder_n_layers = 4\n",
    "dropout = 0.1\n",
    "batch_size = 64\n",
    "\n",
    "# Set training and optimization parameters\n",
    "clip = 50.0\n",
    "teacher_forcing_ratio = 1.0\n",
    "learning_rate = 0.0001\n",
    "decoder_learning_ratio = 5.0\n",
    "n_iteration = 16000\n",
    "print_every = 1\n",
    "save_every = 8000\n",
    "\n",
    "# Initialize word embeddings and encoder & decoder models\n",
    "embedding = nn.Embedding(voc.num_words, hidden_features)\n",
    "# Initialize the EncoderRNN\n",
    "encoder = dl.EncoderRNN(\n",
    "    in_features=in_features,\n",
    "    hidden_features=hidden_features,\n",
    "    num_layers=encoder_n_layers,\n",
    "    embedding=embedding,\n",
    "    rnn_type=rnn_type,\n",
    "    dropout=dropout,\n",
    ").to(device)\n",
    "\n",
    "decoder = dl.DecoderRNN(\n",
    "    in_features=in_features,\n",
    "    hidden_features=hidden_features,\n",
    "    out_features=voc.num_words,\n",
    "    num_layers=decoder_n_layers,\n",
    "    embedding=embedding,\n",
    "    rnn_type=rnn_type,\n",
    "    dropout=dropout,\n",
    ").to(device)\n",
    "\n",
    "encoder.build()\n",
    "decoder.build()\n",
    "# Set models to training mode\n",
    "encoder.train()\n",
    "decoder.train()\n",
    "\n",
    "# Initialize optimizers for the encoder and decoder\n",
    "encoder_optimizer = torch.optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = torch.optim.Adam(\n",
    "    decoder.parameters(), lr=learning_rate * decoder_learning_ratio\n",
    ")\n",
    "\n",
    "# Move optimizer states to GPU if necessary\n",
    "if torch.cuda.is_available():\n",
    "    for state in encoder_optimizer.state.values():\n",
    "        for k, v in state.items():\n",
    "            if isinstance(v, torch.Tensor):\n",
    "                state[k] = v.cuda()\n",
    "\n",
    "    for state in decoder_optimizer.state.values():\n",
    "        for k, v in state.items():\n",
    "            if isinstance(v, torch.Tensor):\n",
    "                state[k] = v.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Begin the training process\n",
    "train_iters(\n",
    "    model_name,\n",
    "    voc,\n",
    "    history_pairs,\n",
    "    encoder,\n",
    "    decoder,\n",
    "    encoder_optimizer,\n",
    "    decoder_optimizer,\n",
    "    embedding,\n",
    "    encoder_n_layers,\n",
    "    decoder_n_layers,\n",
    "    save_dir,\n",
    "    n_iteration,\n",
    "    batch_size,\n",
    "    print_every,\n",
    "    save_every,\n",
    "    clip,\n",
    "    corpus_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XRpMW5HFdtdo"
   },
   "outputs": [],
   "source": [
    "def evaluate(searcher, voc, conversation_history, max_length=MAX_LENGTH):\n",
    "    \"\"\"\n",
    "    Evaluate a conversation history using the encoder, decoder, and searcher provided.\n",
    "    \"\"\"\n",
    "    # Join the conversation history into a single input and normalize\n",
    "    input_sentence = \" \".join(conversation_history)\n",
    "    input_sentence = normalize_string(input_sentence)\n",
    "    input_sentence = input_sentence.replace(\"eos\", \"EOS\")\n",
    "\n",
    "    # Prepare the input sentence as a batch of word indexes\n",
    "    indexes_batch = [indexes_from_sentence(voc, input_sentence)]\n",
    "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
    "    input_batch = torch.LongTensor(indexes_batch).transpose(0, 1).to(device)\n",
    "    lengths = lengths.to(\"cpu\")  # Lengths need to be on CPU for pack_padded_sequence\n",
    "\n",
    "    # Decode the sentence with the searcher\n",
    "    tokens, scores = searcher(input_batch, lengths, max_length)\n",
    "    decoded_words = [voc.index_to_word[token.item()] for token in tokens]\n",
    "\n",
    "    return decoded_words\n",
    "\n",
    "\n",
    "def evaluate_input(searcher, voc):\n",
    "    \"\"\"\n",
    "    Interactively evaluate input from the user, considering the entire conversation history.\n",
    "    \"\"\"\n",
    "    conversation_history = []\n",
    "    while True:\n",
    "        try:\n",
    "            input_sentence = input(\"> \")\n",
    "            if input_sentence in (\"q\", \"quit\"):\n",
    "                break\n",
    "\n",
    "            # Normalize and add the user's input to the conversation history\n",
    "            print(input_sentence)\n",
    "            input_sentence = normalize_string(input_sentence)\n",
    "            conversation_history.append(input_sentence)\n",
    "            conversation_history.append(\"EOS\")\n",
    "\n",
    "            # Evaluate the conversation history\n",
    "            output_words = evaluate(searcher, voc, conversation_history)\n",
    "            print(\n",
    "                \"Bot:\",\n",
    "                \" \".join([word for word in output_words if word not in (\"EOS\", \"PAD\")]),\n",
    "            )\n",
    "\n",
    "            # Add the bot's response to the conversation history\n",
    "            conversation_history.extend(output_words)\n",
    "            conversation_history = conversation_history[-MAX_HISTORY * MAX_LENGTH :]\n",
    "\n",
    "        except KeyError:\n",
    "            print(\"Error: Encountered unknown word.\")\n",
    "\n",
    "\n",
    "# Set dropout layers to ``eval`` mode\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "\n",
    "# Initialize search module\n",
    "searcher = GreedySearchDecoder(encoder, decoder)\n",
    "\n",
    "# Begin chatting (uncomment and run the following line to begin)\n",
    "evaluate_input(searcher, voc)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
