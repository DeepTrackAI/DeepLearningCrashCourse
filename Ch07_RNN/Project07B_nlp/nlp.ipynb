{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this guide, we delve into an engaging and intriguing application of recurrent sequence-to-sequence models by training a basic chatbot using film scripts from the Cornell Movie-Dialogs Corpus.\n",
    "\n",
    "Conversational models are currently a trending subject in the field of AI research. Chatbots are commonly used in various scenarios such as customer service platforms and online help desks. These bots typically utilize retrieval-based models that provide pre-set responses to specific types of questions. While these models might be adequate for highly specific domains like a company's IT helpdesk, they lack the robustness required for broader applications. However, the recent surge in deep learning, spearheaded recently by ChatGPT, has led to the development of potent multi-domain generative conversational models. In this guide, we will create one such model using the tools we have learnt so far.\n",
    "\n",
    "To begin, download the dialog dataset: \n",
    "\n",
    "https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html (should be replaced with direct data source for DLCC)\n",
    "\n",
    "and put in a ``data/`` directory under the current directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://zissou.infosci.cornell.edu/convokit/datasets/movie-corpus/movie-corpus.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import csv\n",
    "import re\n",
    "import os\n",
    "import unicodedata\n",
    "from io import open\n",
    "import json\n",
    "import deeplay as dl\n",
    "\n",
    "# Check for CUDA availability for PyTorch\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n",
    "\n",
    "# Mount Google Drive if using Google Colab\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/gdrive')\n",
    "#os.chdir(\"/content/gdrive/My Drive\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing\n",
    "\n",
    "This step involves reorganizing our data file and loading the data into formats that are manageable.\n",
    "\n",
    "The [Cornell Movie-Dialogs Corpus](https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html) is a comprehensive dataset of dialogues from movie characters:\n",
    "\n",
    "- It contains 220,579 conversational exchanges between 10,292 pairs of movie characters.\n",
    "- It features 9,035 characters from 617 movies.\n",
    "- It has a total of 304,713 utterances.\n",
    "\n",
    "This dataset is vast and varied, with a wide range of language formality, time periods, sentiment, etc. We anticipate that this diversity will make our model capable of handling a variety of inputs and queries.\n",
    "\n",
    "Initially, we will examine some lines from our data file to understand the original format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "hHZkeW4LEtZd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"id\": \"L1045\", \"conversation_id\": \"L1044\", \"text\": \"They do not!\", \"speaker\": \"u0\", \"meta\": {\"movie_id\": \"m0\", \"parsed\": [{\"rt\": 1, \"toks\": [{\"tok\": \"They\", \"tag\": \"PRP\", \"dep\": \"nsubj\", \"up\": 1, \"dn\": []}, {\"tok\": \"do\", \"tag\": \"VBP\", \"dep\": \"ROOT\", \"dn\": [0, 2, 3]}, {\"tok\": \"not\", \"tag\": \"RB\", \"dep\": \"neg\", \"up\": 1, \"dn\": []}, {\"tok\": \"!\", \"tag\": \".\", \"dep\": \"punct\", \"up\": 1, \"dn\": []}]}]}, \"reply-to\": \"L1044\", \"timestamp\": null, \"vectors\": []}\n",
      "{\"id\": \"L1044\", \"conversation_id\": \"L1044\", \"text\": \"They do to!\", \"speaker\": \"u2\", \"meta\": {\"movie_id\": \"m0\", \"parsed\": [{\"rt\": 1, \"toks\": [{\"tok\": \"They\", \"tag\": \"PRP\", \"dep\": \"nsubj\", \"up\": 1, \"dn\": []}, {\"tok\": \"do\", \"tag\": \"VBP\", \"dep\": \"ROOT\", \"dn\": [0, 2, 3]}, {\"tok\": \"to\", \"tag\": \"TO\", \"dep\": \"dobj\", \"up\": 1, \"dn\": []}, {\"tok\": \"!\", \"tag\": \".\", \"dep\": \"punct\", \"up\": 1, \"dn\": []}]}]}, \"reply-to\": null, \"timestamp\": null, \"vectors\": []}\n",
      "{\"id\": \"L985\", \"conversation_id\": \"L984\", \"text\": \"I hope so.\", \"speaker\": \"u0\", \"meta\": {\"movie_id\": \"m0\", \"parsed\": [{\"rt\": 1, \"toks\": [{\"tok\": \"I\", \"tag\": \"PRP\", \"dep\": \"nsubj\", \"up\": 1, \"dn\": []}, {\"tok\": \"hope\", \"tag\": \"VBP\", \"dep\": \"ROOT\", \"dn\": [0, 2, 3]}, {\"tok\": \"so\", \"tag\": \"RB\", \"dep\": \"advmod\", \"up\": 1, \"dn\": []}, {\"tok\": \".\", \"tag\": \".\", \"dep\": \"punct\", \"up\": 1, \"dn\": []}]}]}, \"reply-to\": \"L984\", \"timestamp\": null, \"vectors\": []}\n",
      "{\"id\": \"L984\", \"conversation_id\": \"L984\", \"text\": \"She okay?\", \"speaker\": \"u2\", \"meta\": {\"movie_id\": \"m0\", \"parsed\": [{\"rt\": 1, \"toks\": [{\"tok\": \"She\", \"tag\": \"PRP\", \"dep\": \"nsubj\", \"up\": 1, \"dn\": []}, {\"tok\": \"okay\", \"tag\": \"RB\", \"dep\": \"ROOT\", \"dn\": [0, 2]}, {\"tok\": \"?\", \"tag\": \".\", \"dep\": \"punct\", \"up\": 1, \"dn\": []}]}]}, \"reply-to\": null, \"timestamp\": null, \"vectors\": []}\n",
      "{\"id\": \"L925\", \"conversation_id\": \"L924\", \"text\": \"Let's go.\", \"speaker\": \"u0\", \"meta\": {\"movie_id\": \"m0\", \"parsed\": [{\"rt\": 0, \"toks\": [{\"tok\": \"Let\", \"tag\": \"VB\", \"dep\": \"ROOT\", \"dn\": [2, 3]}, {\"tok\": \"'s\", \"tag\": \"PRP\", \"dep\": \"nsubj\", \"up\": 2, \"dn\": []}, {\"tok\": \"go\", \"tag\": \"VB\", \"dep\": \"ccomp\", \"up\": 0, \"dn\": [1]}, {\"tok\": \".\", \"tag\": \".\", \"dep\": \"punct\", \"up\": 0, \"dn\": []}]}]}, \"reply-to\": \"L924\", \"timestamp\": null, \"vectors\": []}\n",
      "{\"id\": \"L924\", \"conversation_id\": \"L924\", \"text\": \"Wow\", \"speaker\": \"u2\", \"meta\": {\"movie_id\": \"m0\", \"parsed\": [{\"rt\": 0, \"toks\": [{\"tok\": \"Wow\", \"tag\": \"UH\", \"dep\": \"ROOT\", \"dn\": []}]}]}, \"reply-to\": null, \"timestamp\": null, \"vectors\": []}\n",
      "{\"id\": \"L872\", \"conversation_id\": \"L870\", \"text\": \"Okay -- you're gonna need to learn how to lie.\", \"speaker\": \"u0\", \"meta\": {\"movie_id\": \"m0\", \"parsed\": [{\"rt\": 4, \"toks\": [{\"tok\": \"Okay\", \"tag\": \"UH\", \"dep\": \"intj\", \"up\": 4, \"dn\": []}, {\"tok\": \"--\", \"tag\": \":\", \"dep\": \"punct\", \"up\": 4, \"dn\": []}, {\"tok\": \"you\", \"tag\": \"PRP\", \"dep\": \"nsubj\", \"up\": 4, \"dn\": []}, {\"tok\": \"'re\", \"tag\": \"VBP\", \"dep\": \"aux\", \"up\": 4, \"dn\": []}, {\"tok\": \"gon\", \"tag\": \"VBG\", \"dep\": \"ROOT\", \"dn\": [0, 1, 2, 3, 6, 12]}, {\"tok\": \"na\", \"tag\": \"TO\", \"dep\": \"aux\", \"up\": 6, \"dn\": []}, {\"tok\": \"need\", \"tag\": \"VB\", \"dep\": \"xcomp\", \"up\": 4, \"dn\": [5, 8]}, {\"tok\": \"to\", \"tag\": \"TO\", \"dep\": \"aux\", \"up\": 8, \"dn\": []}, {\"tok\": \"learn\", \"tag\": \"VB\", \"dep\": \"xcomp\", \"up\": 6, \"dn\": [7, 11]}, {\"tok\": \"how\", \"tag\": \"WRB\", \"dep\": \"advmod\", \"up\": 11, \"dn\": []}, {\"tok\": \"to\", \"tag\": \"TO\", \"dep\": \"aux\", \"up\": 11, \"dn\": []}, {\"tok\": \"lie\", \"tag\": \"VB\", \"dep\": \"xcomp\", \"up\": 8, \"dn\": [9, 10]}, {\"tok\": \".\", \"tag\": \".\", \"dep\": \"punct\", \"up\": 4, \"dn\": []}]}]}, \"reply-to\": \"L871\", \"timestamp\": null, \"vectors\": []}\n",
      "{\"id\": \"L871\", \"conversation_id\": \"L870\", \"text\": \"No\", \"speaker\": \"u2\", \"meta\": {\"movie_id\": \"m0\", \"parsed\": [{\"rt\": 0, \"toks\": [{\"tok\": \"No\", \"tag\": \"UH\", \"dep\": \"ROOT\", \"dn\": []}]}]}, \"reply-to\": \"L870\", \"timestamp\": null, \"vectors\": []}\n",
      "{\"id\": \"L870\", \"conversation_id\": \"L870\", \"text\": \"I'm kidding.  You know how sometimes you just become this \\\"persona\\\"?  And you don't know how to quit?\", \"speaker\": \"u0\", \"meta\": {\"movie_id\": \"m0\", \"parsed\": [{\"rt\": 2, \"toks\": [{\"tok\": \"I\", \"tag\": \"PRP\", \"dep\": \"nsubj\", \"up\": 2, \"dn\": []}, {\"tok\": \"'m\", \"tag\": \"VBP\", \"dep\": \"aux\", \"up\": 2, \"dn\": []}, {\"tok\": \"kidding\", \"tag\": \"VBG\", \"dep\": \"ROOT\", \"dn\": [0, 1, 3]}, {\"tok\": \".\", \"tag\": \".\", \"dep\": \"punct\", \"up\": 2, \"dn\": [4]}, {\"tok\": \" \", \"tag\": \"_SP\", \"dep\": \"\", \"up\": 3, \"dn\": []}]}, {\"rt\": 1, \"toks\": [{\"tok\": \"You\", \"tag\": \"PRP\", \"dep\": \"nsubj\", \"up\": 1, \"dn\": []}, {\"tok\": \"know\", \"tag\": \"VBP\", \"dep\": \"ROOT\", \"dn\": [0, 6, 11]}, {\"tok\": \"how\", \"tag\": \"WRB\", \"dep\": \"advmod\", \"up\": 3, \"dn\": []}, {\"tok\": \"sometimes\", \"tag\": \"RB\", \"dep\": \"advmod\", \"up\": 6, \"dn\": [2]}, {\"tok\": \"you\", \"tag\": \"PRP\", \"dep\": \"nsubj\", \"up\": 6, \"dn\": []}, {\"tok\": \"just\", \"tag\": \"RB\", \"dep\": \"advmod\", \"up\": 6, \"dn\": []}, {\"tok\": \"become\", \"tag\": \"VBP\", \"dep\": \"ccomp\", \"up\": 1, \"dn\": [3, 4, 5, 9]}, {\"tok\": \"this\", \"tag\": \"DT\", \"dep\": \"det\", \"up\": 9, \"dn\": []}, {\"tok\": \"\\\"\", \"tag\": \"``\", \"dep\": \"punct\", \"up\": 9, \"dn\": []}, {\"tok\": \"persona\", \"tag\": \"NN\", \"dep\": \"attr\", \"up\": 6, \"dn\": [7, 8, 10]}, {\"tok\": \"\\\"\", \"tag\": \"''\", \"dep\": \"punct\", \"up\": 9, \"dn\": []}, {\"tok\": \"?\", \"tag\": \".\", \"dep\": \"punct\", \"up\": 1, \"dn\": [12]}, {\"tok\": \" \", \"tag\": \"_SP\", \"dep\": \"\", \"up\": 11, \"dn\": []}]}, {\"rt\": 4, \"toks\": [{\"tok\": \"And\", \"tag\": \"CC\", \"dep\": \"cc\", \"up\": 4, \"dn\": []}, {\"tok\": \"you\", \"tag\": \"PRP\", \"dep\": \"nsubj\", \"up\": 4, \"dn\": []}, {\"tok\": \"do\", \"tag\": \"VBP\", \"dep\": \"aux\", \"up\": 4, \"dn\": []}, {\"tok\": \"n't\", \"tag\": \"RB\", \"dep\": \"neg\", \"up\": 4, \"dn\": []}, {\"tok\": \"know\", \"tag\": \"VB\", \"dep\": \"ROOT\", \"dn\": [0, 1, 2, 3, 7, 8]}, {\"tok\": \"how\", \"tag\": \"WRB\", \"dep\": \"advmod\", \"up\": 7, \"dn\": []}, {\"tok\": \"to\", \"tag\": \"TO\", \"dep\": \"aux\", \"up\": 7, \"dn\": []}, {\"tok\": \"quit\", \"tag\": \"VB\", \"dep\": \"xcomp\", \"up\": 4, \"dn\": [5, 6]}, {\"tok\": \"?\", \"tag\": \".\", \"dep\": \"punct\", \"up\": 4, \"dn\": []}]}]}, \"reply-to\": null, \"timestamp\": null, \"vectors\": []}\n",
      "{\"id\": \"L869\", \"conversation_id\": \"L866\", \"text\": \"Like my fear of wearing pastels?\", \"speaker\": \"u0\", \"meta\": {\"movie_id\": \"m0\", \"parsed\": [{\"rt\": 0, \"toks\": [{\"tok\": \"Like\", \"tag\": \"IN\", \"dep\": \"ROOT\", \"dn\": [2, 6]}, {\"tok\": \"my\", \"tag\": \"PRP$\", \"dep\": \"poss\", \"up\": 2, \"dn\": []}, {\"tok\": \"fear\", \"tag\": \"NN\", \"dep\": \"pobj\", \"up\": 0, \"dn\": [1, 3]}, {\"tok\": \"of\", \"tag\": \"IN\", \"dep\": \"prep\", \"up\": 2, \"dn\": [4]}, {\"tok\": \"wearing\", \"tag\": \"VBG\", \"dep\": \"pcomp\", \"up\": 3, \"dn\": [5]}, {\"tok\": \"pastels\", \"tag\": \"NNS\", \"dep\": \"dobj\", \"up\": 4, \"dn\": []}, {\"tok\": \"?\", \"tag\": \".\", \"dep\": \"punct\", \"up\": 0, \"dn\": []}]}]}, \"reply-to\": \"L868\", \"timestamp\": null, \"vectors\": []}\n"
     ]
    }
   ],
   "source": [
    "# Set the corpus name\n",
    "corpus_name = \"movie-corpus\"\n",
    "\n",
    "# Function to print first 'n' lines from a file\n",
    "def print_lines(file, n=10):\n",
    "    with open(file, 'r', encoding='utf-8') as datafile:\n",
    "        lines = datafile.readlines()\n",
    "    for line in lines[:n]:\n",
    "        print(line.strip())\n",
    "\n",
    "print_lines(os.path.join(corpus_name, \"utterances.jsonl\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Formatted Data File\n",
    "\n",
    "For ease of use, we will generate a well-structured data file where each line comprises a tab-separated pair of a *query sentence* and a *response sentence*.\n",
    "\n",
    "The functions below aid in parsing the raw `utterances.jsonl` data file.\n",
    "\n",
    "- `loadLinesAndConversations` breaks down each line of the file into a dictionary of lines with fields: `lineID`, `characterID`, and text, and then groups these into conversations with fields: `conversationID`, `movieID`, and lines.\n",
    "- `extractSentencePairs` pulls out pairs of sentences from the conversations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "NxwWW8PrION_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing corpus into lines and conversations...\n",
      "\n",
      "Writing newly formatted file...\n",
      "\n",
      "Sample lines from file:\n",
      "They do not!\tThey do to!\n",
      "I hope so.\tShe okay?\n",
      "Let's go.\tWow\n",
      "Okay -- you're gonna need to learn how to lie.\tNo\n",
      "No\t\"I'm kidding.  You know how sometimes you just become this \"\"persona\"\"?  And you don't know how to quit?\"\n"
     ]
    }
   ],
   "source": [
    "# Load lines and conversations\n",
    "def load_data(file_name):\n",
    "    lines, conversations = {}, {}\n",
    "    with open(file_name, 'r', encoding='iso-8859-1') as file:\n",
    "        for line in file:\n",
    "            data = json.loads(line)\n",
    "            lines[data[\"id\"]] = {\"lineID\": data[\"id\"], \"characterID\": data[\"speaker\"], \"text\": data[\"text\"]}\n",
    "            conv_id = data[\"conversation_id\"]\n",
    "            if conv_id not in conversations:\n",
    "                conversations[conv_id] = {\n",
    "                    \"conversationID\": conv_id,\n",
    "                    \"movieID\": data[\"meta\"][\"movie_id\"],\n",
    "                    \"lines\": [lines[data[\"id\"]]]\n",
    "                }\n",
    "            else:\n",
    "                conversations[conv_id][\"lines\"].append(lines[data[\"id\"]])\n",
    "    return lines, conversations\n",
    "\n",
    "# Extract sentence pairs from conversations\n",
    "def extract_pairs(conversations):\n",
    "    return [[input_line[\"text\"].strip(), target_line[\"text\"].strip()]\n",
    "            for conversation in conversations.values()\n",
    "            for input_line, target_line in zip(conversation[\"lines\"], conversation[\"lines\"][1:])\n",
    "            if input_line[\"text\"].strip() and target_line[\"text\"].strip()]\n",
    "    \n",
    "# Processing and writing to file\n",
    "def process_and_write(corpus_name):\n",
    "    # Define path to new file\n",
    "    datafile = os.path.join(corpus_name, \"formatted_movie_lines.txt\")\n",
    "    delimiter = '\\t'  # Tab character, no need to unescape\n",
    "\n",
    "    # Load lines and conversations\n",
    "    print(\"\\nProcessing corpus into lines and conversations...\")\n",
    "    lines, conversations = load_data(os.path.join(corpus_name, \"utterances.jsonl\"))\n",
    "\n",
    "    # Write new csv file\n",
    "    print(\"\\nWriting newly formatted file...\")\n",
    "    with open(datafile, 'w', encoding='utf-8') as outputfile:\n",
    "        writer = csv.writer(outputfile, delimiter=delimiter, lineterminator='\\n')\n",
    "        for pair in extract_pairs(conversations):\n",
    "            writer.writerow(pair)\n",
    "\n",
    "    # Print a sample of lines (assuming a function printLines exists)\n",
    "    print(\"\\nSample lines from file:\")\n",
    "    # If printLines is not defined, we can simply print the first few lines from the file\n",
    "    with open(datafile, 'r', encoding='utf-8') as file:\n",
    "        for _ in range(5):  # Print first 5 lines as a sample\n",
    "            print(file.readline().strip())\n",
    "    return datafile\n",
    "\n",
    "datafile=process_and_write(corpus_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading and Trimming\n",
    "\n",
    "The next step involves creating a vocabulary and loading query/response sentence pairs into memory.\n",
    "\n",
    "Keep in mind that we are working with sequences of **words**, which do not inherently map to a discrete numerical space. Therefore, we need to create such a mapping by associating each unique word we encounter in our dataset with an index value.\n",
    "\n",
    "To achieve this, we define a `Vocabulary` class, which maintains a mapping from words to indexes, a reverse mapping from indexes to words, a count of each word, and a total word count. The class offers methods for adding a word to the vocabulary (`add_word`), adding all words in a sentence (`add_sentence`), and trimming infrequently seen words (`trim`). We will discuss trimming in more detail later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "ub-LHZiKIaS9"
   },
   "outputs": [],
   "source": [
    "# Default word tokens\n",
    "PAD_TOKEN = 0  # Used for padding short sentences\n",
    "SOS_TOKEN = 1  # Start-of-sentence token\n",
    "EOS_TOKEN = 2  # End-of-sentence token\n",
    "\n",
    "class Vocabulary:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.trimmed = False\n",
    "        self.word_to_index = {\"PAD\":PAD_TOKEN,\"SOS\":SOS_TOKEN, \"EOS\":EOS_TOKEN}\n",
    "        self.word_to_count = {}\n",
    "        self.index_to_word = {PAD_TOKEN: \"PAD\", SOS_TOKEN: \"SOS\", EOS_TOKEN: \"EOS\"}\n",
    "        self.num_words = 3  # Count SOS, EOS, PAD\n",
    "\n",
    "    def add_sentence(self, sentence):\n",
    "        for word in sentence.split():\n",
    "            self.add_word(word)\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if word not in self.word_to_index:\n",
    "            self.word_to_index[word] = self.num_words\n",
    "            self.word_to_count[word] = 1\n",
    "            self.index_to_word[self.num_words] = word\n",
    "            self.num_words += 1\n",
    "        else:\n",
    "            self.word_to_count[word] += 1\n",
    "\n",
    "    def trim(self, min_count):\n",
    "        if self.trimmed:\n",
    "            return\n",
    "        self.trimmed = True\n",
    "\n",
    "        keep_words = [word for word, count in self.word_to_count.items() if count >= min_count]\n",
    "\n",
    "        # Save the counts for the kept words\n",
    "        keep_word_counts = {word: self.word_to_count[word] for word in keep_words}\n",
    "\n",
    "        # Reinitialize dictionaries\n",
    "        self.word_to_index, self.word_to_count, self.index_to_word = {\"PAD\":PAD_TOKEN,\"SOS\":SOS_TOKEN, \"EOS\":EOS_TOKEN}, {}, {PAD_TOKEN: \"PAD\", SOS_TOKEN: \"SOS\", EOS_TOKEN: \"EOS\"}\n",
    "        self.num_words = 3  # Reset to count default tokens\n",
    "\n",
    "        for word in keep_words:\n",
    "            self.word_to_index[word] = self.num_words\n",
    "            self.word_to_count[word] = keep_word_counts[word]\n",
    "            self.index_to_word[self.num_words] = word\n",
    "            self.num_words += 1\n",
    "\n",
    "        # Provide a report on the trimming process\n",
    "        print(f'keep_words {len(keep_words)} / {len(self.index_to_word) - 3} = '\n",
    "              f'{len(keep_words) / (len(self.index_to_word) - 3):.4f}')\n",
    "\n",
    "# Helper function to add multiple sentences to a Vocabulary instance\n",
    "def add_sentences_to_vocabulary(vocab, sentences):\n",
    "    for sentence in sentences:\n",
    "        vocab.add_sentence(sentence)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now compile our vocabulary and query/response sentence pairs. However, before we can utilize this data, we need to carry out some preprocessing steps.\n",
    "\n",
    "Initially, we need to transform the Unicode strings into ASCII using `unicodeToAscii`. Subsequently, we should convert all characters to lowercase and remove all non-letter characters, excluding basic punctuation (`normalize_string`). Lastly, to facilitate training convergence, we will exclude sentences exceeding the `MAX_LENGTH` threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "_x74e1vgIlfX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "pairs:\n",
      "['they do not !', 'they do to !']\n",
      "['i hope so .', 'she okay ?']\n",
      "['let s go .', 'wow']\n",
      "['like my fear of wearing pastels ?', 'the real you .']\n",
      "['the real you .', 'what good stuff ?']\n",
      "['what crap ?', 'do you listen to this crap ?']\n",
      "['you always been this selfish ?', 'but']\n",
      "['but', 'then that s all you had to say .']\n",
      "['then that s all you had to say .', 'well no . . .']\n",
      "['tons', 'have fun tonight ?']\n"
     ]
    }
   ],
   "source": [
    "MAX_LENGTH = 10  # Maximum sentence length to consider\n",
    "\n",
    "# Turn a Unicode string to plain ASCII\n",
    "def normalize_string(s):\n",
    "    \"\"\"\n",
    "    Normalize the string: Convert to ASCII, make lowercase, strip leading/trailing whitespace,\n",
    "    separate punctuation with spaces, and remove non-letter characters. \n",
    "    \"\"\"  \n",
    "    s = unicodedata.normalize('NFD', s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s))\n",
    "    s = re.sub(r\"\\s+\", r\" \", s).strip()\n",
    "    \n",
    "    return s\n",
    "\n",
    "def load_prepare_data(corpus_name, datafile):\n",
    "    \"\"\"\n",
    "    Load and prepare data: Open the datafile, split into lines, normalize,\n",
    "    filter by length, create a vocabulary, and add each sentence to it.\n",
    "    \"\"\"\n",
    "    with open(datafile, encoding='utf-8') as file:\n",
    "        lines = [line.split('\\t') for line in file.read().strip().split('\\n')]\n",
    "    pairs = [[normalize_string(s) for s in pair] for pair in lines]\n",
    "    pairs = [pair for pair in pairs if all(len(s.split()) < MAX_LENGTH for s in pair)]\n",
    "    voc = Vocabulary(corpus_name)\n",
    "    for pair in pairs:\n",
    "        for s in pair:\n",
    "            voc.add_sentence(s)\n",
    "    return voc, pairs\n",
    "\n",
    "# Load/Assemble voc and pairs\n",
    "save_dir = os.path.join(\"data\", \"save\")\n",
    "voc, pairs = load_prepare_data(corpus_name, datafile)\n",
    "# Print some pairs to validate\n",
    "print(\"\\npairs:\")\n",
    "for pair in pairs[:10]:\n",
    "    print(pair)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another tactic that is beneficial to achieving faster convergence during\n",
    "training is trimming rarely used words out of our vocabulary. Decreasing\n",
    "the feature space will also soften the difficulty of the function that\n",
    "the model must learn to approximate. We will do this as a two-step\n",
    "process:\n",
    "\n",
    "1) Trim words used under ``MIN_COUNT`` threshold using the ``voc.trim``\n",
    "   function.\n",
    "\n",
    "2) Filter out pairs with trimmed words.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "AoMvLVJ5ImbB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keep_words 7832 / 7832 = 1.0000\n",
      "Trimmed from 64259 pairs to 53074, 0.8259 of total\n"
     ]
    }
   ],
   "source": [
    "MIN_COUNT = 3    # Minimum word count threshold for trimming\n",
    "\n",
    "def trim_rare_words(voc, pairs, MIN_COUNT):\n",
    "    # Trim words used under the MIN_COUNT from the voc\n",
    "    voc.trim(MIN_COUNT)\n",
    "    # Filter out pairs with trimmed words\n",
    "    keep_pairs = []\n",
    "    for pair in pairs:\n",
    "        input_sentence = pair[0]\n",
    "        output_sentence = pair[1]\n",
    "        keep_input = True\n",
    "        keep_output = True\n",
    "        # Check input sentence\n",
    "        for word in input_sentence.split(' '):\n",
    "            if word not in voc.word_to_index:\n",
    "                keep_input = False\n",
    "                break\n",
    "        # Check output sentence\n",
    "        for word in output_sentence.split(' '):\n",
    "            if word not in voc.word_to_index:\n",
    "                keep_output = False\n",
    "                break\n",
    "\n",
    "        # Only keep pairs that do not contain trimmed word(s) in their input or output sentence\n",
    "        if keep_input and keep_output:\n",
    "            keep_pairs.append(pair)\n",
    "\n",
    "    print(\"Trimmed from {} pairs to {}, {:.4f} of total\".format(len(pairs), len(keep_pairs), len(keep_pairs) / len(pairs)))\n",
    "    return keep_pairs\n",
    "\n",
    "\n",
    "# Trim voc and pairs\n",
    "pairs = trim_rare_words(voc, pairs, MIN_COUNT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation for Models\n",
    "\n",
    "Despite our extensive efforts to curate and process our data into a convenient vocabulary object and list of sentence pairs, our models will ultimately require numerical torch tensors as inputs.  \n",
    "\n",
    " To accommodate sentences of different sizes in the same batch, we will create our batched input tensor of shape (max_length, batch_size), where sentences shorter than the max_length are zero padded after an EOS_token.\n",
    "\n",
    "If we simply convert our English sentences to tensors by converting words to their indexes and zero-pad, our tensor would have shape (batch_size, max_length) and indexing the first dimension would return a full sequence across all time-steps. However, we need to be able to index our batch along time, and across all sequences in the batch. Therefore, we transpose our input batch shape to (max_length, batch_size), so that indexing across the first dimension returns a time step across all sentences in the batch.\n",
    "\n",
    "The output function palso returns a binary mask tensor and a maximum target sentence length. The binary mask tensor has the same shape as the output target tensor, but every element that is a PAD_token is 0 and all others are 1.\n",
    "\n",
    "`batch_to_train_data` simply takes a bunch of pairs and returns the input and target tensors using the aforementioned functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Cny8pPMnIrQC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variable: tensor([[  45, 4551,    8,  100,  160],\n",
      "        [  26,   11,   53,   16,  161],\n",
      "        [ 355,   40,  100,  344,   14],\n",
      "        [1813,   81,  340, 2270,    2],\n",
      "        [   7, 1257,   11,   11,    0],\n",
      "        [6226,  200,    2,    2,    0],\n",
      "        [  81,   11,    0,    0,    0],\n",
      "        [  14,    2,    0,    0,    0],\n",
      "        [   2,    0,    0,    0,    0]])\n",
      "lengths: tensor([9, 8, 6, 6, 4])\n",
      "target_variable: tensor([[ 276,   27,  100,  450,  107],\n",
      "        [  11,   16, 2983,  143,   64],\n",
      "        [   2,   38,   26,   11,  530],\n",
      "        [   0,  143,  663,    2,   11],\n",
      "        [   0,   67,  581,    0,    2],\n",
      "        [   0,   14,   11,    0,    0],\n",
      "        [   0,    2,  100,    0,    0],\n",
      "        [   0,    0,   91,    0,    0],\n",
      "        [   0,    0,    2,    0,    0]])\n",
      "mask: tensor([[ True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True],\n",
      "        [False,  True,  True,  True,  True],\n",
      "        [False,  True,  True, False,  True],\n",
      "        [False,  True,  True, False, False],\n",
      "        [False,  True,  True, False, False],\n",
      "        [False, False,  True, False, False],\n",
      "        [False, False,  True, False, False]])\n",
      "max_target_len: 9\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import torch\n",
    "import random\n",
    "\n",
    "# Assuming PAD_token and EOS_token are defined with their respective integral values\n",
    "\n",
    "def indexes_from_sentence(vocabulary, sentence):\n",
    "    \"\"\"\n",
    "    Convert sentence to a list of indexes, appending the EOS token at the end.\n",
    "    \"\"\"\n",
    "    return [vocabulary.word_to_index[word] for word in sentence.split(' ')] + [EOS_TOKEN]\n",
    "\n",
    "def binary_matrix(l, value=PAD_TOKEN):\n",
    "    \"\"\"\n",
    "    Create a binary matrix representing the padding of sentences.\n",
    "    \"\"\"\n",
    "    return [[0 if token == value else 1 for token in seq] for seq in l]\n",
    "\n",
    "def batch_to_train_data(vocabulary, pair_batch):\n",
    "    \"\"\"\n",
    "    Prepare the batch for training: sort by input length, create tensors for input/target variables.\n",
    "    \"\"\"\n",
    "    pair_batch.sort(key=lambda x: len(x[0].split(\" \")), reverse=True)\n",
    "    input_batch, output_batch = zip(*pair_batch)\n",
    "    input_indexes = [indexes_from_sentence(vocabulary, sentence) for sentence in input_batch]\n",
    "    input_lengths = torch.tensor([len(indexes) for indexes in input_indexes])\n",
    "    input_padded = torch.LongTensor(list(itertools.zip_longest(*input_indexes, fillvalue=PAD_TOKEN)))\n",
    "\n",
    "    output_indexes = [indexes_from_sentence(vocabulary, sentence) for sentence in output_batch]\n",
    "    output_padded = torch.LongTensor(list(itertools.zip_longest(*output_indexes, fillvalue=PAD_TOKEN)))\n",
    "    output_mask = torch.BoolTensor(binary_matrix(output_padded))\n",
    "    max_target_len = max(len(indexes) for indexes in output_indexes)\n",
    "\n",
    "    return input_padded, input_lengths, output_padded, output_mask, max_target_len\n",
    "\n",
    "# Example for validation\n",
    "small_batch_size = 5\n",
    "batches = batch_to_train_data(voc, [random.choice(pairs) for _ in range(small_batch_size)])\n",
    "input_variable, lengths, target_variable, mask, max_target_len = batches\n",
    "\n",
    "print(\"input_variable:\", input_variable)\n",
    "print(\"lengths:\", lengths)\n",
    "print(\"target_variable:\", target_variable)\n",
    "print(\"mask:\", mask)\n",
    "print(\"max_target_len:\", max_target_len)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Procedure Definition\n",
    "\n",
    "### Loss with Masking\n",
    "\n",
    "Given that we're working with batches of padded sequences, we can't compute loss using all tensor elements. We establish `mask_nll_loss` to compute our loss based on the decoder's output tensor, the target tensor, and a binary mask tensor that indicates the padding of the target tensor. This loss function computes the average negative log likelihood of the elements that align with a *1* in the mask tensor.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "KSjDpLkxI8VX"
   },
   "outputs": [],
   "source": [
    "def mask_nll_loss(inp, target, mask, device):\n",
    "    \"\"\"\n",
    "    Calculate the negative log likelihood loss with a mask over the lengths of target sequences.\n",
    "    \"\"\"\n",
    "    n_total = mask.sum()\n",
    "    cross_entropy = -torch.log(torch.gather(inp, 1, target.view(-1, 1)).squeeze(1))\n",
    "    loss = cross_entropy.masked_select(mask).mean()\n",
    "    loss = loss.to(device)\n",
    "    return loss, n_total.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Training Iteration Procedure\n",
    "\n",
    "The `train` function encapsulates the process for a single training iteration (a single batch of inputs).\n",
    "\n",
    "We employ two strategies to aid convergence:\n",
    "\n",
    "-  **Teacher forcing**: At a probability determined by `teacher_forcing_ratio`, we use the current target word as the decoder’s next input instead of the decoder’s current guess. This helps in efficient training but can cause instability during inference. Hence, the `teacher_forcing_ratio` must be set carefully.\n",
    "\n",
    "-  **Gradient clipping**: This technique counters the \"exploding gradient\" problem by capping gradients to a maximum value, preventing them from growing exponentially and causing overflow or overshooting steep cost function cliffs.\n",
    "\n",
    "**Procedure:**\n",
    "\n",
    "   1) Pass the entire input batch through the encoder.\n",
    "   2) Initialize decoder inputs as SOS_token, and hidden state as the encoder's final hidden state.\n",
    "   3) Pass the input batch sequence through the decoder one time step at a time.\n",
    "   4) If teacher forcing: set next decoder input as the current target; else: set next decoder input as current decoder output.\n",
    "   5) Calculate and accumulate loss.\n",
    "   6) Perform backpropagation.\n",
    "   7) Clip gradients.\n",
    "   8) Update encoder and decoder model parameters.\n",
    "\n",
    "Note: PyTorch’s RNN modules (`RNN`, `LSTM`, `GRU`) can be used like any other non-recurrent layers by passing them the entire input sequence. We use the `GRU` layer like this in the `encoder`. However, you can also run these modules one time-step at a time, as we do for the `decoder` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "NF5Kt7PBI9qq"
   },
   "outputs": [],
   "source": [
    "def train(input_variable, lengths, target_variable, mask, max_target_len, encoder, decoder, embedding,\n",
    "          encoder_optimizer, decoder_optimizer, batch_size, clip, max_length=MAX_LENGTH):\n",
    "\n",
    "    # Zero gradients\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    # Set device options\n",
    "    input_variable = input_variable.to(device)\n",
    "    target_variable = target_variable.to(device)\n",
    "    mask = mask.to(device)\n",
    "    # Lengths for RNN packing should always be on the CPU\n",
    "    lengths = lengths.to(\"cpu\")\n",
    "\n",
    "    # Initialize variables\n",
    "    loss = 0\n",
    "    print_losses = []\n",
    "    n_totals = 0\n",
    "\n",
    "    # Forward pass through encoder\n",
    "    encoder_outputs, encoder_hidden = encoder(input_variable, lengths)\n",
    "\n",
    "    # Create initial decoder input (start with SOS tokens for each sentence)\n",
    "    decoder_input = torch.LongTensor([[SOS_TOKEN for _ in range(batch_size)]])\n",
    "    decoder_input = decoder_input.to(device)\n",
    "\n",
    "    # Set initial decoder hidden state to the encoder's final hidden state\n",
    "    decoder_hidden = encoder_hidden[:decoder.n_layers]\n",
    "\n",
    "    # Determine if we are using teacher forcing this iteration\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    # Forward batch of sequences through decoder one time step at a time\n",
    "    if use_teacher_forcing:\n",
    "        for t in range(max_target_len):\n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden\n",
    "            )\n",
    "            # Teacher forcing: next input is current target\n",
    "            decoder_input = target_variable[t].view(1, -1)\n",
    "            # Calculate and accumulate loss\n",
    "            mask_loss, nTotal = mask_nll_loss(decoder_output, target_variable[t], mask[t],device)\n",
    "            loss += mask_loss\n",
    "            print_losses.append(mask_loss.item() * nTotal)\n",
    "            n_totals += nTotal\n",
    "    else:\n",
    "        for t in range(max_target_len):\n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden\n",
    "            )\n",
    "            # No teacher forcing: next input is decoder's own current output\n",
    "            _, topi = decoder_output.topk(1)\n",
    "            decoder_input = torch.LongTensor([[topi[i][0] for i in range(batch_size)]])\n",
    "            decoder_input = decoder_input.to(device)\n",
    "            # Calculate and accumulate loss\n",
    "            mask_loss, nTotal = mask_nll_loss(decoder_output, target_variable[t], mask[t],device)\n",
    "            loss += mask_loss\n",
    "            print_losses.append(mask_loss.item() * nTotal)\n",
    "            n_totals += nTotal\n",
    "\n",
    "    # Perform backpropagation\n",
    "    loss.backward()\n",
    "\n",
    "    # Clip gradients: gradients are modified in place\n",
    "    _ = nn.utils.clip_grad_norm_(encoder.parameters(), clip)\n",
    "    _ = nn.utils.clip_grad_norm_(decoder.parameters(), clip)\n",
    "\n",
    "    # Adjust model weights\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return sum(print_losses) / n_totals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Iterations\n",
    "\n",
    "Now we can integrate the complete training procedure with the data. The `train_iters` function executes `n_iterations` of training using the provided models, optimizers, data, etc. Most of the complex work is handled by the `train` function.\n",
    "\n",
    "It's important to note that when we save our model, we store a tarball that includes the encoder and decoder `state_dicts` (parameters), the optimizers' `state_dicts`, the loss, the iteration, etc. Saving the model in this way provides maximum flexibility with the checkpoint. After loading a checkpoint, we can either use the model parameters to run inference or continue training from where we left off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "S7BAQWthJAxf"
   },
   "outputs": [],
   "source": [
    "def train_iters(model_name, voc, pairs, encoder, decoder, encoder_optimizer, decoder_optimizer, embedding,\n",
    "                encoder_n_layers, decoder_n_layers, save_dir, n_iteration, batch_size, print_every, save_every,\n",
    "                clip, corpus_name):\n",
    "    \"\"\"\n",
    "    Run training for a set number of iterations.\n",
    "    \"\"\"\n",
    "    # Load batches for each iteration\n",
    "    training_batches = [batch_to_train_data(voc, [random.choice(pairs) for _ in range(batch_size)])\n",
    "                        for _ in range(n_iteration)]\n",
    "\n",
    "    print('Initializing ...')\n",
    "    start_iteration = 1\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "\n",
    "    print(\"Training...\")\n",
    "    for iteration in range(start_iteration, n_iteration + 1):\n",
    "        training_batch = training_batches[iteration - 1]\n",
    "        input_variable, lengths, target_variable, mask, max_target_len = training_batch\n",
    "\n",
    "        # Run a training iteration with batch\n",
    "        loss = train(input_variable, lengths, target_variable, mask, max_target_len, encoder,\n",
    "                     decoder, embedding, encoder_optimizer, decoder_optimizer, batch_size, clip)\n",
    "        print_loss_total += loss\n",
    "\n",
    "        # Print progress\n",
    "        if iteration % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print(f\"Iteration: {iteration}; Percent complete: {iteration / n_iteration * 100:.1f}%; Average loss: {print_loss_avg:.4f}\")\n",
    "            print_loss_total = 0\n",
    "\n",
    "        # Save checkpoint\n",
    "        if iteration % save_every == 0:\n",
    "            directory = os.path.join(save_dir, model_name, corpus_name, f'{encoder_n_layers}-{decoder_n_layers}_{hidden_features}')\n",
    "            if not os.path.exists(directory):\n",
    "                os.makedirs(directory)\n",
    "            torch.save({\n",
    "                'iteration': iteration,\n",
    "                'en': encoder.state_dict(),\n",
    "                'de': decoder.state_dict(),\n",
    "                'en_opt': encoder_optimizer.state_dict(),\n",
    "                'de_opt': decoder_optimizer.state_dict(),\n",
    "                'loss': loss,\n",
    "                'voc_dict': voc.__dict__,\n",
    "                'embedding': embedding.state_dict()\n",
    "            }, os.path.join(directory, f'{iteration}_checkpoint.tar'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "After training the model, we want to interact with the bot. We need to define how the model decodes the encoded input.\n",
    "\n",
    "### Greedy Decoding\n",
    "\n",
    "Greedy decoding is used during training when we're not using teacher forcing. At each time step, we choose the word from `decoder_output` with the highest softmax value. This method is optimal at a single time-step level.\n",
    "\n",
    "We define a `GreedySearchDecoder` class to perform greedy decoding. The input sentence is evaluated as follows:\n",
    "\n",
    "**Computation Steps:**\n",
    "\n",
    "   1) Pass input through the encoder model.\n",
    "   2) Prepare the encoder's final hidden layer to be the first hidden input to the decoder.\n",
    "   3) Initialize the decoder's first input as SOS_token.\n",
    "   4) Initialize tensors to append decoded words to.\n",
    "   5) Iteratively decode one word token at a time:\n",
    "       a) Pass through the decoder.\n",
    "       b) Obtain the most likely word token and its softmax score.\n",
    "       c) Record the token and score.\n",
    "       d) Prepare the current token to be the next decoder input.\n",
    "   6) Return collections of word tokens and scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "eGzjE-ggJCcZ"
   },
   "outputs": [],
   "source": [
    "class GreedySearchDecoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        \"\"\"\n",
    "        Greedy decoding module initialization.\n",
    "        \"\"\"\n",
    "        super(GreedySearchDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, input_seq, input_length, max_length):\n",
    "        \"\"\"\n",
    "        Forward propagation of the input to produce a sequence of tokens.\n",
    "        \"\"\"\n",
    "        # Forward input through encoder model\n",
    "        encoder_outputs, encoder_hidden = self.encoder(input_seq, input_length)\n",
    "\n",
    "        # Prepare encoder's final hidden layer to be first hidden input to the decoder\n",
    "        decoder_hidden = encoder_hidden[:self.decoder.n_layers]\n",
    "\n",
    "        # Initialize decoder input with SOS_token\n",
    "        decoder_input = torch.tensor([[SOS_TOKEN]], device=device, dtype=torch.long)\n",
    "\n",
    "        # Initialize tensors to append decoded words to\n",
    "        all_tokens = torch.zeros(0, dtype=torch.long, device=device)\n",
    "        all_scores = torch.zeros(0, device=device)\n",
    "\n",
    "        # Iteratively decode one word token at a time\n",
    "        for _ in range(max_length):\n",
    "            # Forward pass through decoder\n",
    "            decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden)\n",
    "\n",
    "            # Obtain most likely word token and its softmax score\n",
    "            decoder_scores, decoder_input = torch.max(decoder_output, dim=1)\n",
    "\n",
    "            # Record token and score\n",
    "            all_tokens = torch.cat((all_tokens, decoder_input), dim=0)\n",
    "            all_scores = torch.cat((all_scores, decoder_scores), dim=0)\n",
    "\n",
    "            # Prepare current token to be next decoder input (add a dimension)\n",
    "            decoder_input = torch.unsqueeze(decoder_input, 0)\n",
    "\n",
    "        # Return collections of word tokens and scores\n",
    "        return all_tokens, all_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Evaluation\n",
    "\n",
    "With our decoding method defined, we can create functions to evaluate a string input sentence. The `evaluate` function handles the input sentence, it formats the sentence as an input batch of word indexes with *batch_size==1*. This is done by converting the sentence words to their corresponding indexes and transposing the dimensions to prepare the tensor for our models. A `lengths` tensor is also created which contains the length of our input sentence. The decoded response sentence tensor is obtained using our `GreedySearchDecoder` object (`searcher`). Finally, the response’s indexes are converted to words and the list of decoded words is returned.\n",
    "\n",
    "`evaluate_input` serves as the user interface for our chatbot. It prompts an input text field where we can enter our query sentence. After entering our input sentence and pressing *Enter*, our text is normalized like our training data, and is fed to the `evaluate` function to obtain a decoded output sentence. This process is looped for continuous interaction with our bot until we enter either “q” or “quit”.\n",
    "\n",
    "If a sentence is entered that contains a word not in the vocabulary, an error message is printed and the user is prompted to enter another sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "7LY0AotNJEzk"
   },
   "outputs": [],
   "source": [
    "def evaluate(searcher, voc, sentence, max_length=MAX_LENGTH):\n",
    "    \"\"\"\n",
    "    Evaluate a sentence using the encoder, decoder, and searcher provided.\n",
    "    \"\"\"\n",
    "    # Prepare the input sentence as a batch of word indexes\n",
    "    indexes_batch = [indexes_from_sentence(voc, sentence)]\n",
    "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
    "    input_batch = torch.LongTensor(indexes_batch).transpose(0, 1).to(device)\n",
    "    lengths = lengths.to(\"cpu\")  # Lengths need to be on CPU for pack_padded_sequence\n",
    "\n",
    "    # Decode the sentence with the searcher\n",
    "    tokens, scores = searcher(input_batch, lengths, max_length)\n",
    "    decoded_words = [voc.index_to_word[token.item()] for token in tokens]\n",
    "\n",
    "    return decoded_words\n",
    "\n",
    "def evaluate_input(encoder, decoder, searcher, voc):\n",
    "    \"\"\"\n",
    "    Interactively evaluate input from the user.\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        try:\n",
    "            input_sentence = input('> ')\n",
    "            if input_sentence in ('q', 'quit'):\n",
    "                break\n",
    "            print (input_sentence)\n",
    "            input_sentence = normalize_string(input_sentence)\n",
    "            output_words = evaluate(searcher, voc, input_sentence)\n",
    "            output_words = [word for word in output_words if word not in ('EOS', 'PAD')]\n",
    "            print('Bot:', ' '.join(output_words))\n",
    "\n",
    "        except KeyError:\n",
    "            print(\"Error: Encountered unknown word.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Models Overview\n",
    "\n",
    "### Seq2Seq Model\n",
    "\n",
    "Our chatbot uses a sequence-to-sequence (seq2seq) model, which takes a variable-length sequence as input and returns a variable-length sequence as output. This is achieved by using two separate recurrent neural nets (RNNs): an **encoder** and a **decoder**. The encoder encodes the input sequence into a fixed-length context vector, which theoretically contains semantic information about the input sentence. The decoder takes an input word and the context vector, and returns a guess for the next word in the sequence and a hidden state for the next iteration.\n",
    "\n",
    "### Encoder\n",
    "\n",
    "The encoder RNN iterates through the input sentence one token at a time, outputting an \"output\" vector and a \"hidden state\" vector at each time step. The hidden state vector is passed to the next time step, while the output vector is recorded. The encoder uses a multi-layered Gated Recurrent Unit (GRU) and a bidirectional variant of the GRU. An `embedding` layer is used to encode our word indices in an arbitrarily sized feature space. \n",
    "\n",
    "### Decoder\n",
    "\n",
    "The decoder RNN generates the response sentence in a token-by-token fashion. It uses the encoder’s context vectors, and internal hidden states to generate the next word in the sequence. To avoid information loss, especially when dealing with long input sequences, an \"attention mechanism\" is typically used that allows the decoder to pay attention to certain parts of the input sequence, rather than using the entire fixed context at every step. However, in this tutorial, we only consider the use of standard RNNs, which will limit the performance of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'deeplay' has no attribute 'EncoderRNN'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 22\u001b[0m\n\u001b[1;32m     20\u001b[0m embedding \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mEmbedding(voc\u001b[38;5;241m.\u001b[39mnum_words, hidden_features)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Initialize the EncoderRNN\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m encoder \u001b[38;5;241m=\u001b[39m\u001b[43mdl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEncoderRNN\u001b[49m(\n\u001b[1;32m     23\u001b[0m     in_features\u001b[38;5;241m=\u001b[39min_features, \n\u001b[1;32m     24\u001b[0m     hidden_features\u001b[38;5;241m=\u001b[39mhidden_features,\n\u001b[1;32m     25\u001b[0m     num_layers\u001b[38;5;241m=\u001b[39mencoder_n_layers, \n\u001b[1;32m     26\u001b[0m     embedding\u001b[38;5;241m=\u001b[39membedding,\n\u001b[1;32m     27\u001b[0m     rnn_type\u001b[38;5;241m=\u001b[39mrnn_type, \n\u001b[1;32m     28\u001b[0m     dropout\u001b[38;5;241m=\u001b[39mdropout\n\u001b[1;32m     29\u001b[0m )\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     32\u001b[0m encoder\u001b[38;5;241m.\u001b[39mbuild()\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(encoder)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'deeplay' has no attribute 'EncoderRNN'"
     ]
    }
   ],
   "source": [
    "# Set configuration parameters for the model\n",
    "model_name = 'cb_model'\n",
    "hidden_features = 500\n",
    "in_features=hidden_features\n",
    "encoder_n_layers = 2\n",
    "decoder_n_layers = 2\n",
    "rnn_type = 'GRU'\n",
    "dropout = 0.1\n",
    "batch_size = 64\n",
    "\n",
    "# Set training and optimization parameters\n",
    "clip = 50.0\n",
    "teacher_forcing_ratio = 1.0\n",
    "learning_rate = 0.0001\n",
    "decoder_learning_ratio = 5.0\n",
    "n_iteration = 4000\n",
    "print_every = 1\n",
    "save_every = 500\n",
    "\n",
    "embedding = nn.Embedding(voc.num_words, hidden_features)\n",
    "# Initialize the EncoderRNN\n",
    "encoder =dl.EncoderRNN(\n",
    "    in_features=in_features, \n",
    "    hidden_features=hidden_features,\n",
    "    num_layers=encoder_n_layers, \n",
    "    embedding=embedding,\n",
    "    rnn_type=rnn_type, \n",
    "    dropout=dropout\n",
    ").to(device)\n",
    "\n",
    "\n",
    "encoder.build()\n",
    "print(encoder)\n",
    "\n",
    "decoder =dl.DecoderRNN(\n",
    "    in_features=in_features, \n",
    "    hidden_features=hidden_features,\n",
    "    out_features=voc.num_words,\n",
    "    num_layers=decoder_n_layers, \n",
    "    embedding=embedding,\n",
    "    rnn_type=rnn_type, \n",
    "    dropout=dropout\n",
    ").to(device)\n",
    "\n",
    "\n",
    "encoder.build()\n",
    "print(decoder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define and train model\n",
    "\n",
    "Now, we're ready to run our model!\n",
    "\n",
    "Whether we're training or testing the chatbot model, we need to initialize the encoder and decoder models. In the next block, we configure the parameters, and construct and initialize the models. You're encouraged to experiment with different model configurations to enhance performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize word embeddings and encoder & decoder models\n",
    "# Set models to training mode\n",
    "encoder.train()\n",
    "decoder.train()\n",
    "\n",
    "# Initialize optimizers for the encoder and decoder\n",
    "encoder_optimizer = torch.optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=learning_rate * decoder_learning_ratio)\n",
    "\n",
    "# Move optimizer states to GPU if necessary\n",
    "if torch.cuda.is_available():\n",
    "    for state in encoder_optimizer.state.values():\n",
    "        for k, v in state.items():\n",
    "            if isinstance(v, torch.Tensor):\n",
    "                state[k] = v.cuda()\n",
    "\n",
    "    for state in decoder_optimizer.state.values():\n",
    "        for k, v in state.items():\n",
    "            if isinstance(v, torch.Tensor):\n",
    "                state[k] = v.cuda()\n",
    "\n",
    "# Begin the training process\n",
    "train_iters(model_name, voc, pairs, encoder, decoder, encoder_optimizer, decoder_optimizer,\n",
    "           embedding, encoder_n_layers, decoder_n_layers, save_dir, n_iteration, batch_size,\n",
    "           print_every, save_every, clip, corpus_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1OSKoKwg87mR"
   },
   "outputs": [],
   "source": [
    "# Set dropout layers to ``eval`` mode\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "\n",
    "# Initialize search module\n",
    "searcher = GreedySearchDecoder(encoder, decoder)\n",
    "\n",
    "# Begin chatting (uncomment and run the following line to begin)\n",
    "evaluate_input(encoder, decoder, searcher, voc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's review the changes made to incorporate conversation history into your chatbot model training and evaluation:\n",
    "### Changes for Training with Conversation History\n",
    "\n",
    "create_history_pairs Function: This function creates training pairs that include the history of the conversation. For each exchange in the pairs, it appends a certain number of previous exchanges (up to MAX_HISTORY) separated by the <EOS> token. These pairs are then used for training.\n",
    "\n",
    "Batch Preparation (batch_to_train_data): The function for preparing a batch of training data remains largely the same. The only difference is that it now handles input sequences that include conversation history.\n",
    "\n",
    "Training Function (train):\n",
    "    The core training logic remains unchanged.\n",
    "    The function receives input sequences that now contain conversation history.\n",
    "    The forward pass through the encoder and the decoding steps are performed as usual, without any specific changes needed to accommodate the conversation history.\n",
    "\n",
    "Data Preparation for Training:\n",
    "    The history_pairs are generated using the create_history_pairs function.\n",
    "    These pairs are then used throughout the training process.\n",
    "\n",
    "### Changes for Evaluation with Conversation History\n",
    "\n",
    "evaluate Function:\n",
    "    The conversation history is now considered when evaluating a new input.\n",
    "    The history is concatenated with the current input sentence, separated by spaces.\n",
    "    The concatenated string is then processed and fed into the model for generating a response.\n",
    "\n",
    "evaluate_input Function:\n",
    "    Manages interactive evaluation with the user.\n",
    "    Maintains a conversation_history list, appending each user input and the model's response to it.\n",
    "    Passes the accumulated conversation history to the evaluate function for each new input.\n",
    "\n",
    "### Training and Evaluation Process\n",
    "\n",
    "The model is trained with input sequences that include conversation history, allowing it to learn the context of the conversation.\n",
    "During evaluation, the model uses the accumulated conversation history to generate more context-aware responses.\n",
    "\n",
    "### General Setup\n",
    "\n",
    "Model, optimizer, and training configurations are set according to your specifications.\n",
    "The training process (train_iters) follows the standard approach but uses the modified input pairs with conversation history.\n",
    "\n",
    "### Important Notes\n",
    "\n",
    "The effectiveness of including conversation history in the model depends on the depth of context the model can understand and how well it can handle longer input sequences.\n",
    "Fine-tuning and experimenting with the MAX_HISTORY parameter and the MAX_LENGTH of sequences may be necessary to achieve optimal performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9BnEtHHxUHC7"
   },
   "outputs": [],
   "source": [
    "MAX_LENGTH = 10  # Maximum length of a single sentence, adjust as necessary\n",
    "MAX_HISTORY = 5  # Number of previous exchanges to include in the history\n",
    "\n",
    "def create_history_pairs(pairs, max_history=MAX_HISTORY):\n",
    "    history_pairs = []\n",
    "    for i in range(len(pairs)):\n",
    "        dialogue_history = ' EOS '.join([pairs[j][0] for j in range(max(0, i-max_history), i)])\n",
    "        history_pairs.append([dialogue_history, pairs[i][1]])\n",
    "    return history_pairs\n",
    "\n",
    "def zero_padding(l, fillvalue=0):\n",
    "    return list(itertools.zip_longest(*l, fillvalue=fillvalue))\n",
    "\n",
    "def binary_matrix(l, value=0):\n",
    "    m = []\n",
    "    for i, seq in enumerate(l):\n",
    "        m.append([])\n",
    "        for token in seq:\n",
    "            if token == value:\n",
    "                m[i].append(0)\n",
    "            else:\n",
    "                m[i].append(1)\n",
    "    return m\n",
    "\n",
    "def output_var(l, voc):\n",
    "    indexes_batch = [indexes_from_sentence(voc, sentence) for sentence in l]\n",
    "    max_target_len = max([len(indexes) for indexes in indexes_batch])\n",
    "    pad_list = zero_padding(indexes_batch)\n",
    "    mask = binary_matrix(pad_list)\n",
    "    mask = torch.BoolTensor(mask)\n",
    "    pad_var = torch.LongTensor(pad_list)\n",
    "    return pad_var, mask, max_target_len\n",
    "\n",
    "def indexes_from_sentence(voc, sentence):\n",
    "    #return [voc.word_to_index[word] for word in sentence.split(' ')] + [EOS_TOKEN]\n",
    "    return [voc.word_to_index[word] for word in sentence.split()] + [EOS_TOKEN] \n",
    "\n",
    "def batch_to_train_data(voc, pair_batch):\n",
    "    pair_batch.sort(key=lambda p: len(p[0].split()), reverse=True)\n",
    "    input_batch, output_batch = [], []\n",
    "    for pair in pair_batch:\n",
    "        input_batch.append(pair[0])\n",
    "        output_batch.append(pair[1])\n",
    "    inp, lengths = input_var(input_batch, voc)\n",
    "    output, mask, max_target_len = output_var(output_batch, voc)\n",
    "    return inp, lengths, output, mask, max_target_len\n",
    "\n",
    "# Input_var function needs to calculate the correct lengths for packed sequences\n",
    "def input_var(l, voc):\n",
    "    indexes_batch = [indexes_from_sentence(voc, sentence) for sentence in l]\n",
    "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
    "    pad_list = zero_padding(indexes_batch)\n",
    "    pad_var = torch.LongTensor(pad_list)\n",
    "    return pad_var, lengths\n",
    "# Prepare the data with history\n",
    "history_pairs = create_history_pairs(pairs)\n",
    "\n",
    "# Modify the training function to handle the dialogue history in the input sequences\n",
    "def train(input_variable, lengths, target_variable, mask, max_target_len, encoder, decoder, embedding,\n",
    "          encoder_optimizer, decoder_optimizer, batch_size, clip, max_length=MAX_LENGTH):\n",
    "\n",
    "    # Zero gradients\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    # Set device options\n",
    "    input_variable = input_variable.to(device)\n",
    "    target_variable = target_variable.to(device)\n",
    "    mask = mask.to(device)\n",
    "    # Lengths for RNN packing should always be on the CPU\n",
    "    lengths = lengths.to(\"cpu\")\n",
    "\n",
    "    # Initialize variables\n",
    "    loss = 0\n",
    "    print_losses = []\n",
    "    n_totals = 0\n",
    "\n",
    "    # Forward pass through encoder\n",
    "    encoder_outputs, encoder_hidden = encoder(input_variable, lengths)\n",
    "\n",
    "    # Create initial decoder input (start with SOS tokens for each sentence)\n",
    "    decoder_input = torch.LongTensor([[SOS_TOKEN for _ in range(batch_size)]])\n",
    "    decoder_input = decoder_input.to(device)\n",
    "\n",
    "    # Set initial decoder hidden state to the encoder's final hidden state\n",
    "    decoder_hidden = encoder_hidden[:decoder.n_layers]\n",
    "\n",
    "    # Determine if we are using teacher forcing this iteration\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    # Forward batch of sequences through decoder one time step at a time\n",
    "    if use_teacher_forcing:\n",
    "        for t in range(max_target_len):\n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden\n",
    "            )\n",
    "            # Teacher forcing: next input is current target\n",
    "            decoder_input = target_variable[t].view(1, -1)\n",
    "            # Calculate and accumulate loss\n",
    "            mask_loss, nTotal = mask_nll_loss(decoder_output, target_variable[t], mask[t],device)\n",
    "            loss += mask_loss\n",
    "            print_losses.append(mask_loss.item() * nTotal)\n",
    "            n_totals += nTotal\n",
    "    else:\n",
    "        for t in range(max_target_len):\n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden\n",
    "            )\n",
    "            # No teacher forcing: next input is decoder's own current output\n",
    "            _, topi = decoder_output.topk(1)\n",
    "            decoder_input = torch.LongTensor([[topi[i][0] for i in range(batch_size)]])\n",
    "            decoder_input = decoder_input.to(device)\n",
    "            # Calculate and accumulate loss\n",
    "            mask_loss, nTotal = mask_nll_loss(decoder_output, target_variable[t], mask[t],device)\n",
    "            loss += mask_loss\n",
    "            print_losses.append(mask_loss.item() * nTotal)\n",
    "            n_totals += nTotal\n",
    "\n",
    "    # Perform backpropagation\n",
    "    loss.backward()\n",
    "\n",
    "    # Clip gradients: gradients are modified in place\n",
    "    _ = nn.utils.clip_grad_norm_(encoder.parameters(), clip)\n",
    "    _ = nn.utils.clip_grad_norm_(decoder.parameters(), clip)\n",
    "\n",
    "    # Adjust model weights\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return sum(print_losses) / n_totals\n",
    "\n",
    "def train_iters(model_name, voc, history_pairs, encoder, decoder, encoder_optimizer, decoder_optimizer, embedding,\n",
    "                encoder_n_layers, decoder_n_layers, save_dir, n_iteration, batch_size, print_every, save_every,\n",
    "                clip, corpus_name):\n",
    "    \"\"\"\n",
    "    Run training for a set number of iterations.\n",
    "    \"\"\"\n",
    "    # Load batches for each iteration\n",
    "    training_batches = [batch_to_train_data(voc, [random.choice(history_pairs) for _ in range(batch_size)])\n",
    "                        for _ in range(n_iteration)]\n",
    "\n",
    "    print('Initializing ...')\n",
    "    start_iteration = 1\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "\n",
    "    print(\"Training...\")\n",
    "    for iteration in range(start_iteration, n_iteration + 1):\n",
    "        training_batch = training_batches[iteration - 1]\n",
    "        input_variable, lengths, target_variable, mask, max_target_len = training_batch\n",
    "\n",
    "        # Run a training iteration with batch\n",
    "        loss = train(input_variable, lengths, target_variable, mask, max_target_len, encoder,\n",
    "                     decoder, embedding, encoder_optimizer, decoder_optimizer, batch_size, clip)\n",
    "        print_loss_total += loss\n",
    "\n",
    "        # Print progress\n",
    "        if iteration % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print(f\"Iteration: {iteration}; Percent complete: {iteration / n_iteration * 100:.1f}%; Average loss: {print_loss_avg:.4f}\")\n",
    "            print_loss_total = 0\n",
    "\n",
    "        # Save checkpoint\n",
    "        if iteration % save_every == 0:\n",
    "            directory = os.path.join(save_dir, model_name, corpus_name, f'{encoder_n_layers}-{decoder_n_layers}_{hidden_features}')\n",
    "            if not os.path.exists(directory):\n",
    "                os.makedirs(directory)\n",
    "            torch.save({\n",
    "                'iteration': iteration,\n",
    "                'en': encoder.state_dict(),\n",
    "                'de': decoder.state_dict(),\n",
    "                'en_opt': encoder_optimizer.state_dict(),\n",
    "                'de_opt': decoder_optimizer.state_dict(),\n",
    "                'loss': loss,\n",
    "                'voc_dict': voc.__dict__,\n",
    "                'embedding': embedding.state_dict()\n",
    "            }, os.path.join(directory, f'{iteration}_checkpoint.tar'))\n",
    "\n",
    "def evaluate(searcher, voc, conversation_history, max_length=MAX_LENGTH):\n",
    "    \"\"\"\n",
    "    Evaluate a conversation history using the encoder, decoder, and searcher provided.\n",
    "    \"\"\"\n",
    "    # Join the conversation history into a single input and normalize\n",
    "    input_sentence = ' '.join(conversation_history)\n",
    "    input_sentence = normalize_string(input_sentence)\n",
    "\n",
    "    # Prepare the input sentence as a batch of word indexes\n",
    "    indexes_batch = [indexes_from_sentence(voc, input_sentence)]\n",
    "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
    "    input_batch = torch.LongTensor(indexes_batch).transpose(0, 1).to(device)\n",
    "    lengths = lengths.to(\"cpu\")  # Lengths need to be on CPU for pack_padded_sequence\n",
    "\n",
    "    # Decode the sentence with the searcher\n",
    "    tokens, scores = searcher(input_batch, lengths, max_length)\n",
    "    decoded_words = [voc.index_to_word[token.item()] for token in tokens]\n",
    "\n",
    "    return decoded_words\n",
    "\n",
    "\n",
    "def evaluate_input(searcher, voc):\n",
    "    \"\"\"\n",
    "    Interactively evaluate input from the user, considering the entire conversation history.\n",
    "    \"\"\"\n",
    "    conversation_history = []\n",
    "    while True:\n",
    "        try:\n",
    "            input_sentence = input('> ')\n",
    "            if input_sentence in ('q', 'quit'):\n",
    "                break\n",
    "\n",
    "            # Normalize and add the user's input to the conversation history\n",
    "            input_sentence = normalize_string(input_sentence)\n",
    "            conversation_history.append(input_sentence)\n",
    "\n",
    "            # Evaluate the conversation history\n",
    "            output_words = evaluate(searcher, voc, conversation_history)\n",
    "            output_words = [word for word in output_words if word not in ('EOS', 'PAD')]\n",
    "            print('Bot:', ' '.join(output_words))\n",
    "\n",
    "            # Add the bot's response to the conversation history\n",
    "            conversation_history.extend(output_words)\n",
    "            conversation_history=conversation_history[-MAX_HISTORY*MAX_LENGTH:]\n",
    "            \n",
    "        except KeyError:\n",
    "            print(\"Error: Encountered unknown word.\")\n",
    "\n",
    "# Set configuration parameters for the model\n",
    "model_name = 'cb_model'\n",
    "hidden_features = 2000\n",
    "in_features=hidden_features\n",
    "encoder_n_layers = 4\n",
    "decoder_n_layers = 4\n",
    "dropout = 0.1\n",
    "batch_size = 64\n",
    "\n",
    "# Set training and optimization parameters\n",
    "clip = 50.0\n",
    "teacher_forcing_ratio = 1.0\n",
    "learning_rate = 0.0001\n",
    "decoder_learning_ratio = 5.0\n",
    "n_iteration = 16000\n",
    "print_every = 1\n",
    "save_every = 8000\n",
    "\n",
    "# Initialize word embeddings and encoder & decoder models\n",
    "embedding = nn.Embedding(voc.num_words, hidden_features)\n",
    "# Initialize the EncoderRNN\n",
    "encoder =dl.EncoderRNN(\n",
    "    in_features=in_features, \n",
    "    hidden_features=hidden_features,\n",
    "    num_layers=encoder_n_layers, \n",
    "    embedding=embedding,\n",
    "    rnn_type=rnn_type, \n",
    "    dropout=dropout\n",
    ").to(device)\n",
    "\n",
    "decoder =dl.DecoderRNN(\n",
    "    in_features=in_features, \n",
    "    hidden_features=hidden_features,\n",
    "    out_features=voc.num_words,\n",
    "    num_layers=decoder_n_layers, \n",
    "    embedding=embedding,\n",
    "    rnn_type=rnn_type, \n",
    "    dropout=dropout\n",
    ").to(device)\n",
    "\n",
    "encoder.build()\n",
    "decoder.build()\n",
    "# Set models to training mode\n",
    "encoder.train()\n",
    "decoder.train()\n",
    "\n",
    "# Initialize optimizers for the encoder and decoder\n",
    "encoder_optimizer = torch.optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=learning_rate * decoder_learning_ratio)\n",
    "\n",
    "# Move optimizer states to GPU if necessary\n",
    "if torch.cuda.is_available():\n",
    "    for state in encoder_optimizer.state.values():\n",
    "        for k, v in state.items():\n",
    "            if isinstance(v, torch.Tensor):\n",
    "                state[k] = v.cuda()\n",
    "\n",
    "    for state in decoder_optimizer.state.values():\n",
    "        for k, v in state.items():\n",
    "            if isinstance(v, torch.Tensor):\n",
    "                state[k] = v.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Begin the training process\n",
    "train_iters(model_name, voc, history_pairs, encoder, decoder, encoder_optimizer, decoder_optimizer,\n",
    "           embedding, encoder_n_layers, decoder_n_layers, save_dir, n_iteration, batch_size,\n",
    "           print_every, save_every, clip, corpus_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XRpMW5HFdtdo"
   },
   "outputs": [],
   "source": [
    "\n",
    "def evaluate(searcher, voc, conversation_history, max_length=MAX_LENGTH):\n",
    "    \"\"\"\n",
    "    Evaluate a conversation history using the encoder, decoder, and searcher provided.\n",
    "    \"\"\"\n",
    "    # Join the conversation history into a single input and normalize\n",
    "    input_sentence = ' '.join(conversation_history)\n",
    "    input_sentence = normalize_string(input_sentence)\n",
    "    input_sentence=input_sentence.replace(\"eos\",\"EOS\")\n",
    "    \n",
    "    # Prepare the input sentence as a batch of word indexes\n",
    "    indexes_batch = [indexes_from_sentence(voc, input_sentence)]\n",
    "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
    "    input_batch = torch.LongTensor(indexes_batch).transpose(0, 1).to(device)\n",
    "    lengths = lengths.to(\"cpu\")  # Lengths need to be on CPU for pack_padded_sequence\n",
    "\n",
    "    # Decode the sentence with the searcher\n",
    "    tokens, scores = searcher(input_batch, lengths, max_length)\n",
    "    decoded_words = [voc.index_to_word[token.item()] for token in tokens]\n",
    "\n",
    "    return decoded_words\n",
    "\n",
    "def evaluate_input(searcher, voc):\n",
    "    \"\"\"\n",
    "    Interactively evaluate input from the user, considering the entire conversation history.\n",
    "    \"\"\"\n",
    "    conversation_history = []\n",
    "    while True:\n",
    "        try:\n",
    "            input_sentence = input('> ')\n",
    "            if input_sentence in ('q', 'quit'):\n",
    "                break\n",
    "\n",
    "            # Normalize and add the user's input to the conversation history\n",
    "            print(input_sentence)\n",
    "            input_sentence = normalize_string(input_sentence)\n",
    "            conversation_history.append(input_sentence)\n",
    "            conversation_history.append(\"EOS\")\n",
    "\n",
    "\n",
    "            # Evaluate the conversation history\n",
    "            output_words = evaluate(searcher, voc, conversation_history)\n",
    "            print('Bot:', ' '.join([word for word in output_words if word not in ('EOS', 'PAD')]))\n",
    "\n",
    "            # Add the bot's response to the conversation history\n",
    "            conversation_history.extend(output_words)\n",
    "            conversation_history=conversation_history[-MAX_HISTORY*MAX_LENGTH:]\n",
    "            \n",
    "\n",
    "        except KeyError:\n",
    "            print(\"Error: Encountered unknown word.\")\n",
    "\n",
    "# Set dropout layers to ``eval`` mode\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "\n",
    "# Initialize search module\n",
    "searcher = GreedySearchDecoder(encoder, decoder)\n",
    "\n",
    "# Begin chatting (uncomment and run the following line to begin)\n",
    "evaluate_input(searcher, voc)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
