{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"./dialogs.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "import re\n",
    "import contractions\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    standardized_text = contractions.fix(text)\n",
    "\n",
    "    standardized_text = (\n",
    "        standardized_text.replace(\"’\", \"'\")\n",
    "        .replace(\"‘\", \"'\")\n",
    "        .replace(\"´\", \"'\")\n",
    "        .replace(\"“\", '\"')\n",
    "        .replace(\"”\", '\"')\n",
    "        .replace(\"´´\", '\"')\n",
    "    )\n",
    "\n",
    "    tokens = tokenizer(standardized_text)\n",
    "\n",
    "    filtered_tokens = [\n",
    "        token\n",
    "        for token in tokens\n",
    "        if re.match(\n",
    "            r\"^[a-zA-Z0-9.,!?]+(-[a-zA-Z0-9.,!?]+)*(_[a-zA-Z0-9.,!?]+)*$\", token\n",
    "        )\n",
    "    ]\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "\n",
    "def corpus_iterator(filepath):\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "        prev_reply = None\n",
    "        for line in lines:\n",
    "\n",
    "            query, reply = line.strip().split(\"\\t\")\n",
    "\n",
    "            # Check if not the last line and if the current reply is identical to the next query\n",
    "            if query == prev_reply:\n",
    "                out = reply\n",
    "            else:\n",
    "                out = query + reply\n",
    "            prev_reply = reply\n",
    "\n",
    "            yield tokenize(out)\n",
    "\n",
    "\n",
    "# Add EOS, SOS, and PAD to the specials list\n",
    "special_tokens = [\"<pad>\", \"<sos>\", \"<eos>\", \"<unk>\"]\n",
    "\n",
    "vocab = build_vocab_from_iterator(\n",
    "    corpus_iterator(filepath),\n",
    "    specials=special_tokens,\n",
    "    min_freq=2,\n",
    ")\n",
    "vocab.set_default_index(vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1486"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'love'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.lookup_token(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of queries/responses: 2470\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "MAX_LENGTH = 30\n",
    "\n",
    "queries, responses, masks_r = [], [], []\n",
    "\n",
    "\n",
    "def all_words_in_vocab(sentence, vocab):\n",
    "    return all(word in vocab for word in sentence)\n",
    "\n",
    "\n",
    "def process_sentence(sequence, max_length=MAX_LENGTH):\n",
    "    # Calculate the length needed for padding. Subtract 2 for <sos> and <eos> tokens\n",
    "    padding_length = max_length - len(sequence) + 1\n",
    "\n",
    "    # Processed sequence with <sos>, <eos>, and <pad>\n",
    "    processed = [\"<sos>\"] + sequence + [\"<eos>\"] + [\"<pad>\"] * padding_length\n",
    "\n",
    "    # Create a mask: 1s for actual tokens and 0s for padding\n",
    "    # The mask length is len(sequence) + 2 for <sos> and <eos> tokens. The rest are 0s for padding.\n",
    "    mask = [1] * (len(sequence) + 2) + [0] * padding_length\n",
    "\n",
    "    return processed, mask\n",
    "\n",
    "\n",
    "with open(filepath, \"r\", encoding=\"utf-8\") as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "    prev_reply = None\n",
    "    for line in lines:\n",
    "\n",
    "        q, r = line.strip().split(\"\\t\")\n",
    "\n",
    "        query = tokenize(q)\n",
    "        response = tokenize(r)\n",
    "\n",
    "        if (\n",
    "            all_words_in_vocab(query + response, vocab)\n",
    "            and len(query) <= MAX_LENGTH\n",
    "            and len(response) <= MAX_LENGTH\n",
    "        ):\n",
    "            query, _ = process_sentence(query)\n",
    "            response, mask_r = process_sentence(response)\n",
    "\n",
    "            queries.append(vocab(query))\n",
    "            responses.append(vocab(response))\n",
    "            masks_r.append(mask_r)\n",
    "\n",
    "queries = np.asarray(queries)\n",
    "responses = np.asarray(responses)\n",
    "masks_r = np.asarray(masks_r)\n",
    "\n",
    "print(f\"Number of queries/responses: {len(queries)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(queries[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# The dimensionality of GloVe embeddings\n",
    "embedding_dim = 300\n",
    "\n",
    "from torchtext.vocab import GloVe\n",
    "\n",
    "# Load GloVe embeddings\n",
    "glove = GloVe(name=\"42B\", dim=embedding_dim, cache=\"./.vector_cache\")\n",
    "\n",
    "# Get GloVe embeddings for the vocabulary tokens\n",
    "# Assuming 'vocab' is a list of vocabulary tokens including special tokens at the beginning\n",
    "glove_embeddings = glove.get_vecs_by_tokens(vocab.get_itos(), lower_case_backup=True)\n",
    "\n",
    "\n",
    "# Special tokens\n",
    "special_tokens = [\"<pad>\", \"<sos>\", \"<eos>\"]\n",
    "num_special_tokens = len(special_tokens)\n",
    "\n",
    "# Initialize a tensor to hold the embeddings for special tokens\n",
    "# Here, PAD is initialized to zeros, and SOS, EOS to random values\n",
    "special_embeddings = torch.zeros(num_special_tokens, embedding_dim)\n",
    "special_embeddings[1:] = (\n",
    "    torch.rand(num_special_tokens - 1, embedding_dim) * 0.01\n",
    ")  # Small random numbers for SOS and EOS\n",
    "\n",
    "\n",
    "# Concatenate the special token embeddings with the GloVe embeddings\n",
    "extended_embeddings = torch.cat([special_embeddings, glove_embeddings], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/841602/Documents/GitHub/Environments/deeplay_env/lib/python3.10/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import deeplay as dl\n",
    "from deeplay import DeeplayModule, Classifier\n",
    "\n",
    "hidden_features = 150\n",
    "\n",
    "\n",
    "class MyClassifier(Classifier):\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x1, x2, m = batch\n",
    "        y = torch.cat((x2[:, 1:], x2[:, -1:]), dim=1)\n",
    "        y_hat = self(x1, x2)\n",
    "        loss = self.loss(y_hat, y, m)\n",
    "        # loss = self.loss(y_hat.view(-1, y_hat.size(-1)), y.view(-1))\n",
    "        self.log(\n",
    "            f\"train_loss\",\n",
    "            loss,\n",
    "            on_step=True,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "            logger=True,\n",
    "        )\n",
    "\n",
    "        self.log_metrics(\n",
    "            \"train\", y_hat, y, on_step=True, on_epoch=True, prog_bar=True, logger=True\n",
    "        )\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        return self.model(x1, x2)\n",
    "\n",
    "\n",
    "class Encoder(DeeplayModule):\n",
    "    def __init__(self, vocab_size, embedding_dim, lstm_units):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, lstm_units, batch_first=True, dropout=0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        # x = torch.nn.utils.rnn.pack_padded_sequence(x, lengths)\n",
    "        outputs, (hidden, cell) = self.lstm(x)\n",
    "        return hidden, cell\n",
    "\n",
    "\n",
    "class Decoder(DeeplayModule):\n",
    "    def __init__(self, vocab_size, embedding_dim, lstm_units):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, lstm_units, batch_first=True, dropout=0.1)\n",
    "        self.dense = nn.Linear(lstm_units, vocab_size)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x, hidden, cell):\n",
    "        x = self.embedding(x)\n",
    "        outputs, (hidden, cell) = self.lstm(x, (hidden, cell))\n",
    "        outputs = self.dense(outputs)\n",
    "        outputs = self.softmax(outputs)\n",
    "        return outputs, hidden, cell\n",
    "\n",
    "\n",
    "class Seq2Seq(DeeplayModule):\n",
    "    def __init__(self, vocab_size, embedding_dim, lstm_units):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = Encoder(vocab_size, embedding_dim, lstm_units)\n",
    "        self.decoder = Decoder(vocab_size, embedding_dim, lstm_units)\n",
    "        self.vocab_size = vocab_size\n",
    "        self.lstm_units = lstm_units\n",
    "\n",
    "    def forward(self, encoder_input_data, decoder_input_data):\n",
    "        encoder_hidden, encoder_cell = self.encoder(encoder_input_data)\n",
    "\n",
    "        decoder_hidden = encoder_hidden\n",
    "        decoder_cell = encoder_cell\n",
    "\n",
    "        outputs = torch.zeros(\n",
    "            (decoder_input_data.size(0), decoder_input_data.size(1), self.vocab_size)\n",
    "        ).to(\"mps\")\n",
    "        for t in range(decoder_input_data.size(1)):  # Iterate through the sequence\n",
    "            output, decoder_hidden, decoder_cell = self.decoder(\n",
    "                decoder_input_data[:, t].unsqueeze(-1), decoder_hidden, decoder_cell\n",
    "            )\n",
    "            outputs[:, t, :] = output.squeeze(1)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "seq2seq = Seq2Seq(len(vocab), embedding_dim, hidden_features)\n",
    "\n",
    "\n",
    "def NLLLoss(inp, target, mask):\n",
    "    crossEntropy = -torch.log(\n",
    "        torch.gather(inp.view(-1, inp.shape[-1]), 1, target.view(-1, 1))\n",
    "    )\n",
    "    loss = crossEntropy.masked_select(mask.view(-1, 1)).mean()\n",
    "    return loss  # , nTotal.item()\n",
    "\n",
    "\n",
    "seq2seq_classifier = MyClassifier(\n",
    "    model=seq2seq,\n",
    "    loss=NLLLoss,  # nn.CrossEntropyLoss(),\n",
    "    optimizer=dl.Adam(),\n",
    ").create()\n",
    "\n",
    "seq2seq_classifier.model.encoder.embedding.weight.data = extended_embeddings\n",
    "seq2seq_classifier.model.encoder.embedding.weight.requires_grad = False\n",
    "seq2seq_classifier.model.decoder.embedding.weight.data = extended_embeddings\n",
    "seq2seq_classifier.model.decoder.embedding.weight.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deeptrack as dt\n",
    "import torch\n",
    "\n",
    "sources = dt.sources.Source(inputs=queries, targets=responses, masks=masks_r)\n",
    "\n",
    "inputs_pl = dt.Value(sources.inputs) >> dt.pytorch.ToTensor(dtype=torch.int)\n",
    "targets_pl = dt.Value(sources.targets) >> dt.pytorch.ToTensor(dtype=torch.int)\n",
    "masks_pl = dt.Value(sources.masks) >> dt.pytorch.ToTensor(dtype=torch.bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataset = dt.pytorch.Dataset(inputs_pl & targets_pl & masks_pl, inputs=sources)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = dl.Trainer(max_epochs=100, accelerator=\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/841602/Documents/GitHub/Environments/deeplay_env/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:74: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
       "┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name          </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type             </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span>┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span>│ train_metrics │ MetricCollection │      0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 </span>│ val_metrics   │ MetricCollection │      0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2 </span>│ test_metrics  │ MetricCollection │      0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3 </span>│ model         │ Seq2Seq          │  1.7 M │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 4 </span>│ optimizer     │ Adam             │      0 │\n",
       "└───┴───────────────┴──────────────────┴────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
       "┃\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mName         \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mType            \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
       "│\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m│ train_metrics │ MetricCollection │      0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m1\u001b[0m\u001b[2m \u001b[0m│ val_metrics   │ MetricCollection │      0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m2\u001b[0m\u001b[2m \u001b[0m│ test_metrics  │ MetricCollection │      0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m3\u001b[0m\u001b[2m \u001b[0m│ model         │ Seq2Seq          │  1.7 M │\n",
       "│\u001b[2m \u001b[0m\u001b[2m4\u001b[0m\u001b[2m \u001b[0m│ optimizer     │ Adam             │      0 │\n",
       "└───┴───────────────┴──────────────────┴────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 766 K                                                                                            \n",
       "<span style=\"font-weight: bold\">Non-trainable params</span>: 893 K                                                                                        \n",
       "<span style=\"font-weight: bold\">Total params</span>: 1.7 M                                                                                                \n",
       "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 6                                                                          \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mTrainable params\u001b[0m: 766 K                                                                                            \n",
       "\u001b[1mNon-trainable params\u001b[0m: 893 K                                                                                        \n",
       "\u001b[1mTotal params\u001b[0m: 1.7 M                                                                                                \n",
       "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 6                                                                          \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e74df2b60b764ff9805831dfdcccada4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/841602/Documents/GitHub/Environments/deeplay_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.fit(seq2seq_classifier, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_inference(model, source_text, max_length=MAX_LENGTH):\n",
    "    # Tokenize the source text\n",
    "    query_tokens = tokenize(source_text)\n",
    "\n",
    "    # Process the tokens into the model's expected format, including adding <sos>, <eos>, and padding\n",
    "    query, _ = process_sentence(query_tokens)\n",
    "\n",
    "    # Convert tokens to indices using the vocabulary\n",
    "    query = np.array(vocab(query))\n",
    "\n",
    "    # Convert list of indices to a tensor and add a batch dimension\n",
    "    source_sequence = torch.tensor(query, dtype=torch.int)\n",
    "\n",
    "    # Move tensor to the same device as the model\n",
    "    source_sequence = source_sequence.to(next(model.parameters()).device)\n",
    "\n",
    "    # Encoder inference\n",
    "    with torch.no_grad():\n",
    "        hidden, cell = model.encoder(source_sequence)\n",
    "\n",
    "    # Prepare the initial input to the decoder: <sos> token index\n",
    "    target_index = torch.tensor(vocab([\"<sos>\"]), device=source_sequence.device)\n",
    "\n",
    "    predictions = []\n",
    "    for _ in range(max_length):\n",
    "        with torch.no_grad():\n",
    "            output, hidden, cell = model.decoder(target_index, hidden, cell)\n",
    "            top1 = output.argmax(1)  # Adjust indexing based on output shape\n",
    "            if top1.item() == vocab([\"<eos>\"])[0]:  # Stop if <eos> token is generated\n",
    "                break\n",
    "            predictions.append(top1.item())\n",
    "            target_index = top1\n",
    "\n",
    "    # Convert indices back to tokens\n",
    "    predicted_tokens = [vocab.lookup_token(idx) for idx in predictions]\n",
    "\n",
    "    return \" \".join(predicted_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i am not sure .\n"
     ]
    }
   ],
   "source": [
    "source_text = \"who is the president?\"\n",
    "response = make_inference(seq2seq_classifier.model, source_text)\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
