{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this guide, we delve into an engaging and intriguing application of recurrent sequence-to-sequence models by training a basic chatbot using film scripts from the Cornell Movie-Dialogs Corpus.\n",
    "\n",
    "Conversational models are currently a trending subject in the field of AI research. Chatbots are commonly used in various scenarios such as customer service platforms and online help desks. These bots typically utilize retrieval-based models that provide pre-set responses to specific types of questions. While these models might be adequate for highly specific domains like a company's IT helpdesk, they lack the robustness required for broader applications. However, the recent surge in deep learning, spearheaded recently by ChatGPT, has led to the development of potent multi-domain generative conversational models. In this guide, we will create one such model using the tools we have learnt so far.\n",
    "\n",
    "To begin, download the dialog dataset: \n",
    "\n",
    "https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html (should be replaced with direct data source for DLCC)\n",
    "\n",
    "and put in a ``data/`` directory under the current directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://zissou.infosci.cornell.edu/convokit/datasets/movie-corpus/movie-corpus.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing\n",
    "\n",
    "This step involves reorganizing our data file and loading the data into formats that are manageable.\n",
    "\n",
    "The [Cornell Movie-Dialogs Corpus](https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html) is a comprehensive dataset of dialogues from movie characters:\n",
    "\n",
    "- It contains 220,579 conversational exchanges between 10,292 pairs of movie characters.\n",
    "- It features 9,035 characters from 617 movies.\n",
    "- It has a total of 304,713 utterances.\n",
    "\n",
    "This dataset is vast and varied, with a wide range of language formality, time periods, sentiment, etc. We anticipate that this diversity will make our model capable of handling a variety of inputs and queries.\n",
    "\n",
    "Initially, we will examine some lines from our data file to understand the original format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# from convokit import download, Corpus\n",
    "\n",
    "# import ssl\n",
    "\n",
    "# ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "# corpus_name = \"tennis-corpus\"  # \"movie-corpus\"\n",
    "# if not os.path.exists(corpus_name):\n",
    "#     download(corpus_name, data_dir=\"./\")\n",
    "\n",
    "# corpus = Corpus(corpus_name)\n",
    "# corpus.print_summary_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "an uninterrupted chain of spoken or written language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for _ in range(5):\n",
    "#     print(corpus.random_utterance().text)\n",
    "\n",
    "# ex_text = corpus.random_utterance().text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "if not os.path.exists(\"chatterbot-corpus\"):\n",
    "    os.system(\"git clone https://github.com/gunthercox/chatterbot-corpus.git\")\n",
    "\n",
    "data_dir = os.path.join(\"chatterbot-corpus\", \"chatterbot_corpus\", \"data\", \"english\")\n",
    "files_list = os.listdir(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['politics.yml',\n",
       " 'ai.yml',\n",
       " 'emotion.yml',\n",
       " 'computers.yml',\n",
       " 'botprofile.yml',\n",
       " 'history.yml',\n",
       " 'psychology.yml',\n",
       " 'food.yml',\n",
       " 'literature.yml',\n",
       " 'money.yml',\n",
       " 'trivia.yml',\n",
       " 'gossip.yml',\n",
       " 'humor.yml',\n",
       " 'conversations.yml',\n",
       " 'greetings.yml',\n",
       " 'sports.yml',\n",
       " 'movies.yml',\n",
       " 'science.yml',\n",
       " 'health.yml']"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "import re\n",
    "import contractions\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    standardized_text = contractions.fix(text)\n",
    "\n",
    "    standardized_text = (\n",
    "        standardized_text.replace(\"’\", \"'\")\n",
    "        .replace(\"‘\", \"'\")\n",
    "        .replace(\"´\", \"'\")\n",
    "        .replace(\"“\", '\"')\n",
    "        .replace(\"”\", '\"')\n",
    "        .replace(\"´´\", '\"')\n",
    "    )\n",
    "\n",
    "    tokens = tokenizer(standardized_text)\n",
    "\n",
    "    filtered_tokens = [\n",
    "        token\n",
    "        for token in tokens\n",
    "        if re.match(\n",
    "            r\"^[a-zA-Z0-9.,!?]+(-[a-zA-Z0-9.,!?]+)*(_[a-zA-Z0-9.,!?]+)*$\", token\n",
    "        )\n",
    "    ]\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "\n",
    "# def corpus_iterator(corpus):  # (1)\n",
    "#     for utt_id in corpus.utterances:\n",
    "#         utt = corpus.get_utterance(utt_id)\n",
    "#         yield tokenize(utt.text)\n",
    "\n",
    "\n",
    "# # Add EOS, SOS, and PAD to the specials list\n",
    "# special_tokens = [\"<unk>\", \"<pad>\", \"<sos>\", \"<eos>\"]\n",
    "\n",
    "# vocab = build_vocab_from_iterator(\n",
    "#     corpus_iterator(corpus),  # (2)\n",
    "#     specials=special_tokens,  # (3),\n",
    "#     min_freq=5,  # (4)\n",
    "# )\n",
    "# vocab.set_default_index(vocab[\"<unk>\"])  # (5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "\n",
    "def corpus_iterator(files_list):  # (1)\n",
    "    for filepath in files_list:\n",
    "        subject = open(data_dir + os.sep + filepath, \"rb\")\n",
    "        docs = yaml.safe_load(subject)\n",
    "        conversations = docs[\"conversations\"]\n",
    "        for con in conversations:\n",
    "            range_end = len(con) - (len(con) % 2)\n",
    "            for i in range(0, range_end):\n",
    "                yield tokenize(con[i])\n",
    "\n",
    "\n",
    "# Add EOS, SOS, and PAD to the specials list\n",
    "special_tokens = [\"<pad>\", \"<sos>\", \"<eos>\", \"<unk>\"]\n",
    "\n",
    "vocab = build_vocab_from_iterator(\n",
    "    corpus_iterator(files_list),\n",
    "    specials=special_tokens,\n",
    "    min_freq=2,\n",
    ")\n",
    "vocab.set_default_index(vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "791"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<pad>'"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.lookup_token(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# MAX_LENGTH = 30\n",
    "\n",
    "# queries = []\n",
    "# responses = []\n",
    "\n",
    "\n",
    "# def all_words_in_vocab(sentence, vocab):\n",
    "#     return all(word in vocab for word in sentence)\n",
    "\n",
    "\n",
    "# def process_sentence(sequence, max_length=MAX_LENGTH):\n",
    "#     processed = (\n",
    "#         [\"<sos>\"] + sequence + [\"<eos>\"] + [\"<pad>\"] * (max_length - len(sequence) + 1)\n",
    "#     )\n",
    "#     return processed\n",
    "\n",
    "\n",
    "# for conv_id in corpus.conversations:\n",
    "#     conv = corpus.get_conversation(conv_id)\n",
    "#     utt_ids = conv.get_utterance_ids()\n",
    "\n",
    "#     range_end = len(utt_ids) - (len(utt_ids) % 2)\n",
    "#     # Adjust range to exclude the last utterance in conversations with an odd number of utterances\n",
    "\n",
    "#     for i in range(0, range_end, 2):\n",
    "\n",
    "#         query = tokenize(corpus.get_utterance(utt_ids[i]).text)\n",
    "#         response = tokenize(corpus.get_utterance(utt_ids[i + 1]).text)\n",
    "\n",
    "#         if (\n",
    "#             all_words_in_vocab(query + response, vocab)\n",
    "#             and len(query) <= MAX_LENGTH\n",
    "#             and len(response) <= MAX_LENGTH\n",
    "#         ):\n",
    "#             query = process_sentence(query)\n",
    "#             response = process_sentence(response)\n",
    "\n",
    "#             queries.append(vocab(query))\n",
    "#             responses.append(vocab(response))\n",
    "\n",
    "# queries = np.asarray(queries)\n",
    "# responses = np.asarray(responses)\n",
    "\n",
    "# print(f\"Number of queries/responses: {len(queries)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of queries/responses: 689\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "MAX_LENGTH = 30\n",
    "\n",
    "queries, responses, masks_q, masks_r = [], [], [], []\n",
    "\n",
    "\n",
    "def all_words_in_vocab(sentence, vocab):\n",
    "    return all(word in vocab for word in sentence)\n",
    "\n",
    "\n",
    "# def process_sentence(sequence, max_length=MAX_LENGTH):\n",
    "#     processed = (\n",
    "#         [\"<sos>\"] + sequence + [\"<eos>\"] + [\"<pad>\"] * (max_length - len(sequence) + 1)\n",
    "#     )\n",
    "#     mask =\n",
    "#     return processed, mask\n",
    "def process_sentence(sequence, max_length=MAX_LENGTH):\n",
    "    # Calculate the length needed for padding. Subtract 2 for <sos> and <eos> tokens\n",
    "    padding_length = max_length - len(sequence) + 1\n",
    "\n",
    "    # Processed sequence with <sos>, <eos>, and <pad>\n",
    "    processed = [\"<sos>\"] + sequence + [\"<eos>\"] + [\"<pad>\"] * padding_length\n",
    "\n",
    "    # Create a mask: 1s for actual tokens and 0s for padding\n",
    "    # The mask length is len(sequence) + 2 for <sos> and <eos> tokens. The rest are 0s for padding.\n",
    "    mask = [1] * (len(sequence) + 2) + [0] * padding_length\n",
    "\n",
    "    return processed, mask\n",
    "\n",
    "\n",
    "for filepath in files_list:\n",
    "    subject = open(data_dir + os.sep + filepath, \"rb\")\n",
    "    docs = yaml.safe_load(subject)\n",
    "    conversations = docs[\"conversations\"]\n",
    "    for con in conversations:\n",
    "        range_end = len(con) - (len(con) % 2)\n",
    "\n",
    "        for i in range(0, range_end, 2):\n",
    "            query = tokenize(con[i])\n",
    "            response = tokenize(con[i + 1])\n",
    "\n",
    "            if (\n",
    "                # all_words_in_vocab(query + response, vocab)\n",
    "                len(query) <= MAX_LENGTH\n",
    "                and len(response) <= MAX_LENGTH\n",
    "            ):\n",
    "                query, _ = process_sentence(query)\n",
    "                response, mask_r = process_sentence(response)\n",
    "\n",
    "                queries.append(vocab(query))\n",
    "                responses.append(vocab(response))\n",
    "                # masks_q.append(mask_q)\n",
    "                masks_r.append(mask_r)\n",
    "\n",
    "queries = np.asarray(queries)\n",
    "responses = np.asarray(responses)\n",
    "# masks_q = np.asarray(masks_q)\n",
    "masks_r = np.asarray(masks_r)\n",
    "\n",
    "print(f\"Number of queries/responses: {len(queries)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(queries[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:torchtext.vocab.vectors:Loading vectors from ./.vector_cache/glove.42B.300d.txt.pt\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# The dimensionality of GloVe embeddings\n",
    "embedding_dim = 300\n",
    "\n",
    "# Special tokens\n",
    "special_tokens = [\"<pad>\", \"<sos>\", \"<eos>\"]\n",
    "num_special_tokens = len(special_tokens)\n",
    "\n",
    "# Initialize a tensor to hold the embeddings for special tokens\n",
    "# Here, PAD is initialized to zeros, and SOS, EOS to random values\n",
    "special_embeddings = torch.zeros(num_special_tokens, embedding_dim)\n",
    "special_embeddings[1:] = (\n",
    "    torch.rand(num_special_tokens - 1, embedding_dim) * 0.01\n",
    ")  # Small random numbers for SOS and EOS\n",
    "\n",
    "from torchtext.vocab import GloVe\n",
    "\n",
    "# Load GloVe embeddings\n",
    "glove = GloVe(name=\"42B\", dim=embedding_dim, cache=\"./.vector_cache\")\n",
    "\n",
    "# Get GloVe embeddings for the vocabulary tokens\n",
    "# Assuming 'vocab' is a list of vocabulary tokens including special tokens at the beginning\n",
    "glove_embeddings = glove.get_vecs_by_tokens(vocab.get_itos(), lower_case_backup=True)\n",
    "\n",
    "# Concatenate the special token embeddings with the GloVe embeddings\n",
    "extended_embeddings = torch.cat([special_embeddings, glove_embeddings], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import deeplay as dl\n",
    "# import torch.nn as nn\n",
    "\n",
    "# features_dim = 500\n",
    "# vocab_size = len(vocab)\n",
    "\n",
    "# embedding = dl.Layer(nn.Embedding, vocab_size, embedding_dim)\n",
    "# embedding.weight = extended_embeddings\n",
    "# embedding.weight.requires_grad = False\n",
    "\n",
    "# encoder = dl.RecurrentModel(\n",
    "#     in_features=embedding_dim,\n",
    "#     hidden_features=[features_dim, features_dim],\n",
    "#     out_features=embedding_dim,\n",
    "#     dropout=0.1,\n",
    "#     rnn_type=\"GRU\",\n",
    "# )\n",
    "# encoder.layer[2].configure(nn.Identity)\n",
    "\n",
    "# decoder = dl.RecurrentModel(\n",
    "#     in_features=embedding_dim,\n",
    "#     hidden_features=[features_dim, features_dim],\n",
    "#     out_features=embedding_dim,\n",
    "#     dropout=0.1,\n",
    "#     rnn_type=\"GRU\",\n",
    "# )\n",
    "# decoder.blocks[2].activation.configure(nn.Softmax)\n",
    "# seq2seq = dl.Sequential(\n",
    "#     embedding,\n",
    "#     encoder,\n",
    "#     decoder,\n",
    "# )\n",
    "\n",
    "# reg = dl.Regressor(\n",
    "#     model=seq2seq,\n",
    "#     loss=nn.CrossEntropyLoss(),\n",
    "#     optimizer=dl.Adam(),\n",
    "# )\n",
    "# seq2seq_reg = reg.create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/841602/Documents/GitHub/Environments/deeplay_env/lib/python3.10/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import deeplay as dl\n",
    "from deeplay import DeeplayModule, Classifier\n",
    "\n",
    "hidden_features = 50\n",
    "\n",
    "\n",
    "class MyClassifier(Classifier):\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x1, x2, m = batch\n",
    "        y = torch.cat((x2[:, 1:], x2[:, -1:]), dim=1)\n",
    "        y_hat = self(x1, x2)\n",
    "        # print(y.shape)\n",
    "        # print(y_hat.shape)\n",
    "        # y = y.view(-1)\n",
    "        # y_hat = y_hat.view(-1, y_hat.size(-1))\n",
    "        loss = self.loss(y_hat, y, m)\n",
    "        # loss = self.loss(y_hat.view(-1, y_hat.size(-1)), y.view(-1))\n",
    "        self.log(\n",
    "            f\"train_loss\",\n",
    "            loss,\n",
    "            on_step=True,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "            logger=True,\n",
    "        )\n",
    "\n",
    "        self.log_metrics(\n",
    "            \"train\", y_hat, y, on_step=True, on_epoch=True, prog_bar=True, logger=True\n",
    "        )\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        return self.model(x1, x2)\n",
    "\n",
    "\n",
    "class Encoder(DeeplayModule):\n",
    "    def __init__(self, vocab_size, embedding_dim, lstm_units):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, lstm_units, batch_first=True, dropout=0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        # x = torch.nn.utils.rnn.pack_padded_sequence(x, lengths)\n",
    "        outputs, (hidden, cell) = self.lstm(x)\n",
    "        return hidden, cell\n",
    "\n",
    "\n",
    "class Decoder(DeeplayModule):\n",
    "    def __init__(self, vocab_size, embedding_dim, lstm_units):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, lstm_units, batch_first=True, dropout=0.1)\n",
    "        self.dense = nn.Linear(lstm_units, vocab_size)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x, hidden, cell):\n",
    "        x = self.embedding(x)\n",
    "        outputs, (hidden, cell) = self.lstm(x, (hidden, cell))\n",
    "        outputs = self.dense(outputs)\n",
    "        outputs = self.softmax(outputs)\n",
    "        return outputs, hidden, cell\n",
    "\n",
    "\n",
    "class Seq2Seq(DeeplayModule):\n",
    "    def __init__(self, vocab_size, embedding_dim, lstm_units):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = Encoder(vocab_size, embedding_dim, lstm_units)\n",
    "        self.decoder = Decoder(vocab_size, embedding_dim, lstm_units)\n",
    "        self.vocab_size = vocab_size\n",
    "        self.lstm_units = lstm_units\n",
    "\n",
    "    def forward(self, encoder_input_data, decoder_input_data):\n",
    "        encoder_hidden, encoder_cell = self.encoder(encoder_input_data)\n",
    "\n",
    "        decoder_hidden = encoder_hidden\n",
    "        decoder_cell = encoder_cell\n",
    "\n",
    "        outputs = torch.zeros(\n",
    "            (decoder_input_data.size(0), decoder_input_data.size(1), self.vocab_size)\n",
    "        ).to(\"mps\")\n",
    "        for t in range(decoder_input_data.size(1)):  # Iterate through the sequence\n",
    "            output, decoder_hidden, decoder_cell = self.decoder(\n",
    "                decoder_input_data[:, t].unsqueeze(-1), decoder_hidden, decoder_cell\n",
    "            )\n",
    "            outputs[:, t, :] = output.squeeze(1)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "seq2seq = Seq2Seq(len(vocab), embedding_dim, hidden_features)\n",
    "\n",
    "\n",
    "def NLLLoss(inp, target, mask):\n",
    "    # nTotal = mask.sum()\n",
    "    crossEntropy = -torch.log(\n",
    "        torch.gather(inp.view(-1, inp.shape[-1]), 1, target.view(-1, 1))\n",
    "    )\n",
    "    # loss = crossEntropy.mean()\n",
    "    loss = crossEntropy.masked_select(mask.view(-1, 1)).mean()\n",
    "    # loss = loss.to(device)\n",
    "    return loss  # , nTotal.item()\n",
    "\n",
    "\n",
    "seq2seq_classifier = MyClassifier(\n",
    "    model=seq2seq,\n",
    "    loss=NLLLoss,  # nn.CrossEntropyLoss(),\n",
    "    optimizer=dl.RMSprop(),\n",
    ").create()\n",
    "\n",
    "seq2seq_classifier.model.encoder.embedding.weight.data = extended_embeddings\n",
    "seq2seq_classifier.model.encoder.embedding.weight.requires_grad = False\n",
    "seq2seq_classifier.model.decoder.embedding.weight.data = extended_embeddings\n",
    "seq2seq_classifier.model.decoder.embedding.weight.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deeptrack as dt\n",
    "import torch\n",
    "\n",
    "sources = dt.sources.Source(inputs=queries, targets=responses, masks=masks_r)\n",
    "\n",
    "inputs_pl = dt.Value(sources.inputs) >> dt.pytorch.ToTensor(dtype=torch.int)\n",
    "targets_pl = dt.Value(sources.targets) >> dt.pytorch.ToTensor(dtype=torch.int)\n",
    "masks_pl = dt.Value(sources.masks) >> dt.pytorch.ToTensor(dtype=torch.bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataset = dt.pytorch.Dataset(inputs_pl & targets_pl & masks_pl, inputs=sources)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/841602/Documents/GitHub/Environments/deeplay_env/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:74: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
       "┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name          </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type             </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span>┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span>│ train_metrics │ MetricCollection │      0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 </span>│ val_metrics   │ MetricCollection │      0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2 </span>│ test_metrics  │ MetricCollection │      0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3 </span>│ model         │ Seq2Seq          │  657 K │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 4 </span>│ optimizer     │ RMSprop          │      0 │\n",
       "└───┴───────────────┴──────────────────┴────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
       "┃\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mName         \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mType            \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
       "│\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m│ train_metrics │ MetricCollection │      0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m1\u001b[0m\u001b[2m \u001b[0m│ val_metrics   │ MetricCollection │      0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m2\u001b[0m\u001b[2m \u001b[0m│ test_metrics  │ MetricCollection │      0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m3\u001b[0m\u001b[2m \u001b[0m│ model         │ Seq2Seq          │  657 K │\n",
       "│\u001b[2m \u001b[0m\u001b[2m4\u001b[0m\u001b[2m \u001b[0m│ optimizer     │ RMSprop          │      0 │\n",
       "└───┴───────────────┴──────────────────┴────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 181 K                                                                                            \n",
       "<span style=\"font-weight: bold\">Non-trainable params</span>: 476 K                                                                                        \n",
       "<span style=\"font-weight: bold\">Total params</span>: 657 K                                                                                                \n",
       "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 2                                                                          \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mTrainable params\u001b[0m: 181 K                                                                                            \n",
       "\u001b[1mNon-trainable params\u001b[0m: 476 K                                                                                        \n",
       "\u001b[1mTotal params\u001b[0m: 657 K                                                                                                \n",
       "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 2                                                                          \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d42483f7b4604ce8ae2a56b22d103261",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/841602/Documents/GitHub/Environments/deeplay_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/Users/841602/Documents/GitHub/Environments/deeplay_env/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (22) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/Users/841602/Documents/GitHub/Environments/deeplay_env/lib/python3.10/site-packages/lightning/pytorch/trainer/call\n",
       ".py:54: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/Users/841602/Documents/GitHub/Environments/deeplay_env/lib/python3.10/site-packages/lightning/pytorch/trainer/call\n",
       ".py:54: Detected KeyboardInterrupt, attempting graceful shutdown...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = dl.Trainer(max_epochs=100, accelerator=\"mps\")\n",
    "trainer.fit(seq2seq_classifier, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_inference(model, source_sequence):\n",
    "    # Source sequence tensor is assumed to be prepared accordingly (tokenized, converted to tensor, etc.)\n",
    "\n",
    "    # Encoder inference\n",
    "    with torch.no_grad():\n",
    "        hidden, cell = model.encoder(torch.tensor(source_sequence).to(\"mps\"))\n",
    "\n",
    "    # Prepare inputs to the decoder\n",
    "    target_index = torch.tensor([1]).to(\"mps\")\n",
    "\n",
    "    predictions = []\n",
    "    for _ in range(MAX_LENGTH):\n",
    "        with torch.no_grad():\n",
    "            output, hidden, cell = model.decoder(target_index, hidden, cell)\n",
    "            top1 = output.argmax(1)  # Get the index with the highest probability\n",
    "            predictions.append(top1.item())\n",
    "            target_index = top1\n",
    "            if top1.item() == 2:  # Check if the EOS token was generated\n",
    "                break\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6, 12, 7, 99, 330, 2]\n",
      "i\n",
      "are\n",
      "a\n",
      "bad\n",
      "spouse\n",
      "<eos>\n"
     ]
    }
   ],
   "source": [
    "qu = queries[100]\n",
    "# for q in qu:\n",
    "#     print(vocab.lookup_token(q))\n",
    "\n",
    "predictions = make_inference(seq2seq_classifier.model, qu)\n",
    "print(predictions)\n",
    "\n",
    "for p in predictions:\n",
    "    print(vocab.lookup_token(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i\n",
      "am\n",
      "not\n",
      "<unk>\n",
      ".\n",
      "<eos>\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Formatted Data File\n",
    "\n",
    "For ease of use, we will generate a well-structured data file where each line comprises a tab-separated pair of a *query sentence* and a *response sentence*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pairs = []\n",
    "\n",
    "# for conv_id in corpus.conversations:\n",
    "#     conv = corpus.get_conversation(conv_id)\n",
    "#     utt_ids = conv.get_utterance_ids()\n",
    "\n",
    "#     num_utt = len(utt_ids)\n",
    "#     range_end = num_utt - 2 if num_utt % 2 != 0 else num_utt - 1\n",
    "\n",
    "#     for i in range(range_end):\n",
    "#         query = corpus.get_utterance(utt_ids[i]).text.strip()\n",
    "#         response = corpus.get_utterance(utt_ids[i + 1]).text.strip()\n",
    "#         if query and response:\n",
    "#             pairs.append(f\"{query}|-->|{response}\")\n",
    "\n",
    "# print(f\"Number of pairs: {len(pairs)}\")\n",
    "\n",
    "# # filename = os.path.join(corpus_name, \"formatted_movie_lines.txt\")\n",
    "\n",
    "# # with open(filename, \"w\", encoding=\"utf-8\") as file:\n",
    "# #     for pair in pairs:\n",
    "# #         file.write(pair + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "convert all letters to lowercase \n",
    "\n",
    "trim all non-letter characters except for basic punctuation\n",
    "\n",
    "filter out sentences with length greater than the MAX_LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "\n",
    "\n",
    "# def clean_text(text):\n",
    "#     text = text.strip().lower()\n",
    "#     text = re.sub(r\"[^a-z0-9.!?]+\", r\" \", text)\n",
    "#     text = re.sub(r\"([.!?])\", r\" \\1 \", text)\n",
    "#     text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "#     return text\n",
    "\n",
    "\n",
    "# MAX_LENGTH = 30\n",
    "# proc_pairs = []\n",
    "\n",
    "# for pair in pairs:\n",
    "#     query, response = pair.strip().split(\"|-->|\")\n",
    "#     clean_query = clean_text(query)\n",
    "#     clean_response = clean_text(response)\n",
    "#     if len(clean_query) <= MAX_LENGTH and len(clean_response) <= MAX_LENGTH:\n",
    "#         proc_pairs.append(f\"{clean_query}|-->|{clean_response}\")\n",
    "\n",
    "# print(f\"Number of pairs: {len(proc_pairs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create a vocabulary\n",
    "\n",
    "Remove words below a certain count threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading and Trimming\n",
    "\n",
    "The next step involves creating a vocabulary and loading query/response sentence pairs into memory.\n",
    "\n",
    "Keep in mind that we are working with sequences of **words**, which do not inherently map to a discrete numerical space. Therefore, we need to create such a mapping by associating each unique word we encounter in our dataset with an index value.\n",
    "\n",
    "To achieve this, we define a `Vocabulary` class, which maintains a mapping from words to indexes, a reverse mapping from indexes to words, a count of each word, and a total word count. The class offers methods for adding a word to the vocabulary (`add_word`), adding all words in a sentence (`add_sentence`), and trimming infrequently seen words (`trim`). We will discuss trimming in more detail later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# sentences = [\n",
    "#     sentence.strip() for pair in proc_pairs for sentence in pair.strip().split(\"|-->|\")\n",
    "# ]\n",
    "\n",
    "# MIN_COUNT = 5\n",
    "\n",
    "# # Initialize CountVectorizer\n",
    "# vectorizer = CountVectorizer(\n",
    "#     min_df=MIN_COUNT, tokenizer=lambda txt: txt.strip().split(\" \")\n",
    "# )\n",
    "# X = vectorizer.fit_transform(sentences)\n",
    "\n",
    "# # Get the feature names to build a vocab dictionary\n",
    "# vocab = vectorizer.get_feature_names_out()\n",
    "\n",
    "# print(f\"Vocabulary size: {len(vocab)}\")\n",
    "# # print(f\"Encoded sentence example: {encoded_sentences[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep_pairs = []\n",
    "# for pair in proc_pairs:\n",
    "#     query, response = pair.strip().split(\"|-->|\")\n",
    "\n",
    "#     keep_query = True\n",
    "#     keep_response = True\n",
    "#     # Check input sentence\n",
    "#     for word in query.strip().split(\" \"):\n",
    "#         if word not in vocab:\n",
    "#             keep_query = False\n",
    "#             break\n",
    "#     # Check output sentence\n",
    "#     for word in response.strip().split(\" \"):\n",
    "#         if word not in vocab:\n",
    "#             keep_response = False\n",
    "#             break\n",
    "\n",
    "#     if keep_query and keep_response:\n",
    "#         keep_pairs.append(pair)\n",
    "# print(f\"Number of pairs: {len(keep_pairs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PAD, SOS,EOS token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Mapping words to indices (+3 offset for special tokens)\n",
    "# word_to_idx = {word: i + 3 for i, word in enumerate(vocab)}\n",
    "\n",
    "# # Adding special tokens to the dictionary\n",
    "# word_to_idx[\"<PAD>\"] = 0  # PAD\n",
    "# word_to_idx[\"<SOS>\"] = 1  # SOS\n",
    "# word_to_idx[\"<EOS>\"] = 2  # EOS\n",
    "\n",
    "\n",
    "# # Example of encoding a sentence with SOS, EOS, and converting to indices\n",
    "# def encode_sentence(sentence, word_to_idx, max_len=MAX_LENGTH):\n",
    "#     # Tokenize the sentence\n",
    "#     tokens = sentence.split()\n",
    "\n",
    "#     # Add SOS and EOS tokens\n",
    "#     tokens = [\"<SOS>\"] + tokens + [\"<EOS>\"]\n",
    "\n",
    "#     # Convert tokens to indices\n",
    "#     indices = [word_to_idx.get(token, word_to_idx[\"<EOS>\"]) for token in tokens]\n",
    "\n",
    "#     # Pad the sequence to max_len\n",
    "#     padded_sequence = indices + [word_to_idx[\"<PAD>\"]] * (max_len - len(indices))\n",
    "#     return padded_sequence[:max_len]\n",
    "\n",
    "\n",
    "# sentences = [\n",
    "#     sentence.strip() for pair in keep_pairs for sentence in pair.strip().split(\"|-->|\")\n",
    "# ]\n",
    "# # Example: encoding the first sentence\n",
    "# # max_len = max(len(s.split()) for s in sentences) + 2  # +2 for SOS and EOS tokens\n",
    "# encoded_sentences = [encode_sentence(s, word_to_idx) for s in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"Encoded sentence example: {encoded_sentences[10000]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idx_to_word = {idx: word for word, idx in word_to_idx.items()}\n",
    "\n",
    "\n",
    "# def decode_sentence(encoded_sentence, idx_to_word):\n",
    "#     # Convert indices back to tokens, ignoring special tokens for padding, start, and end\n",
    "#     tokens = [\n",
    "#         idx_to_word.get(idx)\n",
    "#         for idx in encoded_sentence\n",
    "#         if idx in idx_to_word and idx > 2\n",
    "#     ]\n",
    "\n",
    "#     # Join the tokens back into a single string\n",
    "#     sentence = \" \".join(tokens)\n",
    "#     return sentence\n",
    "\n",
    "\n",
    "# # Example: decoding the first encoded sentence\n",
    "# decoded_sentences = [\n",
    "#     decode_sentence(encoded, idx_to_word) for encoded in encoded_sentences\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"Decoded sentence example: {decoded_sentences[0:20]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now compile our vocabulary and query/response sentence pairs. However, before we can utilize this data, we need to carry out some preprocessing steps.\n",
    "\n",
    "Initially, we need to transform the Unicode strings into ASCII using `unicodeToAscii`. Subsequently, we should convert all characters to lowercase and remove all non-letter characters, excluding basic punctuation (`normalize_string`). Lastly, to facilitate training convergence, we will exclude sentences exceeding the `MAX_LENGTH` threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another tactic that is beneficial to achieving faster convergence during\n",
    "training is trimming rarely used words out of our vocabulary. Decreasing\n",
    "the feature space will also soften the difficulty of the function that\n",
    "the model must learn to approximate. We will do this as a two-step\n",
    "process:\n",
    "\n",
    "1) Trim words used under ``MIN_COUNT`` threshold using the ``voc.trim``\n",
    "   function.\n",
    "\n",
    "2) Filter out pairs with trimmed words.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation for Models\n",
    "\n",
    "Despite our extensive efforts to curate and process our data into a convenient vocabulary object and list of sentence pairs, our models will ultimately require numerical torch tensors as inputs.  \n",
    "\n",
    " To accommodate sentences of different sizes in the same batch, we will create our batched input tensor of shape (max_length, batch_size), where sentences shorter than the max_length are zero padded after an EOS_token.\n",
    "\n",
    "If we simply convert our English sentences to tensors by converting words to their indexes and zero-pad, our tensor would have shape (batch_size, max_length) and indexing the first dimension would return a full sequence across all time-steps. However, we need to be able to index our batch along time, and across all sequences in the batch. Therefore, we transpose our input batch shape to (max_length, batch_size), so that indexing across the first dimension returns a time step across all sentences in the batch.\n",
    "\n",
    "The output function palso returns a binary mask tensor and a maximum target sentence length. The binary mask tensor has the same shape as the output target tensor, but every element that is a PAD_token is 0 and all others are 1.\n",
    "\n",
    "`batch_to_train_data` simply takes a bunch of pairs and returns the input and target tensors using the aforementioned functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cny8pPMnIrQC"
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import torch\n",
    "import random\n",
    "\n",
    "# Assuming PAD_token and EOS_token are defined with their respective integral values\n",
    "\n",
    "\n",
    "def indexes_from_sentence(vocabulary, sentence):\n",
    "    \"\"\"\n",
    "    Convert sentence to a list of indexes, appending the EOS token at the end.\n",
    "    \"\"\"\n",
    "    return [vocabulary.word_to_index[word] for word in sentence.split(\" \")] + [\n",
    "        EOS_TOKEN\n",
    "    ]\n",
    "\n",
    "\n",
    "def binary_matrix(l, value=PAD_TOKEN):\n",
    "    \"\"\"\n",
    "    Create a binary matrix representing the padding of sentences.\n",
    "    \"\"\"\n",
    "    return [[0 if token == value else 1 for token in seq] for seq in l]\n",
    "\n",
    "\n",
    "def batch_to_train_data(vocabulary, pair_batch):\n",
    "    \"\"\"\n",
    "    Prepare the batch for training: sort by input length, create tensors for input/target variables.\n",
    "    \"\"\"\n",
    "    pair_batch.sort(key=lambda x: len(x[0].split(\" \")), reverse=True)\n",
    "    input_batch, output_batch = zip(*pair_batch)\n",
    "    input_indexes = [\n",
    "        indexes_from_sentence(vocabulary, sentence) for sentence in input_batch\n",
    "    ]\n",
    "    input_lengths = torch.tensor([len(indexes) for indexes in input_indexes])\n",
    "    input_padded = torch.LongTensor(\n",
    "        list(itertools.zip_longest(*input_indexes, fillvalue=PAD_TOKEN))\n",
    "    )\n",
    "\n",
    "    output_indexes = [\n",
    "        indexes_from_sentence(vocabulary, sentence) for sentence in output_batch\n",
    "    ]\n",
    "    output_padded = torch.LongTensor(\n",
    "        list(itertools.zip_longest(*output_indexes, fillvalue=PAD_TOKEN))\n",
    "    )\n",
    "    output_mask = torch.BoolTensor(binary_matrix(output_padded))\n",
    "    max_target_len = max(len(indexes) for indexes in output_indexes)\n",
    "\n",
    "    return input_padded, input_lengths, output_padded, output_mask, max_target_len\n",
    "\n",
    "\n",
    "# Example for validation\n",
    "small_batch_size = 5\n",
    "batches = batch_to_train_data(\n",
    "    voc, [random.choice(pairs) for _ in range(small_batch_size)]\n",
    ")\n",
    "input_variable, lengths, target_variable, mask, max_target_len = batches\n",
    "\n",
    "print(\"input_variable:\", input_variable)\n",
    "print(\"lengths:\", lengths)\n",
    "print(\"target_variable:\", target_variable)\n",
    "print(\"mask:\", mask)\n",
    "print(\"max_target_len:\", max_target_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Procedure Definition\n",
    "\n",
    "### Loss with Masking\n",
    "\n",
    "Given that we're working with batches of padded sequences, we can't compute loss using all tensor elements. We establish `mask_nll_loss` to compute our loss based on the decoder's output tensor, the target tensor, and a binary mask tensor that indicates the padding of the target tensor. This loss function computes the average negative log likelihood of the elements that align with a *1* in the mask tensor.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KSjDpLkxI8VX"
   },
   "outputs": [],
   "source": [
    "def mask_nll_loss(inp, target, mask, device):\n",
    "    \"\"\"\n",
    "    Calculate the negative log likelihood loss with a mask over the lengths of target sequences.\n",
    "    \"\"\"\n",
    "    n_total = mask.sum()\n",
    "    cross_entropy = -torch.log(torch.gather(inp, 1, target.view(-1, 1)).squeeze(1))\n",
    "    loss = cross_entropy.masked_select(mask).mean()\n",
    "    loss = loss.to(device)\n",
    "    return loss, n_total.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Training Iteration Procedure\n",
    "\n",
    "The `train` function encapsulates the process for a single training iteration (a single batch of inputs).\n",
    "\n",
    "We employ two strategies to aid convergence:\n",
    "\n",
    "-  **Teacher forcing**: At a probability determined by `teacher_forcing_ratio`, we use the current target word as the decoder’s next input instead of the decoder’s current guess. This helps in efficient training but can cause instability during inference. Hence, the `teacher_forcing_ratio` must be set carefully.\n",
    "\n",
    "-  **Gradient clipping**: This technique counters the \"exploding gradient\" problem by capping gradients to a maximum value, preventing them from growing exponentially and causing overflow or overshooting steep cost function cliffs.\n",
    "\n",
    "**Procedure:**\n",
    "\n",
    "   1) Pass the entire input batch through the encoder.\n",
    "   2) Initialize decoder inputs as SOS_token, and hidden state as the encoder's final hidden state.\n",
    "   3) Pass the input batch sequence through the decoder one time step at a time.\n",
    "   4) If teacher forcing: set next decoder input as the current target; else: set next decoder input as current decoder output.\n",
    "   5) Calculate and accumulate loss.\n",
    "   6) Perform backpropagation.\n",
    "   7) Clip gradients.\n",
    "   8) Update encoder and decoder model parameters.\n",
    "\n",
    "Note: PyTorch’s RNN modules (`RNN`, `LSTM`, `GRU`) can be used like any other non-recurrent layers by passing them the entire input sequence. We use the `GRU` layer like this in the `encoder`. However, you can also run these modules one time-step at a time, as we do for the `decoder` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NF5Kt7PBI9qq"
   },
   "outputs": [],
   "source": [
    "def train(\n",
    "    input_variable,\n",
    "    lengths,\n",
    "    target_variable,\n",
    "    mask,\n",
    "    max_target_len,\n",
    "    encoder,\n",
    "    decoder,\n",
    "    embedding,\n",
    "    encoder_optimizer,\n",
    "    decoder_optimizer,\n",
    "    batch_size,\n",
    "    clip,\n",
    "    max_length=MAX_LENGTH,\n",
    "):\n",
    "\n",
    "    # Zero gradients\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    # Set device options\n",
    "    input_variable = input_variable.to(device)\n",
    "    target_variable = target_variable.to(device)\n",
    "    mask = mask.to(device)\n",
    "    # Lengths for RNN packing should always be on the CPU\n",
    "    lengths = lengths.to(\"cpu\")\n",
    "\n",
    "    # Initialize variables\n",
    "    loss = 0\n",
    "    print_losses = []\n",
    "    n_totals = 0\n",
    "\n",
    "    # Forward pass through encoder\n",
    "    encoder_outputs, encoder_hidden = encoder(input_variable, lengths)\n",
    "\n",
    "    # Create initial decoder input (start with SOS tokens for each sentence)\n",
    "    decoder_input = torch.LongTensor([[SOS_TOKEN for _ in range(batch_size)]])\n",
    "    decoder_input = decoder_input.to(device)\n",
    "\n",
    "    # Set initial decoder hidden state to the encoder's final hidden state\n",
    "    decoder_hidden = encoder_hidden[: decoder.n_layers]\n",
    "\n",
    "    # Determine if we are using teacher forcing this iteration\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    # Forward batch of sequences through decoder one time step at a time\n",
    "    if use_teacher_forcing:\n",
    "        for t in range(max_target_len):\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "            # Teacher forcing: next input is current target\n",
    "            decoder_input = target_variable[t].view(1, -1)\n",
    "            # Calculate and accumulate loss\n",
    "            mask_loss, nTotal = mask_nll_loss(\n",
    "                decoder_output, target_variable[t], mask[t], device\n",
    "            )\n",
    "            loss += mask_loss\n",
    "            print_losses.append(mask_loss.item() * nTotal)\n",
    "            n_totals += nTotal\n",
    "    else:\n",
    "        for t in range(max_target_len):\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "            # No teacher forcing: next input is decoder's own current output\n",
    "            _, topi = decoder_output.topk(1)\n",
    "            decoder_input = torch.LongTensor([[topi[i][0] for i in range(batch_size)]])\n",
    "            decoder_input = decoder_input.to(device)\n",
    "            # Calculate and accumulate loss\n",
    "            mask_loss, nTotal = mask_nll_loss(\n",
    "                decoder_output, target_variable[t], mask[t], device\n",
    "            )\n",
    "            loss += mask_loss\n",
    "            print_losses.append(mask_loss.item() * nTotal)\n",
    "            n_totals += nTotal\n",
    "\n",
    "    # Perform backpropagation\n",
    "    loss.backward()\n",
    "\n",
    "    # Clip gradients: gradients are modified in place\n",
    "    _ = nn.utils.clip_grad_norm_(encoder.parameters(), clip)\n",
    "    _ = nn.utils.clip_grad_norm_(decoder.parameters(), clip)\n",
    "\n",
    "    # Adjust model weights\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return sum(print_losses) / n_totals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Iterations\n",
    "\n",
    "Now we can integrate the complete training procedure with the data. The `train_iters` function executes `n_iterations` of training using the provided models, optimizers, data, etc. Most of the complex work is handled by the `train` function.\n",
    "\n",
    "It's important to note that when we save our model, we store a tarball that includes the encoder and decoder `state_dicts` (parameters), the optimizers' `state_dicts`, the loss, the iteration, etc. Saving the model in this way provides maximum flexibility with the checkpoint. After loading a checkpoint, we can either use the model parameters to run inference or continue training from where we left off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S7BAQWthJAxf"
   },
   "outputs": [],
   "source": [
    "def train_iters(\n",
    "    model_name,\n",
    "    voc,\n",
    "    pairs,\n",
    "    encoder,\n",
    "    decoder,\n",
    "    encoder_optimizer,\n",
    "    decoder_optimizer,\n",
    "    embedding,\n",
    "    encoder_n_layers,\n",
    "    decoder_n_layers,\n",
    "    save_dir,\n",
    "    n_iteration,\n",
    "    batch_size,\n",
    "    print_every,\n",
    "    save_every,\n",
    "    clip,\n",
    "    corpus_name,\n",
    "):\n",
    "    \"\"\"\n",
    "    Run training for a set number of iterations.\n",
    "    \"\"\"\n",
    "    # Load batches for each iteration\n",
    "    training_batches = [\n",
    "        batch_to_train_data(voc, [random.choice(pairs) for _ in range(batch_size)])\n",
    "        for _ in range(n_iteration)\n",
    "    ]\n",
    "\n",
    "    print(\"Initializing ...\")\n",
    "    start_iteration = 1\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "\n",
    "    print(\"Training...\")\n",
    "    for iteration in range(start_iteration, n_iteration + 1):\n",
    "        training_batch = training_batches[iteration - 1]\n",
    "        input_variable, lengths, target_variable, mask, max_target_len = training_batch\n",
    "\n",
    "        # Run a training iteration with batch\n",
    "        loss = train(\n",
    "            input_variable,\n",
    "            lengths,\n",
    "            target_variable,\n",
    "            mask,\n",
    "            max_target_len,\n",
    "            encoder,\n",
    "            decoder,\n",
    "            embedding,\n",
    "            encoder_optimizer,\n",
    "            decoder_optimizer,\n",
    "            batch_size,\n",
    "            clip,\n",
    "        )\n",
    "        print_loss_total += loss\n",
    "\n",
    "        # Print progress\n",
    "        if iteration % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print(\n",
    "                f\"Iteration: {iteration}; Percent complete: {iteration / n_iteration * 100:.1f}%; Average loss: {print_loss_avg:.4f}\"\n",
    "            )\n",
    "            print_loss_total = 0\n",
    "\n",
    "        # Save checkpoint\n",
    "        if iteration % save_every == 0:\n",
    "            directory = os.path.join(\n",
    "                save_dir,\n",
    "                model_name,\n",
    "                corpus_name,\n",
    "                f\"{encoder_n_layers}-{decoder_n_layers}_{hidden_features}\",\n",
    "            )\n",
    "            if not os.path.exists(directory):\n",
    "                os.makedirs(directory)\n",
    "            torch.save(\n",
    "                {\n",
    "                    \"iteration\": iteration,\n",
    "                    \"en\": encoder.state_dict(),\n",
    "                    \"de\": decoder.state_dict(),\n",
    "                    \"en_opt\": encoder_optimizer.state_dict(),\n",
    "                    \"de_opt\": decoder_optimizer.state_dict(),\n",
    "                    \"loss\": loss,\n",
    "                    \"voc_dict\": voc.__dict__,\n",
    "                    \"embedding\": embedding.state_dict(),\n",
    "                },\n",
    "                os.path.join(directory, f\"{iteration}_checkpoint.tar\"),\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "After training the model, we want to interact with the bot. We need to define how the model decodes the encoded input.\n",
    "\n",
    "### Greedy Decoding\n",
    "\n",
    "Greedy decoding is used during training when we're not using teacher forcing. At each time step, we choose the word from `decoder_output` with the highest softmax value. This method is optimal at a single time-step level.\n",
    "\n",
    "We define a `GreedySearchDecoder` class to perform greedy decoding. The input sentence is evaluated as follows:\n",
    "\n",
    "**Computation Steps:**\n",
    "\n",
    "   1) Pass input through the encoder model.\n",
    "   2) Prepare the encoder's final hidden layer to be the first hidden input to the decoder.\n",
    "   3) Initialize the decoder's first input as SOS_token.\n",
    "   4) Initialize tensors to append decoded words to.\n",
    "   5) Iteratively decode one word token at a time:\n",
    "       a) Pass through the decoder.\n",
    "       b) Obtain the most likely word token and its softmax score.\n",
    "       c) Record the token and score.\n",
    "       d) Prepare the current token to be the next decoder input.\n",
    "   6) Return collections of word tokens and scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eGzjE-ggJCcZ"
   },
   "outputs": [],
   "source": [
    "class GreedySearchDecoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        \"\"\"\n",
    "        Greedy decoding module initialization.\n",
    "        \"\"\"\n",
    "        super(GreedySearchDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, input_seq, input_length, max_length):\n",
    "        \"\"\"\n",
    "        Forward propagation of the input to produce a sequence of tokens.\n",
    "        \"\"\"\n",
    "        # Forward input through encoder model\n",
    "        encoder_outputs, encoder_hidden = self.encoder(input_seq, input_length)\n",
    "\n",
    "        # Prepare encoder's final hidden layer to be first hidden input to the decoder\n",
    "        decoder_hidden = encoder_hidden[: self.decoder.n_layers]\n",
    "\n",
    "        # Initialize decoder input with SOS_token\n",
    "        decoder_input = torch.tensor([[SOS_TOKEN]], device=device, dtype=torch.long)\n",
    "\n",
    "        # Initialize tensors to append decoded words to\n",
    "        all_tokens = torch.zeros(0, dtype=torch.long, device=device)\n",
    "        all_scores = torch.zeros(0, device=device)\n",
    "\n",
    "        # Iteratively decode one word token at a time\n",
    "        for _ in range(max_length):\n",
    "            # Forward pass through decoder\n",
    "            decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden)\n",
    "\n",
    "            # Obtain most likely word token and its softmax score\n",
    "            decoder_scores, decoder_input = torch.max(decoder_output, dim=1)\n",
    "\n",
    "            # Record token and score\n",
    "            all_tokens = torch.cat((all_tokens, decoder_input), dim=0)\n",
    "            all_scores = torch.cat((all_scores, decoder_scores), dim=0)\n",
    "\n",
    "            # Prepare current token to be next decoder input (add a dimension)\n",
    "            decoder_input = torch.unsqueeze(decoder_input, 0)\n",
    "\n",
    "        # Return collections of word tokens and scores\n",
    "        return all_tokens, all_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Evaluation\n",
    "\n",
    "With our decoding method defined, we can create functions to evaluate a string input sentence. The `evaluate` function handles the input sentence, it formats the sentence as an input batch of word indexes with *batch_size==1*. This is done by converting the sentence words to their corresponding indexes and transposing the dimensions to prepare the tensor for our models. A `lengths` tensor is also created which contains the length of our input sentence. The decoded response sentence tensor is obtained using our `GreedySearchDecoder` object (`searcher`). Finally, the response’s indexes are converted to words and the list of decoded words is returned.\n",
    "\n",
    "`evaluate_input` serves as the user interface for our chatbot. It prompts an input text field where we can enter our query sentence. After entering our input sentence and pressing *Enter*, our text is normalized like our training data, and is fed to the `evaluate` function to obtain a decoded output sentence. This process is looped for continuous interaction with our bot until we enter either “q” or “quit”.\n",
    "\n",
    "If a sentence is entered that contains a word not in the vocabulary, an error message is printed and the user is prompted to enter another sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7LY0AotNJEzk"
   },
   "outputs": [],
   "source": [
    "def evaluate(searcher, voc, sentence, max_length=MAX_LENGTH):\n",
    "    \"\"\"\n",
    "    Evaluate a sentence using the encoder, decoder, and searcher provided.\n",
    "    \"\"\"\n",
    "    # Prepare the input sentence as a batch of word indexes\n",
    "    indexes_batch = [indexes_from_sentence(voc, sentence)]\n",
    "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
    "    input_batch = torch.LongTensor(indexes_batch).transpose(0, 1).to(device)\n",
    "    lengths = lengths.to(\"cpu\")  # Lengths need to be on CPU for pack_padded_sequence\n",
    "\n",
    "    # Decode the sentence with the searcher\n",
    "    tokens, scores = searcher(input_batch, lengths, max_length)\n",
    "    decoded_words = [voc.index_to_word[token.item()] for token in tokens]\n",
    "\n",
    "    return decoded_words\n",
    "\n",
    "\n",
    "def evaluate_input(encoder, decoder, searcher, voc):\n",
    "    \"\"\"\n",
    "    Interactively evaluate input from the user.\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        try:\n",
    "            input_sentence = input(\"> \")\n",
    "            if input_sentence in (\"q\", \"quit\"):\n",
    "                break\n",
    "            print(input_sentence)\n",
    "            input_sentence = normalize_string(input_sentence)\n",
    "            output_words = evaluate(searcher, voc, input_sentence)\n",
    "            output_words = [word for word in output_words if word not in (\"EOS\", \"PAD\")]\n",
    "            print(\"Bot:\", \" \".join(output_words))\n",
    "\n",
    "        except KeyError:\n",
    "            print(\"Error: Encountered unknown word.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Models Overview\n",
    "\n",
    "### Seq2Seq Model\n",
    "\n",
    "Our chatbot uses a sequence-to-sequence (seq2seq) model, which takes a variable-length sequence as input and returns a variable-length sequence as output. This is achieved by using two separate recurrent neural nets (RNNs): an **encoder** and a **decoder**. The encoder encodes the input sequence into a fixed-length context vector, which theoretically contains semantic information about the input sentence. The decoder takes an input word and the context vector, and returns a guess for the next word in the sequence and a hidden state for the next iteration.\n",
    "\n",
    "### Encoder\n",
    "\n",
    "The encoder RNN iterates through the input sentence one token at a time, outputting an \"output\" vector and a \"hidden state\" vector at each time step. The hidden state vector is passed to the next time step, while the output vector is recorded. The encoder uses a multi-layered Gated Recurrent Unit (GRU) and a bidirectional variant of the GRU. An `embedding` layer is used to encode our word indices in an arbitrarily sized feature space. \n",
    "\n",
    "### Decoder\n",
    "\n",
    "The decoder RNN generates the response sentence in a token-by-token fashion. It uses the encoder’s context vectors, and internal hidden states to generate the next word in the sequence. To avoid information loss, especially when dealing with long input sequences, an \"attention mechanism\" is typically used that allows the decoder to pay attention to certain parts of the input sequence, rather than using the entire fixed context at every step. However, in this tutorial, we only consider the use of standard RNNs, which will limit the performance of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set configuration parameters for the model\n",
    "model_name = \"cb_model\"\n",
    "hidden_features = 500\n",
    "in_features = hidden_features\n",
    "encoder_n_layers = 2\n",
    "decoder_n_layers = 2\n",
    "rnn_type = \"GRU\"\n",
    "dropout = 0.1\n",
    "batch_size = 64\n",
    "\n",
    "# Set training and optimization parameters\n",
    "clip = 50.0\n",
    "teacher_forcing_ratio = 1.0\n",
    "learning_rate = 0.0001\n",
    "decoder_learning_ratio = 5.0\n",
    "n_iteration = 4000\n",
    "print_every = 1\n",
    "save_every = 500\n",
    "\n",
    "embedding = nn.Embedding(voc.num_words, hidden_features)\n",
    "# Initialize the EncoderRNN\n",
    "encoder = dl.EncoderRNN(\n",
    "    in_features=in_features,\n",
    "    hidden_features=hidden_features,\n",
    "    num_layers=encoder_n_layers,\n",
    "    embedding=embedding,\n",
    "    rnn_type=rnn_type,\n",
    "    dropout=dropout,\n",
    ").to(device)\n",
    "\n",
    "\n",
    "encoder.build()\n",
    "print(encoder)\n",
    "\n",
    "decoder = dl.DecoderRNN(\n",
    "    in_features=in_features,\n",
    "    hidden_features=hidden_features,\n",
    "    out_features=voc.num_words,\n",
    "    num_layers=decoder_n_layers,\n",
    "    embedding=embedding,\n",
    "    rnn_type=rnn_type,\n",
    "    dropout=dropout,\n",
    ").to(device)\n",
    "\n",
    "\n",
    "encoder.build()\n",
    "print(decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define and train model\n",
    "\n",
    "Now, we're ready to run our model!\n",
    "\n",
    "Whether we're training or testing the chatbot model, we need to initialize the encoder and decoder models. In the next block, we configure the parameters, and construct and initialize the models. You're encouraged to experiment with different model configurations to enhance performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize word embeddings and encoder & decoder models\n",
    "# Set models to training mode\n",
    "encoder.train()\n",
    "decoder.train()\n",
    "\n",
    "# Initialize optimizers for the encoder and decoder\n",
    "encoder_optimizer = torch.optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = torch.optim.Adam(\n",
    "    decoder.parameters(), lr=learning_rate * decoder_learning_ratio\n",
    ")\n",
    "\n",
    "# Move optimizer states to GPU if necessary\n",
    "if torch.cuda.is_available():\n",
    "    for state in encoder_optimizer.state.values():\n",
    "        for k, v in state.items():\n",
    "            if isinstance(v, torch.Tensor):\n",
    "                state[k] = v.cuda()\n",
    "\n",
    "    for state in decoder_optimizer.state.values():\n",
    "        for k, v in state.items():\n",
    "            if isinstance(v, torch.Tensor):\n",
    "                state[k] = v.cuda()\n",
    "\n",
    "# Begin the training process\n",
    "train_iters(\n",
    "    model_name,\n",
    "    voc,\n",
    "    pairs,\n",
    "    encoder,\n",
    "    decoder,\n",
    "    encoder_optimizer,\n",
    "    decoder_optimizer,\n",
    "    embedding,\n",
    "    encoder_n_layers,\n",
    "    decoder_n_layers,\n",
    "    save_dir,\n",
    "    n_iteration,\n",
    "    batch_size,\n",
    "    print_every,\n",
    "    save_every,\n",
    "    clip,\n",
    "    corpus_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1OSKoKwg87mR"
   },
   "outputs": [],
   "source": [
    "# Set dropout layers to ``eval`` mode\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "\n",
    "# Initialize search module\n",
    "searcher = GreedySearchDecoder(encoder, decoder)\n",
    "\n",
    "# Begin chatting (uncomment and run the following line to begin)\n",
    "evaluate_input(encoder, decoder, searcher, voc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's review the changes made to incorporate conversation history into your chatbot model training and evaluation:\n",
    "### Changes for Training with Conversation History\n",
    "\n",
    "create_history_pairs Function: This function creates training pairs that include the history of the conversation. For each exchange in the pairs, it appends a certain number of previous exchanges (up to MAX_HISTORY) separated by the <EOS> token. These pairs are then used for training.\n",
    "\n",
    "Batch Preparation (batch_to_train_data): The function for preparing a batch of training data remains largely the same. The only difference is that it now handles input sequences that include conversation history.\n",
    "\n",
    "Training Function (train):\n",
    "    The core training logic remains unchanged.\n",
    "    The function receives input sequences that now contain conversation history.\n",
    "    The forward pass through the encoder and the decoding steps are performed as usual, without any specific changes needed to accommodate the conversation history.\n",
    "\n",
    "Data Preparation for Training:\n",
    "    The history_pairs are generated using the create_history_pairs function.\n",
    "    These pairs are then used throughout the training process.\n",
    "\n",
    "### Changes for Evaluation with Conversation History\n",
    "\n",
    "evaluate Function:\n",
    "    The conversation history is now considered when evaluating a new input.\n",
    "    The history is concatenated with the current input sentence, separated by spaces.\n",
    "    The concatenated string is then processed and fed into the model for generating a response.\n",
    "\n",
    "evaluate_input Function:\n",
    "    Manages interactive evaluation with the user.\n",
    "    Maintains a conversation_history list, appending each user input and the model's response to it.\n",
    "    Passes the accumulated conversation history to the evaluate function for each new input.\n",
    "\n",
    "### Training and Evaluation Process\n",
    "\n",
    "The model is trained with input sequences that include conversation history, allowing it to learn the context of the conversation.\n",
    "During evaluation, the model uses the accumulated conversation history to generate more context-aware responses.\n",
    "\n",
    "### General Setup\n",
    "\n",
    "Model, optimizer, and training configurations are set according to your specifications.\n",
    "The training process (train_iters) follows the standard approach but uses the modified input pairs with conversation history.\n",
    "\n",
    "### Important Notes\n",
    "\n",
    "The effectiveness of including conversation history in the model depends on the depth of context the model can understand and how well it can handle longer input sequences.\n",
    "Fine-tuning and experimenting with the MAX_HISTORY parameter and the MAX_LENGTH of sequences may be necessary to achieve optimal performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9BnEtHHxUHC7"
   },
   "outputs": [],
   "source": [
    "MAX_LENGTH = 10  # Maximum length of a single sentence, adjust as necessary\n",
    "MAX_HISTORY = 5  # Number of previous exchanges to include in the history\n",
    "\n",
    "\n",
    "def create_history_pairs(pairs, max_history=MAX_HISTORY):\n",
    "    history_pairs = []\n",
    "    for i in range(len(pairs)):\n",
    "        dialogue_history = \" EOS \".join(\n",
    "            [pairs[j][0] for j in range(max(0, i - max_history), i)]\n",
    "        )\n",
    "        history_pairs.append([dialogue_history, pairs[i][1]])\n",
    "    return history_pairs\n",
    "\n",
    "\n",
    "def zero_padding(l, fillvalue=0):\n",
    "    return list(itertools.zip_longest(*l, fillvalue=fillvalue))\n",
    "\n",
    "\n",
    "def binary_matrix(l, value=0):\n",
    "    m = []\n",
    "    for i, seq in enumerate(l):\n",
    "        m.append([])\n",
    "        for token in seq:\n",
    "            if token == value:\n",
    "                m[i].append(0)\n",
    "            else:\n",
    "                m[i].append(1)\n",
    "    return m\n",
    "\n",
    "\n",
    "def output_var(l, voc):\n",
    "    indexes_batch = [indexes_from_sentence(voc, sentence) for sentence in l]\n",
    "    max_target_len = max([len(indexes) for indexes in indexes_batch])\n",
    "    pad_list = zero_padding(indexes_batch)\n",
    "    mask = binary_matrix(pad_list)\n",
    "    mask = torch.BoolTensor(mask)\n",
    "    pad_var = torch.LongTensor(pad_list)\n",
    "    return pad_var, mask, max_target_len\n",
    "\n",
    "\n",
    "def indexes_from_sentence(voc, sentence):\n",
    "    # return [voc.word_to_index[word] for word in sentence.split(' ')] + [EOS_TOKEN]\n",
    "    return [voc.word_to_index[word] for word in sentence.split()] + [EOS_TOKEN]\n",
    "\n",
    "\n",
    "def batch_to_train_data(voc, pair_batch):\n",
    "    pair_batch.sort(key=lambda p: len(p[0].split()), reverse=True)\n",
    "    input_batch, output_batch = [], []\n",
    "    for pair in pair_batch:\n",
    "        input_batch.append(pair[0])\n",
    "        output_batch.append(pair[1])\n",
    "    inp, lengths = input_var(input_batch, voc)\n",
    "    output, mask, max_target_len = output_var(output_batch, voc)\n",
    "    return inp, lengths, output, mask, max_target_len\n",
    "\n",
    "\n",
    "# Input_var function needs to calculate the correct lengths for packed sequences\n",
    "def input_var(l, voc):\n",
    "    indexes_batch = [indexes_from_sentence(voc, sentence) for sentence in l]\n",
    "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
    "    pad_list = zero_padding(indexes_batch)\n",
    "    pad_var = torch.LongTensor(pad_list)\n",
    "    return pad_var, lengths\n",
    "\n",
    "\n",
    "# Prepare the data with history\n",
    "history_pairs = create_history_pairs(pairs)\n",
    "\n",
    "\n",
    "# Modify the training function to handle the dialogue history in the input sequences\n",
    "def train(\n",
    "    input_variable,\n",
    "    lengths,\n",
    "    target_variable,\n",
    "    mask,\n",
    "    max_target_len,\n",
    "    encoder,\n",
    "    decoder,\n",
    "    embedding,\n",
    "    encoder_optimizer,\n",
    "    decoder_optimizer,\n",
    "    batch_size,\n",
    "    clip,\n",
    "    max_length=MAX_LENGTH,\n",
    "):\n",
    "\n",
    "    # Zero gradients\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    # Set device options\n",
    "    input_variable = input_variable.to(device)\n",
    "    target_variable = target_variable.to(device)\n",
    "    mask = mask.to(device)\n",
    "    # Lengths for RNN packing should always be on the CPU\n",
    "    lengths = lengths.to(\"cpu\")\n",
    "\n",
    "    # Initialize variables\n",
    "    loss = 0\n",
    "    print_losses = []\n",
    "    n_totals = 0\n",
    "\n",
    "    # Forward pass through encoder\n",
    "    encoder_outputs, encoder_hidden = encoder(input_variable, lengths)\n",
    "\n",
    "    # Create initial decoder input (start with SOS tokens for each sentence)\n",
    "    decoder_input = torch.LongTensor([[SOS_TOKEN for _ in range(batch_size)]])\n",
    "    decoder_input = decoder_input.to(device)\n",
    "\n",
    "    # Set initial decoder hidden state to the encoder's final hidden state\n",
    "    decoder_hidden = encoder_hidden[: decoder.n_layers]\n",
    "\n",
    "    # Determine if we are using teacher forcing this iteration\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    # Forward batch of sequences through decoder one time step at a time\n",
    "    if use_teacher_forcing:\n",
    "        for t in range(max_target_len):\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "            # Teacher forcing: next input is current target\n",
    "            decoder_input = target_variable[t].view(1, -1)\n",
    "            # Calculate and accumulate loss\n",
    "            mask_loss, nTotal = mask_nll_loss(\n",
    "                decoder_output, target_variable[t], mask[t], device\n",
    "            )\n",
    "            loss += mask_loss\n",
    "            print_losses.append(mask_loss.item() * nTotal)\n",
    "            n_totals += nTotal\n",
    "    else:\n",
    "        for t in range(max_target_len):\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "            # No teacher forcing: next input is decoder's own current output\n",
    "            _, topi = decoder_output.topk(1)\n",
    "            decoder_input = torch.LongTensor([[topi[i][0] for i in range(batch_size)]])\n",
    "            decoder_input = decoder_input.to(device)\n",
    "            # Calculate and accumulate loss\n",
    "            mask_loss, nTotal = mask_nll_loss(\n",
    "                decoder_output, target_variable[t], mask[t], device\n",
    "            )\n",
    "            loss += mask_loss\n",
    "            print_losses.append(mask_loss.item() * nTotal)\n",
    "            n_totals += nTotal\n",
    "\n",
    "    # Perform backpropagation\n",
    "    loss.backward()\n",
    "\n",
    "    # Clip gradients: gradients are modified in place\n",
    "    _ = nn.utils.clip_grad_norm_(encoder.parameters(), clip)\n",
    "    _ = nn.utils.clip_grad_norm_(decoder.parameters(), clip)\n",
    "\n",
    "    # Adjust model weights\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return sum(print_losses) / n_totals\n",
    "\n",
    "\n",
    "def train_iters(\n",
    "    model_name,\n",
    "    voc,\n",
    "    history_pairs,\n",
    "    encoder,\n",
    "    decoder,\n",
    "    encoder_optimizer,\n",
    "    decoder_optimizer,\n",
    "    embedding,\n",
    "    encoder_n_layers,\n",
    "    decoder_n_layers,\n",
    "    save_dir,\n",
    "    n_iteration,\n",
    "    batch_size,\n",
    "    print_every,\n",
    "    save_every,\n",
    "    clip,\n",
    "    corpus_name,\n",
    "):\n",
    "    \"\"\"\n",
    "    Run training for a set number of iterations.\n",
    "    \"\"\"\n",
    "    # Load batches for each iteration\n",
    "    training_batches = [\n",
    "        batch_to_train_data(\n",
    "            voc, [random.choice(history_pairs) for _ in range(batch_size)]\n",
    "        )\n",
    "        for _ in range(n_iteration)\n",
    "    ]\n",
    "\n",
    "    print(\"Initializing ...\")\n",
    "    start_iteration = 1\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "\n",
    "    print(\"Training...\")\n",
    "    for iteration in range(start_iteration, n_iteration + 1):\n",
    "        training_batch = training_batches[iteration - 1]\n",
    "        input_variable, lengths, target_variable, mask, max_target_len = training_batch\n",
    "\n",
    "        # Run a training iteration with batch\n",
    "        loss = train(\n",
    "            input_variable,\n",
    "            lengths,\n",
    "            target_variable,\n",
    "            mask,\n",
    "            max_target_len,\n",
    "            encoder,\n",
    "            decoder,\n",
    "            embedding,\n",
    "            encoder_optimizer,\n",
    "            decoder_optimizer,\n",
    "            batch_size,\n",
    "            clip,\n",
    "        )\n",
    "        print_loss_total += loss\n",
    "\n",
    "        # Print progress\n",
    "        if iteration % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print(\n",
    "                f\"Iteration: {iteration}; Percent complete: {iteration / n_iteration * 100:.1f}%; Average loss: {print_loss_avg:.4f}\"\n",
    "            )\n",
    "            print_loss_total = 0\n",
    "\n",
    "        # Save checkpoint\n",
    "        if iteration % save_every == 0:\n",
    "            directory = os.path.join(\n",
    "                save_dir,\n",
    "                model_name,\n",
    "                corpus_name,\n",
    "                f\"{encoder_n_layers}-{decoder_n_layers}_{hidden_features}\",\n",
    "            )\n",
    "            if not os.path.exists(directory):\n",
    "                os.makedirs(directory)\n",
    "            torch.save(\n",
    "                {\n",
    "                    \"iteration\": iteration,\n",
    "                    \"en\": encoder.state_dict(),\n",
    "                    \"de\": decoder.state_dict(),\n",
    "                    \"en_opt\": encoder_optimizer.state_dict(),\n",
    "                    \"de_opt\": decoder_optimizer.state_dict(),\n",
    "                    \"loss\": loss,\n",
    "                    \"voc_dict\": voc.__dict__,\n",
    "                    \"embedding\": embedding.state_dict(),\n",
    "                },\n",
    "                os.path.join(directory, f\"{iteration}_checkpoint.tar\"),\n",
    "            )\n",
    "\n",
    "\n",
    "def evaluate(searcher, voc, conversation_history, max_length=MAX_LENGTH):\n",
    "    \"\"\"\n",
    "    Evaluate a conversation history using the encoder, decoder, and searcher provided.\n",
    "    \"\"\"\n",
    "    # Join the conversation history into a single input and normalize\n",
    "    input_sentence = \" \".join(conversation_history)\n",
    "    input_sentence = normalize_string(input_sentence)\n",
    "\n",
    "    # Prepare the input sentence as a batch of word indexes\n",
    "    indexes_batch = [indexes_from_sentence(voc, input_sentence)]\n",
    "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
    "    input_batch = torch.LongTensor(indexes_batch).transpose(0, 1).to(device)\n",
    "    lengths = lengths.to(\"cpu\")  # Lengths need to be on CPU for pack_padded_sequence\n",
    "\n",
    "    # Decode the sentence with the searcher\n",
    "    tokens, scores = searcher(input_batch, lengths, max_length)\n",
    "    decoded_words = [voc.index_to_word[token.item()] for token in tokens]\n",
    "\n",
    "    return decoded_words\n",
    "\n",
    "\n",
    "def evaluate_input(searcher, voc):\n",
    "    \"\"\"\n",
    "    Interactively evaluate input from the user, considering the entire conversation history.\n",
    "    \"\"\"\n",
    "    conversation_history = []\n",
    "    while True:\n",
    "        try:\n",
    "            input_sentence = input(\"> \")\n",
    "            if input_sentence in (\"q\", \"quit\"):\n",
    "                break\n",
    "\n",
    "            # Normalize and add the user's input to the conversation history\n",
    "            input_sentence = normalize_string(input_sentence)\n",
    "            conversation_history.append(input_sentence)\n",
    "\n",
    "            # Evaluate the conversation history\n",
    "            output_words = evaluate(searcher, voc, conversation_history)\n",
    "            output_words = [word for word in output_words if word not in (\"EOS\", \"PAD\")]\n",
    "            print(\"Bot:\", \" \".join(output_words))\n",
    "\n",
    "            # Add the bot's response to the conversation history\n",
    "            conversation_history.extend(output_words)\n",
    "            conversation_history = conversation_history[-MAX_HISTORY * MAX_LENGTH :]\n",
    "\n",
    "        except KeyError:\n",
    "            print(\"Error: Encountered unknown word.\")\n",
    "\n",
    "\n",
    "# Set configuration parameters for the model\n",
    "model_name = \"cb_model\"\n",
    "hidden_features = 2000\n",
    "in_features = hidden_features\n",
    "encoder_n_layers = 4\n",
    "decoder_n_layers = 4\n",
    "dropout = 0.1\n",
    "batch_size = 64\n",
    "\n",
    "# Set training and optimization parameters\n",
    "clip = 50.0\n",
    "teacher_forcing_ratio = 1.0\n",
    "learning_rate = 0.0001\n",
    "decoder_learning_ratio = 5.0\n",
    "n_iteration = 16000\n",
    "print_every = 1\n",
    "save_every = 8000\n",
    "\n",
    "# Initialize word embeddings and encoder & decoder models\n",
    "embedding = nn.Embedding(voc.num_words, hidden_features)\n",
    "# Initialize the EncoderRNN\n",
    "encoder = dl.EncoderRNN(\n",
    "    in_features=in_features,\n",
    "    hidden_features=hidden_features,\n",
    "    num_layers=encoder_n_layers,\n",
    "    embedding=embedding,\n",
    "    rnn_type=rnn_type,\n",
    "    dropout=dropout,\n",
    ").to(device)\n",
    "\n",
    "decoder = dl.DecoderRNN(\n",
    "    in_features=in_features,\n",
    "    hidden_features=hidden_features,\n",
    "    out_features=voc.num_words,\n",
    "    num_layers=decoder_n_layers,\n",
    "    embedding=embedding,\n",
    "    rnn_type=rnn_type,\n",
    "    dropout=dropout,\n",
    ").to(device)\n",
    "\n",
    "encoder.build()\n",
    "decoder.build()\n",
    "# Set models to training mode\n",
    "encoder.train()\n",
    "decoder.train()\n",
    "\n",
    "# Initialize optimizers for the encoder and decoder\n",
    "encoder_optimizer = torch.optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = torch.optim.Adam(\n",
    "    decoder.parameters(), lr=learning_rate * decoder_learning_ratio\n",
    ")\n",
    "\n",
    "# Move optimizer states to GPU if necessary\n",
    "if torch.cuda.is_available():\n",
    "    for state in encoder_optimizer.state.values():\n",
    "        for k, v in state.items():\n",
    "            if isinstance(v, torch.Tensor):\n",
    "                state[k] = v.cuda()\n",
    "\n",
    "    for state in decoder_optimizer.state.values():\n",
    "        for k, v in state.items():\n",
    "            if isinstance(v, torch.Tensor):\n",
    "                state[k] = v.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Begin the training process\n",
    "train_iters(\n",
    "    model_name,\n",
    "    voc,\n",
    "    history_pairs,\n",
    "    encoder,\n",
    "    decoder,\n",
    "    encoder_optimizer,\n",
    "    decoder_optimizer,\n",
    "    embedding,\n",
    "    encoder_n_layers,\n",
    "    decoder_n_layers,\n",
    "    save_dir,\n",
    "    n_iteration,\n",
    "    batch_size,\n",
    "    print_every,\n",
    "    save_every,\n",
    "    clip,\n",
    "    corpus_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XRpMW5HFdtdo"
   },
   "outputs": [],
   "source": [
    "def evaluate(searcher, voc, conversation_history, max_length=MAX_LENGTH):\n",
    "    \"\"\"\n",
    "    Evaluate a conversation history using the encoder, decoder, and searcher provided.\n",
    "    \"\"\"\n",
    "    # Join the conversation history into a single input and normalize\n",
    "    input_sentence = \" \".join(conversation_history)\n",
    "    input_sentence = normalize_string(input_sentence)\n",
    "    input_sentence = input_sentence.replace(\"eos\", \"EOS\")\n",
    "\n",
    "    # Prepare the input sentence as a batch of word indexes\n",
    "    indexes_batch = [indexes_from_sentence(voc, input_sentence)]\n",
    "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
    "    input_batch = torch.LongTensor(indexes_batch).transpose(0, 1).to(device)\n",
    "    lengths = lengths.to(\"cpu\")  # Lengths need to be on CPU for pack_padded_sequence\n",
    "\n",
    "    # Decode the sentence with the searcher\n",
    "    tokens, scores = searcher(input_batch, lengths, max_length)\n",
    "    decoded_words = [voc.index_to_word[token.item()] for token in tokens]\n",
    "\n",
    "    return decoded_words\n",
    "\n",
    "\n",
    "def evaluate_input(searcher, voc):\n",
    "    \"\"\"\n",
    "    Interactively evaluate input from the user, considering the entire conversation history.\n",
    "    \"\"\"\n",
    "    conversation_history = []\n",
    "    while True:\n",
    "        try:\n",
    "            input_sentence = input(\"> \")\n",
    "            if input_sentence in (\"q\", \"quit\"):\n",
    "                break\n",
    "\n",
    "            # Normalize and add the user's input to the conversation history\n",
    "            print(input_sentence)\n",
    "            input_sentence = normalize_string(input_sentence)\n",
    "            conversation_history.append(input_sentence)\n",
    "            conversation_history.append(\"EOS\")\n",
    "\n",
    "            # Evaluate the conversation history\n",
    "            output_words = evaluate(searcher, voc, conversation_history)\n",
    "            print(\n",
    "                \"Bot:\",\n",
    "                \" \".join([word for word in output_words if word not in (\"EOS\", \"PAD\")]),\n",
    "            )\n",
    "\n",
    "            # Add the bot's response to the conversation history\n",
    "            conversation_history.extend(output_words)\n",
    "            conversation_history = conversation_history[-MAX_HISTORY * MAX_LENGTH :]\n",
    "\n",
    "        except KeyError:\n",
    "            print(\"Error: Encountered unknown word.\")\n",
    "\n",
    "\n",
    "# Set dropout layers to ``eval`` mode\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "\n",
    "# Initialize search module\n",
    "searcher = GreedySearchDecoder(encoder, decoder)\n",
    "\n",
    "# Begin chatting (uncomment and run the following line to begin)\n",
    "evaluate_input(searcher, voc)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
