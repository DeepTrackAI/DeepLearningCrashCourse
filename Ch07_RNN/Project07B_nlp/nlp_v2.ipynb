{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this guide, we delve into an engaging and intriguing application of recurrent sequence-to-sequence models by training a basic chatbot using film scripts from the Cornell Movie-Dialogs Corpus.\n",
    "\n",
    "Conversational models are currently a trending subject in the field of AI research. Chatbots are commonly used in various scenarios such as customer service platforms and online help desks. These bots typically utilize retrieval-based models that provide pre-set responses to specific types of questions. While these models might be adequate for highly specific domains like a company's IT helpdesk, they lack the robustness required for broader applications. However, the recent surge in deep learning, spearheaded recently by ChatGPT, has led to the development of potent multi-domain generative conversational models. In this guide, we will create one such model using the tools we have learnt so far.\n",
    "\n",
    "To begin, download the dialog dataset: \n",
    "\n",
    "https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html (should be replaced with direct data source for DLCC)\n",
    "\n",
    "and put in a ``data/`` directory under the current directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://zissou.infosci.cornell.edu/convokit/datasets/movie-corpus/movie-corpus.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing\n",
    "\n",
    "This step involves reorganizing our data file and loading the data into formats that are manageable.\n",
    "\n",
    "The [Cornell Movie-Dialogs Corpus](https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html) is a comprehensive dataset of dialogues from movie characters:\n",
    "\n",
    "- It contains 220,579 conversational exchanges between 10,292 pairs of movie characters.\n",
    "- It features 9,035 characters from 617 movies.\n",
    "- It has a total of 304,713 utterances.\n",
    "\n",
    "This dataset is vast and varied, with a wide range of language formality, time periods, sentiment, etc. We anticipate that this diversity will make our model capable of handling a variety of inputs and queries.\n",
    "\n",
    "Initially, we will examine some lines from our data file to understand the original format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "an uninterrupted chain of spoken or written language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"./dialogs.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "import re\n",
    "import contractions\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    standardized_text = contractions.fix(text)\n",
    "\n",
    "    standardized_text = (\n",
    "        standardized_text.replace(\"’\", \"'\")\n",
    "        .replace(\"‘\", \"'\")\n",
    "        .replace(\"´\", \"'\")\n",
    "        .replace(\"“\", '\"')\n",
    "        .replace(\"”\", '\"')\n",
    "        .replace(\"´´\", '\"')\n",
    "    )\n",
    "\n",
    "    tokens = tokenizer(standardized_text)\n",
    "\n",
    "    filtered_tokens = [\n",
    "        token\n",
    "        for token in tokens\n",
    "        if re.match(\n",
    "            r\"^[a-zA-Z0-9.,!?]+(-[a-zA-Z0-9.,!?]+)*(_[a-zA-Z0-9.,!?]+)*$\", token\n",
    "        )\n",
    "    ]\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "\n",
    "def corpus_iterator(filepath):\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "        prev_reply = None\n",
    "        for line in lines:\n",
    "\n",
    "            query, reply = line.strip().split(\"\\t\")\n",
    "\n",
    "            # Check if not the last line and if the current reply is identical to the next query\n",
    "            if query == prev_reply:\n",
    "                out = reply\n",
    "            else:\n",
    "                out = query + reply\n",
    "            prev_reply = reply\n",
    "\n",
    "            yield tokenize(out)\n",
    "\n",
    "\n",
    "# Add EOS, SOS, and PAD to the specials list\n",
    "special_tokens = [\"<pad>\", \"<sos>\", \"<eos>\", \"<unk>\"]\n",
    "\n",
    "vocab = build_vocab_from_iterator(\n",
    "    corpus_iterator(filepath),\n",
    "    specials=special_tokens,\n",
    "    min_freq=2,\n",
    ")\n",
    "vocab.set_default_index(vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.lookup_token(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "MAX_LENGTH = 30\n",
    "\n",
    "queries, responses, masks_r = [], [], []\n",
    "\n",
    "\n",
    "def all_words_in_vocab(sentence, vocab):\n",
    "    return all(word in vocab for word in sentence)\n",
    "\n",
    "\n",
    "def process_sentence(sequence, max_length=MAX_LENGTH):\n",
    "    # Calculate the length needed for padding. Subtract 2 for <sos> and <eos> tokens\n",
    "    padding_length = max_length - len(sequence) + 1\n",
    "\n",
    "    # Processed sequence with <sos>, <eos>, and <pad>\n",
    "    processed = [\"<sos>\"] + sequence + [\"<eos>\"] + [\"<pad>\"] * padding_length\n",
    "\n",
    "    # Create a mask: 1s for actual tokens and 0s for padding\n",
    "    # The mask length is len(sequence) + 2 for <sos> and <eos> tokens. The rest are 0s for padding.\n",
    "    mask = [1] * (len(sequence) + 2) + [0] * padding_length\n",
    "\n",
    "    return processed, mask\n",
    "\n",
    "\n",
    "with open(filepath, \"r\", encoding=\"utf-8\") as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "    prev_reply = None\n",
    "    for line in lines:\n",
    "\n",
    "        q, r = line.strip().split(\"\\t\")\n",
    "\n",
    "        query = tokenize(q)\n",
    "        response = tokenize(r)\n",
    "\n",
    "        if (\n",
    "            all_words_in_vocab(query + response, vocab)\n",
    "            and len(query) <= MAX_LENGTH\n",
    "            and len(response) <= MAX_LENGTH\n",
    "        ):\n",
    "            query, _ = process_sentence(query)\n",
    "            response, mask_r = process_sentence(response)\n",
    "\n",
    "            queries.append(vocab(query))\n",
    "            responses.append(vocab(response))\n",
    "            masks_r.append(mask_r)\n",
    "\n",
    "queries = np.asarray(queries)\n",
    "responses = np.asarray(responses)\n",
    "masks_r = np.asarray(masks_r)\n",
    "\n",
    "print(f\"Number of queries/responses: {len(queries)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(queries[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# The dimensionality of GloVe embeddings\n",
    "embedding_dim = 300\n",
    "\n",
    "from torchtext.vocab import GloVe\n",
    "\n",
    "# Load GloVe embeddings\n",
    "glove = GloVe(name=\"42B\", dim=embedding_dim, cache=\"./.vector_cache\")\n",
    "\n",
    "# Get GloVe embeddings for the vocabulary tokens\n",
    "# Assuming 'vocab' is a list of vocabulary tokens including special tokens at the beginning\n",
    "glove_embeddings = glove.get_vecs_by_tokens(vocab.get_itos(), lower_case_backup=True)\n",
    "\n",
    "\n",
    "# Special tokens\n",
    "special_tokens = [\"<pad>\", \"<sos>\", \"<eos>\"]\n",
    "num_special_tokens = len(special_tokens)\n",
    "\n",
    "# Initialize a tensor to hold the embeddings for special tokens\n",
    "# Here, PAD is initialized to zeros, and SOS, EOS to random values\n",
    "special_embeddings = torch.zeros(num_special_tokens, embedding_dim)\n",
    "special_embeddings[1:] = (\n",
    "    torch.rand(num_special_tokens - 1, embedding_dim) * 0.01\n",
    ")  # Small random numbers for SOS and EOS\n",
    "\n",
    "\n",
    "# Concatenate the special token embeddings with the GloVe embeddings\n",
    "extended_embeddings = torch.cat([special_embeddings, glove_embeddings], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import deeplay as dl\n",
    "from deeplay import DeeplayModule, Classifier\n",
    "\n",
    "hidden_features = 150\n",
    "\n",
    "\n",
    "class MyClassifier(Classifier):\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x1, x2, m = batch\n",
    "        y = torch.cat((x2[:, 1:], x2[:, -1:]), dim=1)\n",
    "        y_hat = self(x1, x2)\n",
    "        loss = self.loss(y_hat, y, m)\n",
    "        # loss = self.loss(y_hat.view(-1, y_hat.size(-1)), y.view(-1))\n",
    "        self.log(\n",
    "            f\"train_loss\",\n",
    "            loss,\n",
    "            on_step=True,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "            logger=True,\n",
    "        )\n",
    "\n",
    "        self.log_metrics(\n",
    "            \"train\", y_hat, y, on_step=True, on_epoch=True, prog_bar=True, logger=True\n",
    "        )\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        return self.model(x1, x2)\n",
    "\n",
    "\n",
    "class Encoder(DeeplayModule):\n",
    "    def __init__(self, vocab_size, embedding_dim, lstm_units):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, lstm_units, batch_first=True, dropout=0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        # x = torch.nn.utils.rnn.pack_padded_sequence(x, lengths)\n",
    "        outputs, (hidden, cell) = self.lstm(x)\n",
    "        return hidden, cell\n",
    "\n",
    "\n",
    "class Decoder(DeeplayModule):\n",
    "    def __init__(self, vocab_size, embedding_dim, lstm_units):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, lstm_units, batch_first=True, dropout=0.1)\n",
    "        self.dense = nn.Linear(lstm_units, vocab_size)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x, hidden, cell):\n",
    "        x = self.embedding(x)\n",
    "        outputs, (hidden, cell) = self.lstm(x, (hidden, cell))\n",
    "        outputs = self.dense(outputs)\n",
    "        outputs = self.softmax(outputs)\n",
    "        return outputs, hidden, cell\n",
    "\n",
    "\n",
    "class Seq2Seq(DeeplayModule):\n",
    "    def __init__(self, vocab_size, embedding_dim, lstm_units):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = Encoder(vocab_size, embedding_dim, lstm_units)\n",
    "        self.decoder = Decoder(vocab_size, embedding_dim, lstm_units)\n",
    "        self.vocab_size = vocab_size\n",
    "        self.lstm_units = lstm_units\n",
    "\n",
    "    def forward(self, encoder_input_data, decoder_input_data):\n",
    "        encoder_hidden, encoder_cell = self.encoder(encoder_input_data)\n",
    "\n",
    "        decoder_hidden = encoder_hidden\n",
    "        decoder_cell = encoder_cell\n",
    "\n",
    "        outputs = torch.zeros(\n",
    "            (decoder_input_data.size(0), decoder_input_data.size(1), self.vocab_size)\n",
    "        ).to(\"mps\")\n",
    "        for t in range(decoder_input_data.size(1)):  # Iterate through the sequence\n",
    "            output, decoder_hidden, decoder_cell = self.decoder(\n",
    "                decoder_input_data[:, t].unsqueeze(-1), decoder_hidden, decoder_cell\n",
    "            )\n",
    "            outputs[:, t, :] = output.squeeze(1)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "seq2seq = Seq2Seq(len(vocab), embedding_dim, hidden_features)\n",
    "\n",
    "\n",
    "def NLLLoss(inp, target, mask):\n",
    "    crossEntropy = -torch.log(\n",
    "        torch.gather(inp.view(-1, inp.shape[-1]), 1, target.view(-1, 1))\n",
    "    )\n",
    "    loss = crossEntropy.masked_select(mask.view(-1, 1)).mean()\n",
    "    return loss  # , nTotal.item()\n",
    "\n",
    "\n",
    "seq2seq_classifier = MyClassifier(\n",
    "    model=seq2seq,\n",
    "    loss=NLLLoss,  # nn.CrossEntropyLoss(),\n",
    "    optimizer=dl.RMSprop(),\n",
    ").create()\n",
    "\n",
    "seq2seq_classifier.model.encoder.embedding.weight.data = extended_embeddings\n",
    "seq2seq_classifier.model.encoder.embedding.weight.requires_grad = False\n",
    "seq2seq_classifier.model.decoder.embedding.weight.data = extended_embeddings\n",
    "seq2seq_classifier.model.decoder.embedding.weight.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deeptrack as dt\n",
    "import torch\n",
    "\n",
    "sources = dt.sources.Source(inputs=queries, targets=responses, masks=masks_r)\n",
    "\n",
    "inputs_pl = dt.Value(sources.inputs) >> dt.pytorch.ToTensor(dtype=torch.int)\n",
    "targets_pl = dt.Value(sources.targets) >> dt.pytorch.ToTensor(dtype=torch.int)\n",
    "masks_pl = dt.Value(sources.masks) >> dt.pytorch.ToTensor(dtype=torch.bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataset = dt.pytorch.Dataset(inputs_pl & targets_pl & masks_pl, inputs=sources)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = dl.Trainer(max_epochs=100, accelerator=\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(seq2seq_classifier, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_inference(model, source_text, max_length=MAX_LENGTH):\n",
    "    # Tokenize the source text\n",
    "    query_tokens = tokenize(source_text)\n",
    "\n",
    "    # Process the tokens into the model's expected format, including adding <sos>, <eos>, and padding\n",
    "    query, _ = process_sentence(query_tokens)\n",
    "\n",
    "    # Convert tokens to indices using the vocabulary\n",
    "    query = np.array(vocab(query))\n",
    "\n",
    "    # Convert list of indices to a tensor and add a batch dimension\n",
    "    source_sequence = torch.tensor(query, dtype=torch.int)\n",
    "\n",
    "    # Move tensor to the same device as the model\n",
    "    source_sequence = source_sequence.to(next(model.parameters()).device)\n",
    "\n",
    "    # Encoder inference\n",
    "    with torch.no_grad():\n",
    "        hidden, cell = model.encoder(source_sequence)\n",
    "\n",
    "    # Prepare the initial input to the decoder: <sos> token index\n",
    "    target_index = torch.tensor(vocab([\"<sos>\"]), device=source_sequence.device)\n",
    "\n",
    "    predictions = []\n",
    "    for _ in range(max_length):\n",
    "        with torch.no_grad():\n",
    "            output, hidden, cell = model.decoder(target_index, hidden, cell)\n",
    "            top1 = output.argmax(1)  # Adjust indexing based on output shape\n",
    "            if top1.item() == vocab([\"<eos>\"])[0]:  # Stop if <eos> token is generated\n",
    "                break\n",
    "            predictions.append(top1.item())\n",
    "            target_index = top1\n",
    "\n",
    "    # Convert indices back to tokens\n",
    "    predicted_tokens = [vocab.lookup_token(idx) for idx in predictions]\n",
    "\n",
    "    return \" \".join(predicted_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'make_inference' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m source_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou are special!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mmake_inference\u001b[49m(seq2seq_classifier\u001b[38;5;241m.\u001b[39mmodel, source_text)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(response)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'make_inference' is not defined"
     ]
    }
   ],
   "source": [
    "source_text = \"you are special!\"\n",
    "response = make_inference(seq2seq_classifier.model, source_text)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Formatted Data File\n",
    "\n",
    "For ease of use, we will generate a well-structured data file where each line comprises a tab-separated pair of a *query sentence* and a *response sentence*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pairs = []\n",
    "\n",
    "# for conv_id in corpus.conversations:\n",
    "#     conv = corpus.get_conversation(conv_id)\n",
    "#     utt_ids = conv.get_utterance_ids()\n",
    "\n",
    "#     num_utt = len(utt_ids)\n",
    "#     range_end = num_utt - 2 if num_utt % 2 != 0 else num_utt - 1\n",
    "\n",
    "#     for i in range(range_end):\n",
    "#         query = corpus.get_utterance(utt_ids[i]).text.strip()\n",
    "#         response = corpus.get_utterance(utt_ids[i + 1]).text.strip()\n",
    "#         if query and response:\n",
    "#             pairs.append(f\"{query}|-->|{response}\")\n",
    "\n",
    "# print(f\"Number of pairs: {len(pairs)}\")\n",
    "\n",
    "# # filename = os.path.join(corpus_name, \"formatted_movie_lines.txt\")\n",
    "\n",
    "# # with open(filename, \"w\", encoding=\"utf-8\") as file:\n",
    "# #     for pair in pairs:\n",
    "# #         file.write(pair + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "convert all letters to lowercase \n",
    "\n",
    "trim all non-letter characters except for basic punctuation\n",
    "\n",
    "filter out sentences with length greater than the MAX_LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "\n",
    "\n",
    "# def clean_text(text):\n",
    "#     text = text.strip().lower()\n",
    "#     text = re.sub(r\"[^a-z0-9.!?]+\", r\" \", text)\n",
    "#     text = re.sub(r\"([.!?])\", r\" \\1 \", text)\n",
    "#     text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "#     return text\n",
    "\n",
    "\n",
    "# MAX_LENGTH = 30\n",
    "# proc_pairs = []\n",
    "\n",
    "# for pair in pairs:\n",
    "#     query, response = pair.strip().split(\"|-->|\")\n",
    "#     clean_query = clean_text(query)\n",
    "#     clean_response = clean_text(response)\n",
    "#     if len(clean_query) <= MAX_LENGTH and len(clean_response) <= MAX_LENGTH:\n",
    "#         proc_pairs.append(f\"{clean_query}|-->|{clean_response}\")\n",
    "\n",
    "# print(f\"Number of pairs: {len(proc_pairs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create a vocabulary\n",
    "\n",
    "Remove words below a certain count threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading and Trimming\n",
    "\n",
    "The next step involves creating a vocabulary and loading query/response sentence pairs into memory.\n",
    "\n",
    "Keep in mind that we are working with sequences of **words**, which do not inherently map to a discrete numerical space. Therefore, we need to create such a mapping by associating each unique word we encounter in our dataset with an index value.\n",
    "\n",
    "To achieve this, we define a `Vocabulary` class, which maintains a mapping from words to indexes, a reverse mapping from indexes to words, a count of each word, and a total word count. The class offers methods for adding a word to the vocabulary (`add_word`), adding all words in a sentence (`add_sentence`), and trimming infrequently seen words (`trim`). We will discuss trimming in more detail later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# sentences = [\n",
    "#     sentence.strip() for pair in proc_pairs for sentence in pair.strip().split(\"|-->|\")\n",
    "# ]\n",
    "\n",
    "# MIN_COUNT = 5\n",
    "\n",
    "# # Initialize CountVectorizer\n",
    "# vectorizer = CountVectorizer(\n",
    "#     min_df=MIN_COUNT, tokenizer=lambda txt: txt.strip().split(\" \")\n",
    "# )\n",
    "# X = vectorizer.fit_transform(sentences)\n",
    "\n",
    "# # Get the feature names to build a vocab dictionary\n",
    "# vocab = vectorizer.get_feature_names_out()\n",
    "\n",
    "# print(f\"Vocabulary size: {len(vocab)}\")\n",
    "# # print(f\"Encoded sentence example: {encoded_sentences[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep_pairs = []\n",
    "# for pair in proc_pairs:\n",
    "#     query, response = pair.strip().split(\"|-->|\")\n",
    "\n",
    "#     keep_query = True\n",
    "#     keep_response = True\n",
    "#     # Check input sentence\n",
    "#     for word in query.strip().split(\" \"):\n",
    "#         if word not in vocab:\n",
    "#             keep_query = False\n",
    "#             break\n",
    "#     # Check output sentence\n",
    "#     for word in response.strip().split(\" \"):\n",
    "#         if word not in vocab:\n",
    "#             keep_response = False\n",
    "#             break\n",
    "\n",
    "#     if keep_query and keep_response:\n",
    "#         keep_pairs.append(pair)\n",
    "# print(f\"Number of pairs: {len(keep_pairs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PAD, SOS,EOS token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Mapping words to indices (+3 offset for special tokens)\n",
    "# word_to_idx = {word: i + 3 for i, word in enumerate(vocab)}\n",
    "\n",
    "# # Adding special tokens to the dictionary\n",
    "# word_to_idx[\"<PAD>\"] = 0  # PAD\n",
    "# word_to_idx[\"<SOS>\"] = 1  # SOS\n",
    "# word_to_idx[\"<EOS>\"] = 2  # EOS\n",
    "\n",
    "\n",
    "# # Example of encoding a sentence with SOS, EOS, and converting to indices\n",
    "# def encode_sentence(sentence, word_to_idx, max_len=MAX_LENGTH):\n",
    "#     # Tokenize the sentence\n",
    "#     tokens = sentence.split()\n",
    "\n",
    "#     # Add SOS and EOS tokens\n",
    "#     tokens = [\"<SOS>\"] + tokens + [\"<EOS>\"]\n",
    "\n",
    "#     # Convert tokens to indices\n",
    "#     indices = [word_to_idx.get(token, word_to_idx[\"<EOS>\"]) for token in tokens]\n",
    "\n",
    "#     # Pad the sequence to max_len\n",
    "#     padded_sequence = indices + [word_to_idx[\"<PAD>\"]] * (max_len - len(indices))\n",
    "#     return padded_sequence[:max_len]\n",
    "\n",
    "\n",
    "# sentences = [\n",
    "#     sentence.strip() for pair in keep_pairs for sentence in pair.strip().split(\"|-->|\")\n",
    "# ]\n",
    "# # Example: encoding the first sentence\n",
    "# # max_len = max(len(s.split()) for s in sentences) + 2  # +2 for SOS and EOS tokens\n",
    "# encoded_sentences = [encode_sentence(s, word_to_idx) for s in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"Encoded sentence example: {encoded_sentences[10000]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idx_to_word = {idx: word for word, idx in word_to_idx.items()}\n",
    "\n",
    "\n",
    "# def decode_sentence(encoded_sentence, idx_to_word):\n",
    "#     # Convert indices back to tokens, ignoring special tokens for padding, start, and end\n",
    "#     tokens = [\n",
    "#         idx_to_word.get(idx)\n",
    "#         for idx in encoded_sentence\n",
    "#         if idx in idx_to_word and idx > 2\n",
    "#     ]\n",
    "\n",
    "#     # Join the tokens back into a single string\n",
    "#     sentence = \" \".join(tokens)\n",
    "#     return sentence\n",
    "\n",
    "\n",
    "# # Example: decoding the first encoded sentence\n",
    "# decoded_sentences = [\n",
    "#     decode_sentence(encoded, idx_to_word) for encoded in encoded_sentences\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"Decoded sentence example: {decoded_sentences[0:20]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now compile our vocabulary and query/response sentence pairs. However, before we can utilize this data, we need to carry out some preprocessing steps.\n",
    "\n",
    "Initially, we need to transform the Unicode strings into ASCII using `unicodeToAscii`. Subsequently, we should convert all characters to lowercase and remove all non-letter characters, excluding basic punctuation (`normalize_string`). Lastly, to facilitate training convergence, we will exclude sentences exceeding the `MAX_LENGTH` threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another tactic that is beneficial to achieving faster convergence during\n",
    "training is trimming rarely used words out of our vocabulary. Decreasing\n",
    "the feature space will also soften the difficulty of the function that\n",
    "the model must learn to approximate. We will do this as a two-step\n",
    "process:\n",
    "\n",
    "1) Trim words used under ``MIN_COUNT`` threshold using the ``voc.trim``\n",
    "   function.\n",
    "\n",
    "2) Filter out pairs with trimmed words.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation for Models\n",
    "\n",
    "Despite our extensive efforts to curate and process our data into a convenient vocabulary object and list of sentence pairs, our models will ultimately require numerical torch tensors as inputs.  \n",
    "\n",
    " To accommodate sentences of different sizes in the same batch, we will create our batched input tensor of shape (max_length, batch_size), where sentences shorter than the max_length are zero padded after an EOS_token.\n",
    "\n",
    "If we simply convert our English sentences to tensors by converting words to their indexes and zero-pad, our tensor would have shape (batch_size, max_length) and indexing the first dimension would return a full sequence across all time-steps. However, we need to be able to index our batch along time, and across all sequences in the batch. Therefore, we transpose our input batch shape to (max_length, batch_size), so that indexing across the first dimension returns a time step across all sentences in the batch.\n",
    "\n",
    "The output function palso returns a binary mask tensor and a maximum target sentence length. The binary mask tensor has the same shape as the output target tensor, but every element that is a PAD_token is 0 and all others are 1.\n",
    "\n",
    "`batch_to_train_data` simply takes a bunch of pairs and returns the input and target tensors using the aforementioned functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Procedure Definition\n",
    "\n",
    "### Loss with Masking\n",
    "\n",
    "Given that we're working with batches of padded sequences, we can't compute loss using all tensor elements. We establish `mask_nll_loss` to compute our loss based on the decoder's output tensor, the target tensor, and a binary mask tensor that indicates the padding of the target tensor. This loss function computes the average negative log likelihood of the elements that align with a *1* in the mask tensor.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Training Iteration Procedure\n",
    "\n",
    "The `train` function encapsulates the process for a single training iteration (a single batch of inputs).\n",
    "\n",
    "We employ two strategies to aid convergence:\n",
    "\n",
    "-  **Teacher forcing**: At a probability determined by `teacher_forcing_ratio`, we use the current target word as the decoder’s next input instead of the decoder’s current guess. This helps in efficient training but can cause instability during inference. Hence, the `teacher_forcing_ratio` must be set carefully.\n",
    "\n",
    "-  **Gradient clipping**: This technique counters the \"exploding gradient\" problem by capping gradients to a maximum value, preventing them from growing exponentially and causing overflow or overshooting steep cost function cliffs.\n",
    "\n",
    "**Procedure:**\n",
    "\n",
    "   1) Pass the entire input batch through the encoder.\n",
    "   2) Initialize decoder inputs as SOS_token, and hidden state as the encoder's final hidden state.\n",
    "   3) Pass the input batch sequence through the decoder one time step at a time.\n",
    "   4) If teacher forcing: set next decoder input as the current target; else: set next decoder input as current decoder output.\n",
    "   5) Calculate and accumulate loss.\n",
    "   6) Perform backpropagation.\n",
    "   7) Clip gradients.\n",
    "   8) Update encoder and decoder model parameters.\n",
    "\n",
    "Note: PyTorch’s RNN modules (`RNN`, `LSTM`, `GRU`) can be used like any other non-recurrent layers by passing them the entire input sequence. We use the `GRU` layer like this in the `encoder`. However, you can also run these modules one time-step at a time, as we do for the `decoder` model."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
