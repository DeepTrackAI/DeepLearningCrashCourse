{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translating with a Recurrent Neural Networks\n",
    "\n",
    "This notebook provides the complete code example that implements a sequence-to-sequence (seq2seq) model for machine translation using recurrent neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Vocabularies\n",
    "\n",
    "Implement a function to tokenize and standardize text ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import contractions, re\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "tokenizer_eng = get_tokenizer(\"spacy\", language=\"en_core_web_sm\")\n",
    "tokenizer_spa = get_tokenizer(\"spacy\", language=\"es_core_news_sm\")\n",
    "\n",
    "def tokenize(text, lang):\n",
    "    \"\"\"Standardize, tokenize and filter text.\"\"\"\n",
    "    text = (text.replace(\"’\", \"'\").replace(\"‘\", \"'\").replace(\"´\", \"'\")\n",
    "            .replace(\"“\", '\"').replace(\"”\", '\"').replace(\"´´\", '\"'))\n",
    "    tokens = (tokenizer_eng(contractions.fix(text)) if lang == \"eng\"\n",
    "              else tokenizer_spa(text))  # lang == \"spa\"\n",
    "    filtered_tokens = [token for token in tokens if re.match(\n",
    "        r\"\"\"\n",
    "        ^[a-zA-Z0-9áéíóúüñÁÉÍÓÚÜÑ.,!?¡¿]+  # 1+ allowed characters.\n",
    "        (-[a-zA-Z0-9áéíóúüñÁÉÍÓÚÜÑ.,!?¡¿]+)*  # Optional hyphen plus chars.\n",
    "        (_[a-zA-Z0-9áéíóúüñÁÉÍÓÚÜÑ.,!?¡¿]+)*  # Optional underscore plus chars.\n",
    "        $  # End of the string.\n",
    "        \"\"\", token, re.VERBOSE)]\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... a function to read and tokenize sentences by iterating through a corpus file ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corpus_iterator(filename, lang, lang_position):\n",
    "    \"\"\"Read and tokenize texts by iterating through a corpus file.\"\"\"\n",
    "    with open(filename, \"r\", encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            sentences = line.strip().split(\"\\t\")\n",
    "            sentence = sentences[lang_position]\n",
    "            yield tokenize(sentence, lang)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... a function to build a vocabulary from a corpus file ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "def build_vocab(filename, lang, lang_position, specials=\"<unk>\", min_freq=5):\n",
    "    \"\"\"Build vocabulary.\"\"\"\n",
    "    vocab = build_vocab_from_iterator(\n",
    "        corpus_iterator(filename, lang, lang_position),\n",
    "        min_freq=min_freq,\n",
    "        specials=specials,\n",
    "    )\n",
    "    vocab.set_default_index(vocab[specials[-1]])\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and build the vocabularies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_lang, out_lang, filename = \"eng\", \"spa\", \"eng-spa.txt\"\n",
    "special_tokens = [\"<pad>\", \"<sos>\", \"<eos>\", \"<unk>\"]\n",
    "\n",
    "in_vocab = build_vocab(filename, in_lang, lang_position=0, \n",
    "                       specials=special_tokens)\n",
    "out_vocab = build_vocab(filename, out_lang, lang_position=1, \n",
    "                        specials=special_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the Data\n",
    "\n",
    "Implement a function to check if all words in a sentence are present in a vocabulary ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_words_in_vocab(sentence, vocab):\n",
    "    \"\"\"Check whether all words in a sentence are present in a vocabulary\"\"\"\n",
    "    return all(word in vocab for word in sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... a function to pad a sequence of tokens ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad(tokens, max_length=10):\n",
    "    \"\"\"Pad sequence of tokens.\"\"\"\n",
    "    padding_length = max_length - len(tokens)\n",
    "    return [\"<sos>\"] + tokens + [\"<eos>\"] + [\"<pad>\"] * padding_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... a function to process the language corpus ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def process(file, in_lang, out_lang, in_vocab, out_vocab, max_length=10):\n",
    "    \"\"\"Process language corpus.\"\"\"\n",
    "    in_sequences, out_sequences = [], []\n",
    "    for line in file:\n",
    "        texts = line.strip().split(\"\\t\")\n",
    "        in_tokens = tokenize(texts[0], in_lang)\n",
    "        out_tokens = tokenize(texts[1], out_lang)\n",
    "\n",
    "        if (all_words_in_vocab(in_tokens, in_vocab) \n",
    "            and len(in_tokens) <= max_length\n",
    "            and all_words_in_vocab(out_tokens, out_vocab)\n",
    "            and len(out_tokens) <= max_length):\n",
    "            \n",
    "            padded_in_tokens = pad(in_tokens)\n",
    "            in_sequence = in_vocab(padded_in_tokens)\n",
    "            in_sequences.append(in_sequence)\n",
    "            \n",
    "            padded_out_tokens = pad(out_tokens)\n",
    "            out_sequence = out_vocab(padded_out_tokens)\n",
    "            out_sequences.append(out_sequence)\n",
    "\n",
    "    return np.array(in_sequences), np.array(out_sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and build the datasets and data loaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deeptrack as dt\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "with open(filename, \"r\", encoding=\"utf-8\") as file:\n",
    "    in_sequences, out_sequences = \\\n",
    "        process(file, in_lang, out_lang, in_vocab, out_vocab)\n",
    "\n",
    "sources = dt.sources.Source(inputs=in_sequences, targets=out_sequences)\n",
    "train_sources, test_sources = dt.sources.random_split(sources, [0.85, 0.15])\n",
    "\n",
    "inputs_pip = dt.Value(sources.inputs) >> dt.pytorch.ToTensor(dtype=torch.int)\n",
    "outputs_pip = dt.Value(sources.targets) >> dt.pytorch.ToTensor(dtype=torch.int)\n",
    "\n",
    "train_dataset = \\\n",
    "    dt.pytorch.Dataset(inputs_pip & outputs_pip, inputs=train_sources)\n",
    "test_dataset = \\\n",
    "    dt.pytorch.Dataset(inputs_pip & outputs_pip, inputs=test_sources)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing and Training the Sequence-to-Sequence Architecture\n",
    "\n",
    "Implement the encoder ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deeplay as dl\n",
    "\n",
    "class Seq2SeqEncoder(dl.DeeplayModule):\n",
    "    \"\"\"Sequence-to-sequence encoder.\"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, in_features=300, hidden_features=128,\n",
    "                 hidden_layers=1, dropout=0.0, bidirectional=True):\n",
    "        \"\"\"Initialize sequence-to-sequence encoder.\"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden_features = hidden_features\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.bidirectional = bidirectional\n",
    "        \n",
    "        self.embedding = dl.Layer(torch.nn.Embedding, vocab_size, in_features)\n",
    "        self.rnn = dl.Layer(torch.nn.GRU, input_size=in_features, \\\n",
    "            hidden_size=hidden_features, num_layers=hidden_layers, \\\n",
    "            dropout=(0 if hidden_layers == 1 else dropout), \\\n",
    "            bidirectional=bidirectional, batch_first=True)\n",
    "\n",
    "    def forward(self, in_sequences, hidden=None):\n",
    "        \"\"\"Perform forward pass.\"\"\"\n",
    "        in_embedding = self.embedding(in_sequences)\n",
    "        encoder_output, hidden = self.rnn(in_embedding, hidden)\n",
    "        if self.bidirectional:\n",
    "            encoder_output = (encoder_output[:, :, :self.hidden_features]\n",
    "                               + encoder_output[:, :, self.hidden_features:])\n",
    "            hidden = hidden[:self.hidden_layers]\n",
    "        return encoder_output, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... implement the decoder ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqDecoder(dl.DeeplayModule):\n",
    "    \"\"\"Sequence-to-sequence decoder.\"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, in_features=300, hidden_features=128, \n",
    "                 hidden_layers=1, dropout=0.0):\n",
    "        \"\"\"Initialize sequence-to-sequence decoder.\"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = dl.Layer(torch.nn.Embedding, vocab_size, in_features)\n",
    "        self.rnn = dl.Layer(torch.nn.GRU, input_size=in_features, \\\n",
    "            hidden_size=hidden_features, num_layers=hidden_layers, \\\n",
    "            bidirectional=False, batch_first=True, \\\n",
    "            dropout=(0 if hidden_layers == 1 else dropout))\n",
    "        self.dense = dl.Layer(torch.nn.Linear, hidden_features, vocab_size)     ### Carlo: Is this 1-hot encoded?\n",
    "        self.softmax = dl.Layer(torch.nn.Softmax, dim=-1)\n",
    "        self.relu = dl.Layer(torch.nn.ReLU)\n",
    "\n",
    "    def forward(self, out_tokens, hidden):\n",
    "        \"\"\"Perform forward pass.\"\"\"\n",
    "        out_embeddings = self.embedding(out_tokens)\n",
    "        out_embeddings = self.relu(out_embeddings)\n",
    "        decoder_output, hidden = self.rnn(out_embeddings, hidden)\n",
    "        decoder_output = self.dense(decoder_output)\n",
    "        decoder_output = self.softmax(decoder_output)\n",
    "        return decoder_output, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... implement the full seq2seq model combining the encoder and decoder ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqModel(dl.DeeplayModule):\n",
    "    \"\"\"Sequence-to-sequence model.\"\"\"\n",
    "\n",
    "    def __init__(self, in_vocab_size=None, out_vocab_size=None,\n",
    "                 teacher_prob=1.0, embedding_dim=300, hidden_features=128,\n",
    "                 hidden_layers=1, dropout=0.0, bidirectional=True):\n",
    "        \"\"\"Initialize the sequence-to-sequence model.\"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.in_vocab_size, self.out_vocab_size = in_vocab_size, out_vocab_size\n",
    "        self.teacher_prob = teacher_prob\n",
    "        self.encoder = Seq2SeqEncoder(in_vocab_size, embedding_dim, \\\n",
    "            hidden_features, hidden_layers, dropout, bidirectional)\n",
    "        self.decoder = Seq2SeqDecoder(out_vocab_size, embedding_dim, \\\n",
    "            hidden_features, hidden_layers, dropout)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        \"\"\"Perform forward pass.\"\"\"\n",
    "        in_sequences, out_sequences = batch\n",
    "        num_sequences, sequence_length = in_sequences.size()\n",
    "        device = next(self.encoder.parameters()).device\n",
    "        \n",
    "        _, decoder_hidden = self.encoder(in_sequences)  # = encoder_hidden\n",
    "        decoder_outputs = torch.zeros(num_sequences, sequence_length,\n",
    "                                      self.out_vocab_size).to(device)\n",
    "        \n",
    "        for t in range(sequence_length):\n",
    "            if t == 0 or np.random.rand() < self.teacher_prob:\n",
    "                decoder_input = out_sequences[:, t].unsqueeze(-1).to(device)\n",
    "            else:\n",
    "                _, top_decoder_output = decoder_output.topk(1)\n",
    "                decoder_input = \\\n",
    "                    top_decoder_output.squeeze(-1).detach().to(device)\n",
    "\n",
    "            decoder_output, decoder_hidden = \\\n",
    "                self.decoder(decoder_input, decoder_hidden)\n",
    "            decoder_outputs[:, t, :] = decoder_output.squeeze(1)\n",
    "        return decoder_outputs\n",
    "\n",
    "    def evaluate(self, in_sequences):\n",
    "        \"\"\"Evaluate model.\"\"\"\n",
    "        num_sequences, sequence_length = in_sequences.size()\n",
    "        device = next(self.encoder.parameters()).device\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            _, decoder_hidden = self.encoder(in_sequences)  # = encoder_hidden\n",
    "        out_sequences = torch.zeros(num_sequences, sequence_length).to(device)\n",
    "        \n",
    "        for t in range(sequence_length):\n",
    "            if t == 0:\n",
    "                decoder_input = torch.full(size=(num_sequences, 1), \n",
    "                                           fill_value=1, device=device)\n",
    "            else:\n",
    "                decoder_input = top_decoder_output.squeeze(-1).detach()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                decoder_output, decoder_hidden = \\\n",
    "                    self.decoder(decoder_input.to(device), decoder_hidden)\n",
    "            _, top_decoder_output = decoder_output.topk(1)\n",
    "            out_sequences[:, t] = top_decoder_output.squeeze()\n",
    "        return out_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... define the loss function ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maskedNLL(pred_sequences, target_sequences, padding=0):                     ### Carlo: Check names of arguments.\n",
    "    \"\"\"Calculate the masked negative log-likelihood (NLL) loss.\"\"\"\n",
    "    flat_pred_sequences = pred_sequences.view(-1, pred_sequences.shape[-1])     ### Carlo: Is this 1-hot encoded?\n",
    "    flat_target_sequences = target_sequences.view(-1, 1)\n",
    "    pred_probs = torch.gather(flat_pred_sequences, 1, flat_target_sequences)\n",
    "\n",
    "    nll = - torch.log(pred_probs)\n",
    "\n",
    "    mask = target_sequences != padding\n",
    "    masked_nll = nll.masked_select(mask.view(-1, 1))\n",
    "    \n",
    "    return masked_nll.mean()  # Loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... implement the sequence-to-sequence application ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(dl.Application):\n",
    "    \"\"\"Application for the sequence-to-sequence model.\"\"\"\n",
    "\n",
    "    def __init__(self, in_vocab, out_vocab, teacher_prob=1.0):\n",
    "        \"\"\"Initialize the application.\"\"\"\n",
    "        super().__init__(loss=maskedNLL, optimizer=dl.Adam(lr=1e-3))\n",
    "        self.model = Seq2SeqModel(in_vocab_size=len(in_vocab), \\\n",
    "            out_vocab_size=len(out_vocab), teacher_prob=teacher_prob)\n",
    "\n",
    "    def train_preprocess(self, batch):\n",
    "        \"\"\"Adjust the target sequence by shifting it one position backward.\"\"\"\n",
    "        in_sequences, out_sequences = batch\n",
    "        shifted_out_sequences = \\\n",
    "            torch.cat((out_sequences[:, 1:], out_sequences[:, -1:]), dim=1)     ### Carlo: Is this (out_sequences[:, -1:]) just to repeat the pad token number?\n",
    "        return (in_sequences, out_sequences), shifted_out_sequences\n",
    "    \n",
    "    def forward(self, batch):\n",
    "        \"\"\"Perform forward pass.\"\"\"\n",
    "        return self.model(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... load some pretrained embeddings ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:torchtext.vocab.vectors:Loading vectors from ./.glove_cache/glove.42B.300d.txt.pt\n"
     ]
    }
   ],
   "source": [
    "from torchtext.vocab import GloVe\n",
    "\n",
    "embedding_dim = 300\n",
    "\n",
    "glove = GloVe(name=\"42B\", dim=embedding_dim, cache=\"./.glove_cache\")\n",
    "glove_embeddings_in = glove.get_vecs_by_tokens(in_vocab.get_itos(), \n",
    "                                               lower_case_backup=True)\n",
    "glove_embeddings_out = glove.get_vecs_by_tokens(out_vocab.get_itos(), \n",
    "                                                lower_case_backup=True)\n",
    "\n",
    "num_special_tokens = len(special_tokens)\n",
    "glove_embeddings_in[1:num_special_tokens] = \\\n",
    "    torch.rand(num_special_tokens - 1, embedding_dim) * 0.01\n",
    "glove_embeddings_out[1:num_special_tokens] = \\\n",
    "    torch.rand(num_special_tokens - 1, embedding_dim) * 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... instantiate the seq2seq model ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq2seq = Seq2Seq(in_vocab=in_vocab, out_vocab=out_vocab, teacher_prob=0.85)\n",
    "seq2seq = seq2seq.create()\n",
    "\n",
    "seq2seq.model.encoder.embedding.weight.data = glove_embeddings_in\n",
    "seq2seq.model.encoder.embedding.weight.requires_grad = False\n",
    "seq2seq.model.decoder.embedding.weight.data = glove_embeddings_out\n",
    "seq2seq.model.decoder.embedding.weight.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and train the model ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/giovannivolpe/Documents/GitHub/DeepLearningCrashCourse/py_env_dlcc/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:75: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "/Users/giovannivolpe/Documents/GitHub/DeepLearningCrashCourse/py_env_dlcc/lib/python3.12/site-packages/lightning/pytorch/trainer/configuration_validator.py:74: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
       "┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name          </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type             </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span>┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span>│ train_metrics │ MetricCollection │      0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 </span>│ val_metrics   │ MetricCollection │      0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2 </span>│ test_metrics  │ MetricCollection │      0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3 </span>│ model         │ Seq2SeqModel     │  6.3 M │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 4 </span>│ optimizer     │ Adam             │      0 │\n",
       "└───┴───────────────┴──────────────────┴────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
       "┃\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mName         \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mType            \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
       "│\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m│ train_metrics │ MetricCollection │      0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m1\u001b[0m\u001b[2m \u001b[0m│ val_metrics   │ MetricCollection │      0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m2\u001b[0m\u001b[2m \u001b[0m│ test_metrics  │ MetricCollection │      0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m3\u001b[0m\u001b[2m \u001b[0m│ model         │ Seq2SeqModel     │  6.3 M │\n",
       "│\u001b[2m \u001b[0m\u001b[2m4\u001b[0m\u001b[2m \u001b[0m│ optimizer     │ Adam             │      0 │\n",
       "└───┴───────────────┴──────────────────┴────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 1.7 M                                                                                            \n",
       "<span style=\"font-weight: bold\">Non-trainable params</span>: 4.6 M                                                                                        \n",
       "<span style=\"font-weight: bold\">Total params</span>: 6.3 M                                                                                                \n",
       "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 25                                                                         \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mTrainable params\u001b[0m: 1.7 M                                                                                            \n",
       "\u001b[1mNon-trainable params\u001b[0m: 4.6 M                                                                                        \n",
       "\u001b[1mTotal params\u001b[0m: 6.3 M                                                                                                \n",
       "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 25                                                                         \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "958631a432fc49839b451e74bd29b05c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/giovannivolpe/Documents/GitHub/DeepLearningCrashCourse/py_env_dlcc/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=10` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = dl.Trainer(max_epochs=25, accelerator=\"auto\")\n",
    "trainer.fit(seq2seq, train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Model Perfomance\n",
    "\n",
    "Implement a function to convert numerical sequences into their corresponding text ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unprocess(sequences, vocab):\n",
    "    \"\"\"Convert numeric sequences to sentences.\"\"\"\n",
    "    sentences = []\n",
    "    for sequence in sequences:\n",
    "        idxs = sequence[sequence > 2]\n",
    "        words = [vocab.lookup_token(idx) for idx in idxs]\n",
    "        sentences.append(\" \".join(words))\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... a function to translate user-defined sentences ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(in_sentence, model, in_lang, in_vocab, out_vocab):\n",
    "    \"\"\"Translate a sentence.\"\"\"\n",
    "    in_tokens = pad(tokenize(in_sentence, in_lang))\n",
    "    in_sequence = (torch.tensor(in_vocab(in_tokens), dtype=torch.int)\n",
    "                   .unsqueeze(0).to(next(model.parameters()).device))\n",
    "    pred_sequence = model.evaluate(in_sequence)\n",
    "    pred_sentence = unprocess(pred_sequence, out_vocab)\n",
    "    print(f\"Predicted Translation: {pred_sentence[0]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... try to translate a simple sentence ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Translation: Compré un libro .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "in_sentence = \"I bought a book.\"\n",
    "translate(in_sentence, seq2seq.model, in_lang, in_vocab, out_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... another simple sentence ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Translation: Este libro es muy interesante .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "in_sentence = \"This book is very interesting.\"\n",
    "translate(in_sentence, seq2seq.model, in_lang, in_vocab, out_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and a more complex one ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Translation: El libro que compré muy interesante .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "in_sentence = \"The book that I bought is very interesting.\"\n",
    "translate(in_sentence, seq2seq.model, in_lang, in_vocab, out_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Model with the BLEU Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text: Tom did not come to the last meeting .\n",
      "Predicted Translation: Tom no esperaba el fin de octubre .\n",
      "Actual Translation: Tom no vino a la última reunión .\n",
      "\n",
      "Input text: You need more practice .\n",
      "Predicted Translation: Tienes más práctica .\n",
      "Actual Translation: Ustedes necesitan más práctica .\n",
      "\n",
      "Input text: Tom painted .\n",
      "Predicted Translation: Tom pintó .\n",
      "Actual Translation: Tom se puso a pintar .\n",
      "\n",
      "Input text: Can I tell you something ?\n",
      "Predicted Translation: ¿ Puedo decir algo ?\n",
      "Actual Translation: ¿ Puedo decirte algo ?\n",
      "\n",
      "Input text: This is not fair .\n",
      "Predicted Translation: No es justo .\n",
      "Actual Translation: Esto no es justo .\n",
      "\n",
      "Input text: They usually sleep in this room .\n",
      "Predicted Translation: En este mercado se puede dormir en esta habitación .\n",
      "Actual Translation: Normalmente duermen en esta habitación .\n",
      "\n",
      "Input text: The doctor might have said that .\n",
      "Predicted Translation: El médico que debería haber dicho .\n",
      "Actual Translation: Puede que el doctor haya dicho eso .\n",
      "\n",
      "Input text: How much does that lamp cost ?\n",
      "Predicted Translation: ¿ Cuánto cuesta ese precio ?\n",
      "Actual Translation: ¿ Cuánto cuesta esa lámpara ?\n",
      "\n",
      "Input text: It is fun to play baseball .\n",
      "Predicted Translation: Jugar es divertido jugar al béisbol .\n",
      "Actual Translation: Es divertido jugar al béisbol .\n",
      "\n",
      "Input text: I thought you were going to help .\n",
      "Predicted Translation: Pensé que ibas a ayudar .\n",
      "Actual Translation: Pensé que ibas a ayudar .\n",
      "\n",
      "Input text: My dogs are white .\n",
      "Predicted Translation: Mis gatos son azules .\n",
      "Actual Translation: Mis perros son blancos .\n",
      "\n",
      "Input text: Tom put all his eggs in one basket .\n",
      "Predicted Translation: Tom se puso sus cosas en el bolsillo .\n",
      "Actual Translation: Tom puso todos sus huevos en una canasta .\n",
      "\n",
      "Input text: There is no running water .\n",
      "Predicted Translation: No hay agua sin agua .\n",
      "Actual Translation: No hay agua corriente .\n",
      "\n",
      "Input text: I heard the bell .\n",
      "Predicted Translation: Oí la campana .\n",
      "Actual Translation: Oí la campana .\n",
      "\n",
      "Input text: Be attentive .\n",
      "Predicted Translation: Sé arrogante .\n",
      "Actual Translation: Estar atento .\n",
      "\n",
      "Input text: Thanks for showing me how to do that .\n",
      "Predicted Translation: Gracias por qué me hizo eso .\n",
      "Actual Translation: Gracias por mostrarme cómo se hace .\n",
      "\n",
      "Input text: It was terribly cold yesterday .\n",
      "Predicted Translation: Ayer hacía mucho frío .\n",
      "Actual Translation: Ayer hizo un frío terrible .\n",
      "\n",
      "Input text: She is a very good teacher .\n",
      "Predicted Translation: Ella es muy buen profesor .\n",
      "Actual Translation: Ella es una muy buena maestra .\n",
      "\n",
      "Input text: Your pronunciation is excellent .\n",
      "Predicted Translation: Su pronunciación es buena .\n",
      "Actual Translation: Vuestra pronunciación es excelente .\n",
      "\n",
      "Input text: There are not many options .\n",
      "Predicted Translation: No hay muchos opciones .\n",
      "Actual Translation: No hay muchas opciones .\n",
      "\n",
      "Input text: I am not the person I used to be .\n",
      "Predicted Translation: No soy la única persona que solía .\n",
      "Actual Translation: No soy la persona que solía ser .\n",
      "\n",
      "Input text: Give it to me now .\n",
      "Predicted Translation: Dámelo ahora .\n",
      "Actual Translation: Dámelo ahora .\n",
      "\n",
      "Input text: It is too dangerous for you to stay here .\n",
      "Predicted Translation: Aquí demasiado demasiado para quedarse aquí .\n",
      "Actual Translation: Es demasiado peligroso que te quedes aquí .\n",
      "\n",
      "Input text: You have to hurry .\n",
      "Predicted Translation: Tienes que esperar .\n",
      "Actual Translation: Tienes que darte prisa .\n",
      "\n",
      "Input text: I thought we had a deal .\n",
      "Predicted Translation: Pensé que teníamos algo .\n",
      "Actual Translation: Pensé que teníamos un trato .\n",
      "\n",
      "Input text: Tom was tortured .\n",
      "Predicted Translation: Tom estaba muerto .\n",
      "Actual Translation: Tom fue torturado .\n",
      "\n",
      "Input text: He died yesterday .\n",
      "Predicted Translation: Él murió ayer .\n",
      "Actual Translation: Murió ayer .\n",
      "\n",
      "Input text: It will be a difficult decision .\n",
      "Predicted Translation: Será difícil ser difícil .\n",
      "Actual Translation: Va a ser una difícil decisión .\n",
      "\n",
      "Input text: Your house is big .\n",
      "Predicted Translation: Tu casa es grande .\n",
      "Actual Translation: Vuestra casa es grande .\n",
      "\n",
      "Input text: I have to come on Monday .\n",
      "Predicted Translation: Tengo que empezar el lunes .\n",
      "Actual Translation: Tengo que venir el lunes .\n",
      "\n",
      "Input text: He picked up a handkerchief from the floor .\n",
      "Predicted Translation: Él recogió un anillo de la cesta .\n",
      "Actual Translation: Él recogió un pañuelo del suelo .\n",
      "\n",
      "Input text: What is happening to Tom ?\n",
      "Predicted Translation: ¿ Qué le va a Tom ?\n",
      "Actual Translation: ¿ Qué le está pasando a Tom ?\n",
      "\n",
      "Input text: I share a house with two of my friends .\n",
      "Predicted Translation: Comparto una hermana con mi hermana mayor .\n",
      "Actual Translation: Comparto una casa con dos de mis amigos .\n",
      "\n",
      "Input text: We all know why you did that .\n",
      "Predicted Translation: Todos sabemos por qué hicisteis eso .\n",
      "Actual Translation: Todas nosotras sabemos por qué hiciste eso .\n",
      "\n",
      "Input text: We want to go to Boston and Chicago .\n",
      "Predicted Translation: Queremos visitar Boston y Chicago .\n",
      "Actual Translation: Queremos ir de Boston a Chicago .\n",
      "\n",
      "Input text: I tried to pretend that I did not care .\n",
      "Predicted Translation: Intenté fingir que no me traía .\n",
      "Actual Translation: Traté de fingir que no me importaba .\n",
      "\n",
      "Input text: I saw a dog .\n",
      "Predicted Translation: Vi a un perro .\n",
      "Actual Translation: Vi a un perro .\n",
      "\n",
      "Input text: The deadline was yesterday .\n",
      "Predicted Translation: El lunes pasado ayer .\n",
      "Actual Translation: La fecha límite fue ayer .\n",
      "\n",
      "Input text: Could you stay a minute ?\n",
      "Predicted Translation: ¿ Podrías quedarte un minuto ?\n",
      "Actual Translation: ¿ Te puedes quedar un minuto ?\n",
      "\n",
      "Input text: Speak clearly .\n",
      "Predicted Translation: Habla claramente .\n",
      "Actual Translation: Habla claramente .\n",
      "\n",
      "Input text: Who ate all the pies ?\n",
      "Predicted Translation: ¿ Quién se comió todas las galletas ?\n",
      "Actual Translation: ¿ Quién se comió todos los pasteles ?\n",
      "\n",
      "Input text: Could I have some more coffee ?\n",
      "Predicted Translation: ¿ Podría tomar un poco más de café ?\n",
      "Actual Translation: ¿ Podría tener un poco más de café ?\n",
      "\n",
      "Input text: I will try to do my best .\n",
      "Predicted Translation: Intentaré intentar a hacer eso .\n",
      "Actual Translation: Intentaré dar lo mejor de mí mismo .\n",
      "\n",
      "Input text: Do not you have anything to say , Tom ?\n",
      "Predicted Translation: No le digas algo de que Tom , ¿ verdad ?\n",
      "Actual Translation: ¿ No tienes nada que decir , Tom ?\n",
      "\n",
      "Input text: Let us go !\n",
      "Predicted Translation: ¡ Órale !\n",
      "Actual Translation: Vamos .\n",
      "\n",
      "Input text: The boy told me why he was crying .\n",
      "Predicted Translation: El niño me dijo que me había llorando .\n",
      "Actual Translation: El niño me dijo por qué lloraba .\n",
      "\n",
      "Input text: I do not want to study French .\n",
      "Predicted Translation: No quiero estudiar francés .\n",
      "Actual Translation: No quiero estudiar francés .\n",
      "\n",
      "Input text: Tom was surprised by Mary behavior .\n",
      "Predicted Translation: Tom estaba enojado a Mary por John .\n",
      "Actual Translation: Tom quedó sorprendido con el comportamiento de Mary .\n",
      "\n",
      "Input text: I am very glad to meet you today .\n",
      "Predicted Translation: Hoy estoy muy contento de verte . .\n",
      "Actual Translation: Estoy muy feliz de encontrarte hoy .\n",
      "\n",
      "Input text: I am looking forward to working with you again .\n",
      "Predicted Translation: Estoy deseando volver a trabajar con ganas .\n",
      "Actual Translation: Estoy deseando volver a trabajar contigo .\n",
      "\n",
      "Input text: I think you are right .\n",
      "Predicted Translation: Creo que estás equivocado .\n",
      "Actual Translation: Creo que llevas razón .\n",
      "\n",
      "Input text: Soccer is an exciting sport .\n",
      "Predicted Translation: El fútbol es un deporte emocionante .\n",
      "Actual Translation: El fútbol es un deporte emocionante .\n",
      "\n",
      "Input text: Tom bought a pistol .\n",
      "Predicted Translation: Tom compró un arma .\n",
      "Actual Translation: Tom compró una pistola .\n",
      "\n",
      "Input text: Eat as much as you like .\n",
      "Predicted Translation: Come como si te gusta .\n",
      "Actual Translation: Come tanto como quieras .\n",
      "\n",
      "Validation BLEU Score: 0.275\n"
     ]
    }
   ],
   "source": [
    "from torchmetrics.text import BLEUScore\n",
    "\n",
    "bleu_score = BLEUScore()\n",
    "\n",
    "device = next(seq2seq.model.parameters()).device\n",
    "for batch_index, (in_sequences, out_sequences) in enumerate(test_loader):\n",
    "    in_sentences = unprocess(in_sequences.to(device), in_vocab)\n",
    "    pred_sequences = seq2seq.model.evaluate(in_sequences.to(device))\n",
    "    pred_sentences = unprocess(pred_sequences, out_vocab)\n",
    "    out_sentences = unprocess(out_sequences.to(device), out_vocab)\n",
    "    \n",
    "    bleu_score.update(pred_sentences, [[s] for s in out_sentences])\n",
    "\n",
    "    print(f\"Input text: {in_sentences[0]}\\n\" \n",
    "          + f\"Predicted Translation: {pred_sentences[0]}\\n\"\n",
    "          + f\"Actual Translation: {out_sentences[0]}\\n\")\n",
    "\n",
    "final_bleu = bleu_score.compute()\n",
    "print(f\"Validation BLEU Score: {final_bleu:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([109, 12, 9267])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = seq2seq.model.forward((in_sequences.to(device), out_sequences.to(device)))\n",
    "\n",
    "a.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py_env_dlcc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
