{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translating with a Recurrent Neural Networks\n",
    "\n",
    "This notebook provides the complete code example that implements a sequence-to-sequence (seq2seq) model for machine translation using recurrent neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Vocabularies\n",
    "\n",
    "Implement a function to tokenize and standardize text ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "tokenizers = {\"eng\": spacy.blank(\"en\"), \"spa\": spacy.blank(\"es\")}\n",
    "\n",
    "def tokenize(text, lang=\"eng\"):\n",
    "    \"\"\"Tokenize text.\"\"\"\n",
    "    tokens = tokenizers[lang](text)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'a', 'simple', 'example', '!']\n"
     ]
    }
   ],
   "source": [
    "print([token.text for token in tokenize(\"This is a simple example!\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import contractions, spacy\n",
    "\n",
    "tokenizers = {\"eng\": spacy.blank(\"en\"), \"spa\": spacy.blank(\"es\")}\n",
    "\n",
    "def tokenize(text, lang=\"eng\"):\n",
    "    \"\"\"Tokenize text.\"\"\"\n",
    "    text = contractions.fix(text) if lang == \"eng\" else text\n",
    "    tokens = tokenizers[lang](text)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'not', 'the', 'same', 'example', '!']\n"
     ]
    }
   ],
   "source": [
    "print([token.text for token in tokenize(\"This isn't the same example!\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import contractions, re, spacy, unicodedata\n",
    "\n",
    "tokenizers = {\"eng\": spacy.blank(\"en\"), \"spa\": spacy.blank(\"es\")}\n",
    "\n",
    "# Define the verbose pattern to enforce stricter token matching\n",
    "verbose_pattern = re.compile(\n",
    "    unicodedata.normalize(\"NFC\", r\"\"\"\n",
    "    ^[a-zA-Z0-9áéíóúüñÁÉÍÓÚÜÑ.,!?¡¿/:()]+  # 1+ allowed characters.\n",
    "    (-[a-zA-Z0-9áéíóúüñÁÉÍÓÚÜÑ.,!?¡¿/:()]+)*  # Optional hyphen plus chars.\n",
    "    (_[a-zA-Z0-9áéíóúüñÁÉÍÓÚÜÑ.,!?¡¿/:()]+)*  # Optional underscore plus chars.\n",
    "    $  # End of the string.\n",
    "    \"\"\"), re.VERBOSE\n",
    ")\n",
    "\n",
    "def tokenize(text, lang=\"eng\"):\n",
    "    \"\"\"Tokenize text with SpaCy, preserving monetary values and applying filters.\"\"\"\n",
    "\n",
    "    # Standardize text replacements (apostrophes, quotes)\n",
    "    replacements = {\"’\": \"'\", \"‘\": \"'\", \"“\": '\"', \"”\": '\"', \"´\": \"'\", \"´´\": '\"'}\n",
    "    for old, new in replacements.items():\n",
    "        text = text.replace(old, new)\n",
    "\n",
    "    # Replace special characters and expand contractions for English\n",
    "    text = contractions.fix(text) if lang == \"eng\" else text\n",
    "\n",
    "    # Tokenize the text with SpaCy\n",
    "    tokens = tokenizers[lang](text)\n",
    "\n",
    "    # Apply verbose pattern to filter tokens\n",
    "    filtered_tokens = [\n",
    "        token.text\n",
    "        for token in tokens if verbose_pattern.match(token.text)\n",
    "    ]\n",
    "    \n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Double', 'check', 'your', 'code', '!']\n"
     ]
    }
   ],
   "source": [
    "print([token for token in tokenize(\"Double-check your code!\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... a function to read and tokenize sentences by iterating through a corpus file ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corpus_iterator(filename, lang, lang_position):\n",
    "    \"\"\"Read and tokenize texts by iterating through a corpus file.\"\"\"\n",
    "    with open(filename, \"r\", encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            sentences = line.strip().split(\"\\t\")\n",
    "            sentence = unicodedata.normalize(\"NFC\", sentences[lang_position])\n",
    "            yield tokenize(sentence, lang)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... a function to build a vocabulary from a corpus file ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    \"\"\"Callable dictionary.\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_dict, unk_token=\"<unk>\"):\n",
    "        \"\"\"Initialize vocabulary\"\"\"\n",
    "        self.vocab_dict, self.unk_token = vocab_dict, unk_token\n",
    "        self.default_index = vocab_dict.get(unk_token, -1)\n",
    "        self.index_to_token = {idx: token for token, idx in vocab_dict.items()}\n",
    "        \n",
    "    def __call__(self, token_or_tokens):\n",
    "        \"\"\"Return the index(es) for given token or list of tokens.\"\"\"\n",
    "        if not isinstance(token_or_tokens, list):\n",
    "            return self.vocab_dict.get(token_or_tokens, self.default_index)\n",
    "        else:\n",
    "            return [self.vocab_dict.get(token, self.default_index) \n",
    "                    for token in token_or_tokens]\n",
    "    \n",
    "    def set_default_index(self, index):\n",
    "        \"\"\"Set default index for unknown tokens.\"\"\"\n",
    "        self.default_index = index\n",
    "\n",
    "    def lookup_token(self, index_or_indices):\n",
    "        \"\"\"Retrieve token corresponding to given index or list of indices.\"\"\"\n",
    "        if not isinstance(index_or_indices, list):\n",
    "            return self.index_to_token.get(int(index_or_indices), self.unk_token)\n",
    "        else:\n",
    "            return [self.index_to_token.get(int(index), self.unk_token) \n",
    "                    for index in index_or_indices]\n",
    "\n",
    "    def get_itos(self):\n",
    "        \"\"\"Return a list of tokens ordered by their index.\"\"\"\n",
    "        itos = [None] * len(self.index_to_token)\n",
    "        for index, token in self.index_to_token.items():\n",
    "            itos[index] = token\n",
    "        return itos\n",
    "        \n",
    "    def __iter__(self):\n",
    "        \"\"\"Iterate over the tokens in the vocabulary.\"\"\"\n",
    "        return iter(self.vocab_dict)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the number of tokens in the vocabulary.\"\"\"\n",
    "        return len(self.vocab_dict)\n",
    "    \n",
    "    def __contains__(self, token):\n",
    "        \"\"\"Check if a token is in the vocabulary.\"\"\"\n",
    "        return token in self.vocab_dict\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "class Vocab:\n",
    "    \"\"\"Wrapper around a dictionary to make it callable like torchtext's Vocab.\"\"\"\n",
    "    def __init__(self, vocab_dict, unk_token=\"<unk>\"):\n",
    "        self.vocab_dict = vocab_dict\n",
    "        self.unk_token = unk_token\n",
    "        self.default_index = vocab_dict.get(unk_token, -1)\n",
    "        self.index_to_token = {idx: token for token, idx in vocab_dict.items()}\n",
    "        \n",
    "    def __call__(self, token_or_tokens):\n",
    "        \"\"\"Make the vocab callable to return the index for a given token or list of tokens.\"\"\"\n",
    "        if isinstance(token_or_tokens, list):\n",
    "            return [self.vocab_dict.get(token, self.default_index) for token in token_or_tokens]\n",
    "        return self.vocab_dict.get(token_or_tokens, self.default_index)\n",
    "    \n",
    "    def set_default_index(self, index):\n",
    "        \"\"\"Set default index for unknown tokens.\"\"\"\n",
    "        self.default_index = index\n",
    "\n",
    "    def lookup_token(self, index_or_indices):\n",
    "        \"\"\"Retrieve the token corresponding to a given index or list of indices.\"\"\"\n",
    "        if isinstance(index_or_indices, list):\n",
    "            return [self.index_to_token.get(int(index), self.unk_token) for index in index_or_indices]\n",
    "        return self.index_to_token.get(int(index_or_indices), self.unk_token)\n",
    "\n",
    "    def get_itos(self):\n",
    "        \"\"\"Return a list of tokens ordered by their index.\"\"\"\n",
    "        itos = [None] * len(self.index_to_token)\n",
    "        for index, token in self.index_to_token.items():\n",
    "            itos[index] = token\n",
    "        return itos\n",
    "        \n",
    "    def __iter__(self):\n",
    "        \"\"\"Iterate over the tokens in the vocabulary.\"\"\"\n",
    "        return iter(self.vocab_dict)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the number of tokens in the vocabulary.\"\"\"\n",
    "        return len(self.vocab_dict)\n",
    "    \n",
    "    def __contains__(self, token):\n",
    "        \"\"\"Check if a token is in the vocabulary.\"\"\"\n",
    "        return token in self.vocab_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def build_vocab_from_iterator(iterator, specials=None, min_freq=1):\n",
    "    \"\"\"Build vocabulary from an iterator over tokenized sentences.\"\"\"\n",
    "    token_freq = Counter(token for tokens in iterator for token in tokens)\n",
    "    vocab, index = {}, 0\n",
    "    if specials: \n",
    "        for token in specials: \n",
    "            vocab[token] = index\n",
    "            index += 1\n",
    "    for token, freq in token_freq.items():\n",
    "        if freq >= min_freq:\n",
    "            vocab[token] = index\n",
    "            index += 1\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "from collections import Counter\n",
    "\n",
    "def build_vocab_from_iterator(iterator, specials=None, min_freq=1):\n",
    "    \"\"\"Build vocabulary from an iterator over tokenized sentences.\"\"\"\n",
    "    # Count the frequency of each token\n",
    "    counter = Counter(token for tokens in iterator for token in tokens)\n",
    "\n",
    "    # Initialize the vocabulary with special tokens\n",
    "    vocab = {}\n",
    "    index = 0\n",
    "\n",
    "    if specials:\n",
    "        for token in specials:\n",
    "            vocab[token] = index\n",
    "            index += 1\n",
    "\n",
    "    # Add tokens that meet the minimum frequency\n",
    "    for token, freq in counter.items():\n",
    "        if freq >= min_freq:\n",
    "            vocab[token] = index\n",
    "            index += 1\n",
    "\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(filename, lang, lang_position, specials=[\"<unk>\"], min_freq=5):\n",
    "    \"\"\"Build vocabulary.\"\"\"\n",
    "    vocab_dict = build_vocab_from_iterator(\n",
    "        corpus_iterator(filename, lang, lang_position), specials, min_freq,\n",
    "    )\n",
    "    vocab = Vocab(vocab_dict, unk_token=specials[0]) \n",
    "    vocab.set_default_index(vocab(specials[0]))\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "def build_vocab(filename, lang, lang_position, specials=None, min_freq=5):\n",
    "    if specials is None:\n",
    "        specials = [\"<unk>\"]\n",
    "    \n",
    "    vocab_dict = build_vocab_from_iterator(\n",
    "        corpus_iterator(filename, lang, lang_position),\n",
    "        min_freq=min_freq,\n",
    "        specials=specials,\n",
    "    )\n",
    "    vocab = Vocab(vocab_dict, unk_token=specials[0]) \n",
    "    vocab.set_default_index(vocab(specials[0]))\n",
    "    \n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and build the vocabularies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_lang, out_lang, filename = \"eng\", \"spa\", \"eng-spa.txt\"\n",
    "specials = [\"<pad>\", \"<sos>\", \"<eos>\", \"<unk>\"]\n",
    "\n",
    "in_vocab = build_vocab(filename, in_lang, lang_position=0, specials=specials)\n",
    "out_vocab = build_vocab(filename, out_lang, lang_position=1, specials=specials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the Data\n",
    "\n",
    "Implement a function to check if all words in a sentence are present in a vocabulary ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_words_in_vocab(sentence, vocab):\n",
    "    \"\"\"Check whether all words in a sentence are present in a vocabulary\"\"\"\n",
    "    return all(word in vocab for word in sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... a function to pad a sequence of tokens ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad(tokens, max_length=10):\n",
    "    \"\"\"Pad sequence of tokens.\"\"\"\n",
    "    padding_length = max_length - len(tokens)\n",
    "    return [\"<sos>\"] + tokens + [\"<eos>\"] + [\"<pad>\"] * padding_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... a function to process the language corpus ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def process(filename, in_lang, out_lang, in_vocab, out_vocab, max_length=10):\n",
    "    \"\"\"Process language corpus.\"\"\"\n",
    "    in_sequences, out_sequences = [], []\n",
    "    with open(filename, \"r\", encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            texts = line.strip().split(\"\\t\")\n",
    "            in_tokens = tokenize(unicodedata.normalize(\"NFC\", texts[0]), in_lang)\n",
    "            out_tokens = tokenize(unicodedata.normalize(\"NFC\", texts[1]), out_lang)\n",
    "\n",
    "            if (all_words_in_vocab(in_tokens, in_vocab)\n",
    "                and len(in_tokens) <= max_length\n",
    "                and all_words_in_vocab(out_tokens, out_vocab)\n",
    "                and len(out_tokens) <= max_length):\n",
    "                \n",
    "                padded_in_tokens = pad(in_tokens)\n",
    "                in_sequence = in_vocab(padded_in_tokens)\n",
    "                in_sequences.append(in_sequence)\n",
    "\n",
    "                padded_out_tokens = pad(out_tokens)\n",
    "                out_sequence = out_vocab(padded_out_tokens)\n",
    "                out_sequences.append(out_sequence)\n",
    "    return np.array(in_sequences), np.array(out_sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and build the datasets and data loaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deeptrack as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deeptrack as dt\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "in_sequences, out_sequences = \\\n",
    "    process(filename, in_lang, out_lang, in_vocab, out_vocab)\n",
    "\n",
    "sources = dt.sources.Source(inputs=in_sequences, targets=out_sequences)\n",
    "train_sources, test_sources = dt.sources.random_split(sources, [0.85, 0.15])\n",
    "\n",
    "inputs_pip = dt.Value(sources.inputs) >> dt.pytorch.ToTensor(dtype=torch.int)\n",
    "outputs_pip = dt.Value(sources.targets) >> dt.pytorch.ToTensor(dtype=torch.int)\n",
    "\n",
    "train_dataset = \\\n",
    "    dt.pytorch.Dataset(inputs_pip & outputs_pip, inputs=train_sources)\n",
    "test_dataset = \\\n",
    "    dt.pytorch.Dataset(inputs_pip & outputs_pip, inputs=test_sources)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing and Training the Sequence-to-Sequence Architecture\n",
    "\n",
    "Implement the encoder ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deeplay as dl\n",
    "\n",
    "class Seq2SeqEncoder(dl.DeeplayModule):\n",
    "    \"\"\"Sequence-to-sequence encoder.\"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, in_features=300, hidden_features=128,\n",
    "                 hidden_layers=1, dropout=0.0):\n",
    "        \"\"\"Initialize sequence-to-sequence encoder.\"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden_features = hidden_features\n",
    "        self.hidden_layers = hidden_layers\n",
    "        \n",
    "        self.embedding = dl.Layer(torch.nn.Embedding, vocab_size, in_features)\n",
    "        self.rnn = dl.Layer(torch.nn.GRU, input_size=in_features, \\\n",
    "            hidden_size=hidden_features, num_layers=hidden_layers, \\\n",
    "            dropout=(0 if hidden_layers == 1 else dropout), \\\n",
    "            bidirectional=True, batch_first=True)\n",
    "\n",
    "    def forward(self, in_sequences, contexts=None):\n",
    "        \"\"\"Perform forward pass.\"\"\"\n",
    "        in_embeddings = self.embedding(in_sequences)\n",
    "        encoded_sequences, contexts = self.rnn(in_embeddings, contexts)\n",
    "        encoded_sequences = (encoded_sequences[:, :, :self.hidden_features]\n",
    "                          + encoded_sequences[:, :, self.hidden_features:])\n",
    "        contexts = contexts[:self.hidden_layers]\n",
    "        return encoded_sequences, contexts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... implement the decoder ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqDecoder(dl.DeeplayModule):\n",
    "    \"\"\"Sequence-to-sequence decoder.\"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, in_features=300, hidden_features=128, \n",
    "                 hidden_layers=1, dropout=0.0):\n",
    "        \"\"\"Initialize sequence-to-sequence decoder.\"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = dl.Layer(torch.nn.Embedding, vocab_size, in_features)\n",
    "        self.rnn = dl.Layer(torch.nn.GRU, input_size=in_features, \\\n",
    "            hidden_size=hidden_features, num_layers=hidden_layers, \\\n",
    "            bidirectional=False, batch_first=True, \\\n",
    "            dropout=(0 if hidden_layers == 1 else dropout))\n",
    "        self.dense = dl.Layer(torch.nn.Linear, hidden_features, vocab_size)\n",
    "        self.softmax = dl.Layer(torch.nn.Softmax, dim=-1)\n",
    "\n",
    "    def forward(self, decoder_in_values, contexts):\n",
    "        \"\"\"Perform forward pass.\"\"\"\n",
    "        out_embeddings = self.embedding(decoder_in_values)\n",
    "        decoder_outputs, contexts = self.rnn(out_embeddings, contexts)\n",
    "        decoder_outputs = self.dense(decoder_outputs)\n",
    "        decoder_outputs = self.softmax(decoder_outputs)\n",
    "        return decoder_outputs, contexts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... implement the full seq2seq model combining the encoder and decoder ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqModel(dl.DeeplayModule):\n",
    "    \"\"\"Sequence-to-sequence model.\"\"\"\n",
    "\n",
    "    def __init__(self, in_vocab_size=None, out_vocab_size=None,\n",
    "                 embedding_dim=300, hidden_features=128, hidden_layers=1, \n",
    "                 dropout=0.0, teacher_prob=1.0):\n",
    "        \"\"\"Initialize the sequence-to-sequence model.\"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.in_vocab_size, self.out_vocab_size = in_vocab_size, out_vocab_size\n",
    "        self.encoder = Seq2SeqEncoder(in_vocab_size, embedding_dim, \\\n",
    "            hidden_features, hidden_layers, dropout)\n",
    "        self.decoder = Seq2SeqDecoder(out_vocab_size, embedding_dim, \\\n",
    "            hidden_features, hidden_layers, dropout)\n",
    "\n",
    "        self.teacher_prob = teacher_prob\n",
    "\n",
    "    def forward(self, batch):\n",
    "        \"\"\"Perform forward pass.\"\"\"\n",
    "        in_sequences, out_sequences = batch\n",
    "        num_sequences, sequence_length = in_sequences.size()\n",
    "        device = next(self.encoder.parameters()).device\n",
    "        \n",
    "        _, contexts = self.encoder(in_sequences)\n",
    "        \n",
    "        decoder_outputs_vec = torch.zeros(num_sequences, sequence_length,\n",
    "                                          self.out_vocab_size).to(device)\n",
    "        decoder_in_values = torch.full(size=(num_sequences, 1), \n",
    "                                       fill_value=1, device=device)  # <sos>\n",
    "        for t in range(sequence_length):\n",
    "            decoder_outputs, contexts = \\\n",
    "                self.decoder(decoder_in_values, contexts)\n",
    "            decoder_outputs_vec[:, t, :] = decoder_outputs.squeeze(1)\n",
    "            \n",
    "            if (np.random.rand() < self.teacher_prob \n",
    "                and t < sequence_length - 1):  # Teacher forcing.\n",
    "                decoder_in_values = \\\n",
    "                    out_sequences[:, t + 1].unsqueeze(-1).to(device)\n",
    "            else:  # Model prediction.\n",
    "                _, top_decoder_outputs = decoder_outputs.topk(1)\n",
    "                decoder_in_values = \\\n",
    "                    top_decoder_outputs.squeeze(-1).detach().to(device)  \n",
    "                  \n",
    "        return decoder_outputs_vec\n",
    "\n",
    "    def evaluate(self, in_sequences):\n",
    "        \"\"\"Evaluate model.\"\"\"\n",
    "        num_sequences, sequence_length = in_sequences.size()\n",
    "        device = next(self.encoder.parameters()).device\n",
    "\n",
    "        with torch.no_grad():\n",
    "            _, contexts = self.encoder(in_sequences)\n",
    "        \n",
    "        pred_sequences = torch.zeros(num_sequences, sequence_length).to(device)\n",
    "        decoder_in_values = torch.full(size=(num_sequences, 1), \n",
    "                                       fill_value=1, device=device)  # <sos>\n",
    "        for t in range(sequence_length):\n",
    "            with torch.no_grad():\n",
    "                decoder_outputs, contexts = \\\n",
    "                    self.decoder(decoder_in_values.to(device), contexts)\n",
    "            _, top_decoder_outputs = decoder_outputs.topk(1)\n",
    "            pred_sequences[:, t] = top_decoder_outputs.squeeze()\n",
    "            \n",
    "            decoder_in_values = top_decoder_outputs.squeeze(-1).detach()\n",
    "            \n",
    "        return pred_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... define the loss function ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maskedNLL(decoder_outputs, out_sequences, padding=0):\n",
    "    \"\"\"Calculate the masked negative log-likelihood (NLL) loss.\"\"\"\n",
    "    flat_pred_sequences = decoder_outputs.view(-1, decoder_outputs.shape[-1])\n",
    "    flat_target_sequences = out_sequences.view(-1, 1)\n",
    "    pred_probs = torch.gather(flat_pred_sequences, 1, flat_target_sequences)\n",
    "\n",
    "    nll = -torch.log(pred_probs)\n",
    "\n",
    "    mask = out_sequences != padding\n",
    "    masked_nll = nll.masked_select(mask.view(-1, 1))\n",
    "\n",
    "    return masked_nll.mean()  # Loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... implement the sequence-to-sequence application ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(dl.Application):\n",
    "    \"\"\"Application for the sequence-to-sequence model.\"\"\"\n",
    "\n",
    "    def __init__(self, in_vocab, out_vocab, teacher_prob=1.0):\n",
    "        \"\"\"Initialize the application.\"\"\"\n",
    "        super().__init__(loss=maskedNLL, optimizer=dl.Adam(lr=1e-3))\n",
    "        self.model = Seq2SeqModel(in_vocab_size=len(in_vocab), \\\n",
    "            out_vocab_size=len(out_vocab), teacher_prob=teacher_prob)\n",
    "\n",
    "    def train_preprocess(self, batch):\n",
    "        \"\"\"Adjust the target sequence by shifting it one position backward.\"\"\"\n",
    "        in_sequences, out_sequences = batch\n",
    "        shifted_out_sequences = \\\n",
    "            torch.cat((out_sequences[:, 1:], out_sequences[:, -1:]), dim=1) \n",
    "        return (in_sequences, out_sequences), shifted_out_sequences\n",
    "    \n",
    "    def forward(self, batch):\n",
    "        \"\"\"Perform forward pass.\"\"\"\n",
    "        return self.model(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... load some pretrained embeddings (glove.42B.300d.zip) from https://nlp.stanford.edu/projects/glove/ ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torchvision.datasets.utils import download_url, extract_archive\n",
    "\n",
    "glove_folder = os.path.join(\".\", \".glove_cache\")\n",
    "zip_filepath = os.path.join(glove_folder, \"glove.42B.300d.zip\")\n",
    "if not os.path.exists(glove_folder):\n",
    "    os.makedirs(glove_folder, exist_ok=True)\n",
    "    url = \"https://nlp.stanford.edu/data/glove.42B.300d.zip\"\n",
    "    download_url(url, glove_folder)\n",
    "    extract_archive(zip_filepath, glove_folder)\n",
    "    os.remove(zip_filepath)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "\n",
    "def download_glove_embeddings(glove_url, dest_folder='./.glove'):\n",
    "    \"\"\"Download GloVe embeddings from URL if not already downloaded and extracted.\"\"\"\n",
    "    \n",
    "    # File and directory paths\n",
    "    zip_file_path = os.path.join(dest_folder, 'glove.42B.300d.zip')\n",
    "    extracted_file_path = os.path.join(dest_folder, 'glove.42B.300d.txt')\n",
    "\n",
    "    # Create the destination folder if it doesn't exist\n",
    "    if not os.path.exists(dest_folder):\n",
    "        print(f\"Creating folder: {dest_folder}\")\n",
    "        os.makedirs(dest_folder, exist_ok=True)  # Create folder if it doesn't exist\n",
    "    \n",
    "    # Check if the file has already been extracted\n",
    "    if os.path.exists(extracted_file_path):\n",
    "        print(\"GloVe embeddings already extracted.\")\n",
    "        return extracted_file_path\n",
    "    \n",
    "    # Check if the zip file has already been downloaded\n",
    "    if not os.path.exists(zip_file_path):\n",
    "        print(\"Downloading GloVe embeddings...\")\n",
    "        response = requests.get(glove_url, stream=True)\n",
    "        with open(zip_file_path, 'wb') as f:\n",
    "            for chunk in response.iter_content(chunk_size=1024):\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "        print(\"Download complete.\")\n",
    "    else:\n",
    "        print(\"GloVe zip file already exists.\")\n",
    "    \n",
    "    # Extract the zip file\n",
    "    print(\"Extracting GloVe embeddings...\")\n",
    "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(dest_folder)\n",
    "    print(\"Extraction complete.\")\n",
    "    \n",
    "    return extracted_file_path\n",
    "\n",
    "# URL for GloVe embeddings\n",
    "glove_url = 'https://nlp.stanford.edu/data/glove.42B.300d.zip'\n",
    "\n",
    "# Call the function to download and extract GloVe embeddings\n",
    "glove_file_path = download_glove_embeddings(glove_url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove_embeddings(file_path):\n",
    "    \"\"\"Load GloVe embeddings.\"\"\"\n",
    "    glove_embeddings = {}\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            glove_embeddings[word] = np.round(\n",
    "                np.asarray(values[1:], dtype='float32'), decimals=6,\n",
    "            )\n",
    "    return glove_embeddings\n",
    "\n",
    "def get_glove_embeddings(vocab, glove_embeddings, embedding_dim):\n",
    "    \"\"\"Get embeddings for a vocabulary using GloVe.\"\"\"\n",
    "    embeddings_matrix = \\\n",
    "        torch.zeros((len(vocab), embedding_dim), dtype=torch.float32)\n",
    "    for i, token in enumerate(vocab):\n",
    "        embedding = glove_embeddings.get(token)\n",
    "        if embedding is None:\n",
    "            embedding = glove_embeddings.get(token.lower())\n",
    "        if embedding is not None:\n",
    "            embedding = torch.tensor(embedding, dtype=torch.float32)\n",
    "            embeddings_matrix[i] = embedding\n",
    "    return embeddings_matrix\n",
    "\n",
    "glove_file = os.path.join(glove_folder, \"glove.42B.300d.txt\")\n",
    "glove_embeddings, embedding_dim = load_glove_embeddings(glove_file), 300\n",
    "\n",
    "glove_embeddings_in = get_glove_embeddings(in_vocab.get_itos(), \n",
    "                                           glove_embeddings, embedding_dim)\n",
    "glove_embeddings_out = get_glove_embeddings(out_vocab.get_itos(), \n",
    "                                            glove_embeddings, embedding_dim)\n",
    "\n",
    "num_specials = len(specials)\n",
    "glove_embeddings_in[1:num_specials] = \\\n",
    "    torch.rand(num_specials - 1, embedding_dim) * 0.01\n",
    "glove_embeddings_out[1:num_specials] = \\\n",
    "    torch.rand(num_specials - 1, embedding_dim) * 0.01"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "def load_glove_embeddings(file_path):\n",
    "    \"\"\"Load GloVe embeddings from a .npy file if it exists, otherwise load from .txt and save to .npy.\"\"\"\n",
    "    \n",
    "    # Derive the .npy file path from the .txt file path\n",
    "    npy_file_path = file_path.replace('.txt', '.npy')\n",
    "\n",
    "    # If the .npy file exists, load it\n",
    "    if os.path.exists(npy_file_path):\n",
    "        print(f\"Loading GloVe embeddings from {npy_file_path}...\")\n",
    "        glove_embeddings = np.load(npy_file_path, allow_pickle=True).item()  # Load the dictionary stored in .npy\n",
    "    else:\n",
    "        # Otherwise, load from the .txt file and save to .npy\n",
    "        print(f\"Loading GloVe embeddings from {file_path} ...\")\n",
    "        glove_embeddings = {}\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                values = line.split()\n",
    "                word = values[0]\n",
    "                glove_embeddings[word] = np.round(np.asarray(values[1:], dtype='float32'), decimals=6)\n",
    "\n",
    "        # Save the loaded embeddings as a .npy file\n",
    "        print(f\"Saving GloVe embeddings to {npy_file_path} ...\")\n",
    "        np.save(npy_file_path, glove_embeddings)  # Save dictionary to .npy\n",
    "\n",
    "    return glove_embeddings\n",
    "\n",
    "def get_glove_embeddings(vocab, glove_embeddings, embedding_dim):\n",
    "    \"\"\"Get embeddings for a vocabulary using GloVe.\"\"\"\n",
    "    embeddings_matrix = torch.zeros((len(vocab), embedding_dim), dtype=torch.float32)\n",
    "    \n",
    "    for idx, token in enumerate(vocab):\n",
    "        embedding = glove_embeddings.get(token)\n",
    "        if embedding is None:\n",
    "            embedding = glove_embeddings.get(token.lower())\n",
    "        if embedding is not None:\n",
    "            embedding = torch.tensor(embedding, dtype=torch.float32)\n",
    "            embeddings_matrix[idx] = embedding  \n",
    "    \n",
    "    return embeddings_matrix\n",
    "\n",
    "glove_file_path = './.glove/glove.42B.300d.txt'\n",
    "\n",
    "embedding_dim = 300\n",
    "\n",
    "glove_embeddings = load_glove_embeddings(glove_file_path)\n",
    "\n",
    "glove_embeddings_in = get_glove_embeddings(in_vocab.get_itos(), glove_embeddings, embedding_dim)\n",
    "glove_embeddings_out = get_glove_embeddings(out_vocab.get_itos(), glove_embeddings, embedding_dim)\n",
    "\n",
    "num_specials = len(specials)\n",
    "glove_embeddings_in[1:num_specials] = \\\n",
    "    torch.rand(num_specials - 1, embedding_dim) * 0.01\n",
    "glove_embeddings_out[1:num_specials] = \\\n",
    "    torch.rand(num_specials - 1, embedding_dim) * 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... instantiate the seq2seq model ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq2seq = Seq2Seq(in_vocab=in_vocab, out_vocab=out_vocab, teacher_prob=0.85)\n",
    "seq2seq = seq2seq.create()\n",
    "\n",
    "seq2seq.model.encoder.embedding.weight.data = glove_embeddings_in\n",
    "seq2seq.model.encoder.embedding.weight.requires_grad = False\n",
    "seq2seq.model.decoder.embedding.weight.data = glove_embeddings_out\n",
    "seq2seq.model.decoder.embedding.weight.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and train the model ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/giovannivolpe/Documents/GitHub/DeepLearningCrashCourse/py_env_book/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:75: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "/Users/giovannivolpe/Documents/GitHub/DeepLearningCrashCourse/py_env_book/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n",
       "┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name          </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type             </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Mode  </span>┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span>│ train_metrics │ MetricCollection │      0 │ train │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 </span>│ val_metrics   │ MetricCollection │      0 │ train │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2 </span>│ test_metrics  │ MetricCollection │      0 │ train │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3 </span>│ model         │ Seq2SeqModel     │  6.3 M │ train │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 4 </span>│ optimizer     │ Adam             │      0 │ train │\n",
       "└───┴───────────────┴──────────────────┴────────┴───────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n",
       "┃\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mName         \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mType            \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mMode \u001b[0m\u001b[1;35m \u001b[0m┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n",
       "│\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m│ train_metrics │ MetricCollection │      0 │ train │\n",
       "│\u001b[2m \u001b[0m\u001b[2m1\u001b[0m\u001b[2m \u001b[0m│ val_metrics   │ MetricCollection │      0 │ train │\n",
       "│\u001b[2m \u001b[0m\u001b[2m2\u001b[0m\u001b[2m \u001b[0m│ test_metrics  │ MetricCollection │      0 │ train │\n",
       "│\u001b[2m \u001b[0m\u001b[2m3\u001b[0m\u001b[2m \u001b[0m│ model         │ Seq2SeqModel     │  6.3 M │ train │\n",
       "│\u001b[2m \u001b[0m\u001b[2m4\u001b[0m\u001b[2m \u001b[0m│ optimizer     │ Adam             │      0 │ train │\n",
       "└───┴───────────────┴──────────────────┴────────┴───────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 1.7 M                                                                                            \n",
       "<span style=\"font-weight: bold\">Non-trainable params</span>: 4.6 M                                                                                        \n",
       "<span style=\"font-weight: bold\">Total params</span>: 6.3 M                                                                                                \n",
       "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 25                                                                         \n",
       "<span style=\"font-weight: bold\">Modules in train mode</span>: 13                                                                                          \n",
       "<span style=\"font-weight: bold\">Modules in eval mode</span>: 0                                                                                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mTrainable params\u001b[0m: 1.7 M                                                                                            \n",
       "\u001b[1mNon-trainable params\u001b[0m: 4.6 M                                                                                        \n",
       "\u001b[1mTotal params\u001b[0m: 6.3 M                                                                                                \n",
       "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 25                                                                         \n",
       "\u001b[1mModules in train mode\u001b[0m: 13                                                                                          \n",
       "\u001b[1mModules in eval mode\u001b[0m: 0                                                                                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1807dad97be49339e0a72039152badf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/giovannivolpe/Documents/GitHub/DeepLearningCrashCourse/py_env_book/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=10` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = dl.Trainer(max_epochs=25, accelerator=\"auto\")\n",
    "trainer.fit(seq2seq, train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Model Perfomance\n",
    "\n",
    "Implement a function to convert numerical sequences into their corresponding text ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unprocess(sequences, vocab, specials):\n",
    "    \"\"\"Convert numeric sequences to sentences.\"\"\"\n",
    "    sentences = []\n",
    "    for sequence in sequences:\n",
    "        idxs = sequence[sequence > len(specials) - 1]\n",
    "        words = [vocab.lookup_token(idx) for idx in idxs]\n",
    "        sentences.append(\" \".join(words))\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... a function to translate user-defined sentences ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(in_sentence, model, in_lang, in_vocab, out_vocab, specials):\n",
    "    \"\"\"Translate a sentence.\"\"\"\n",
    "    in_sentence = unicodedata.normalize(\"NFC\", in_sentence)\n",
    "    in_tokens = pad(tokenize(in_sentence, in_lang))\n",
    "    in_sequence = (torch.tensor(in_vocab(in_tokens), dtype=torch.int)\n",
    "                   .unsqueeze(0).to(next(model.parameters()).device))\n",
    "    pred_sequence = model.evaluate(in_sequence)\n",
    "    pred_sentence = unprocess(pred_sequence, out_vocab, specials)\n",
    "    print(f\"Predicted Translation: {pred_sentence[0]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... try to translate a simple sentence ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Translation: Compré un libro .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "in_sentence = \"I bought a book.\"\n",
    "translate(in_sentence, seq2seq.model, in_lang, in_vocab, out_vocab, specials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... another simple sentence ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Translation: Este libro es muy interesante .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "in_sentence = \"This book is very interesting.\"\n",
    "translate(in_sentence, seq2seq.model, in_lang, in_vocab, out_vocab, specials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and a more complex one ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Translation: El libro de libro es muy interesante .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "in_sentence = \"The book that I bought is very interesting.\"\n",
    "translate(in_sentence, seq2seq.model, in_lang, in_vocab, out_vocab, specials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Model with the BLEU Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text: You are not Japanese .\n",
      "Predicted Translation: No eres japonés .\n",
      "Actual Translation: Tú no eres japonesa .\n",
      "\n",
      "Input text: Tom forgot to lock the front door .\n",
      "Predicted Translation: Tom olvidó ponerle la llave al grano .\n",
      "Actual Translation: Tom olvidó ponerle llave a la puerta del frente .\n",
      "\n",
      "Input text: Your nose is bleeding .\n",
      "Predicted Translation: Tu ojo está sangrando .\n",
      "Actual Translation: Le está sangrando la nariz .\n",
      "\n",
      "Input text: I have a large collection of stamps .\n",
      "Predicted Translation: Tengo una gran traje de cuero .\n",
      "Actual Translation: Tengo una gran colección de estampillas .\n",
      "\n",
      "Input text: She betrayed you .\n",
      "Predicted Translation: Ella te traicionó .\n",
      "Actual Translation: Ella te traicionó .\n",
      "\n",
      "Input text: I will side with you just this once .\n",
      "Predicted Translation: Estaré en esta vez a la manera de ti .\n",
      "Actual Translation: Seré tu aliado sólo por esta vez .\n",
      "\n",
      "Input text: He came here again .\n",
      "Predicted Translation: Él vino aquí .\n",
      "Actual Translation: Volvió a venir .\n",
      "\n",
      "Input text: I am glad you are here .\n",
      "Predicted Translation: Me alegro de que estés aquí .\n",
      "Actual Translation: Me alegro de que estés aquí .\n",
      "\n",
      "Input text: Tom jumped off a cliff .\n",
      "Predicted Translation: Tom saltó a un un traidor .\n",
      "Actual Translation: Tom saltó de un precipicio .\n",
      "\n",
      "Input text: We have a good heating system .\n",
      "Predicted Translation: Tenemos un buen buen humor .\n",
      "Actual Translation: Tenemos un buen sistema de calefacción .\n",
      "\n",
      "Input text: The mystery of her death was never solved .\n",
      "Predicted Translation: La niña del accidente de su mujer murió .\n",
      "Actual Translation: El misterio de su muerte nunca fue resuelto .\n",
      "\n",
      "Input text: There is an urgent need for new ideas .\n",
      "Predicted Translation: Hay un necesidad urgente de comprar nuevas .\n",
      "Actual Translation: Se necesitan urgentemente nuevas ideas .\n",
      "\n",
      "Input text: He worked for a rich man .\n",
      "Predicted Translation: Él trabajó un hombre rico .\n",
      "Actual Translation: Él trabajó para un hombre rico .\n",
      "\n",
      "Input text: The fridge is empty .\n",
      "Predicted Translation: La despensa está vacía .\n",
      "Actual Translation: La heladera está vacía .\n",
      "\n",
      "Input text: I accept your apology .\n",
      "Predicted Translation: Acepto tus disculpas .\n",
      "Actual Translation: Acepto sus disculpas .\n",
      "\n",
      "Input text: I need sleep .\n",
      "Predicted Translation: Necesito dormir .\n",
      "Actual Translation: Necesito dormir .\n",
      "\n",
      "Input text: Who can tell me how a light bulb works ?\n",
      "Predicted Translation: ¿ Quién me puede contar una foto de historia ?\n",
      "Actual Translation: ¿ Quién puede decirme cómo funciona una bombilla ?\n",
      "\n",
      "Input text: May I use your toilet ?\n",
      "Predicted Translation: ¿ Puedo usar tu coche ?\n",
      "Actual Translation: ¿ Podría usar tu baño ?\n",
      "\n",
      "Input text: You have to ask for permission from your teacher .\n",
      "Predicted Translation: Debes pedirle a tu profesor .\n",
      "Actual Translation: Debes pedirle el permiso a tu profesor .\n",
      "\n",
      "Input text: This is our home .\n",
      "Predicted Translation: Esta es nuestro casa .\n",
      "Actual Translation: Esa es nuestra casa .\n",
      "\n",
      "Input text: Mary is getting her nails done .\n",
      "Predicted Translation: Mary se está poniendo las uñas .\n",
      "Actual Translation: Mary se está haciendo las uñas .\n",
      "\n",
      "Input text: Maybe he will not become famous .\n",
      "Predicted Translation: Quizás él no sabe inglés .\n",
      "Actual Translation: Quizás él nunca se haga famoso .\n",
      "\n",
      "Input text: I am sure Tom will be able to help .\n",
      "Predicted Translation: Estoy seguro de que Tom podrá ayudar .\n",
      "Actual Translation: Estoy seguro de que Tom podrá ayudar .\n",
      "\n",
      "Input text: Can you give an exact report of what happened ?\n",
      "Predicted Translation: ¿ Puedes hacer lo que ocurrió este fin ?\n",
      "Actual Translation: ¿ Puede usted describir exactamente lo que pasó ?\n",
      "\n",
      "Input text: Give it to me now .\n",
      "Predicted Translation: Ahora me . .\n",
      "Actual Translation: Dámelo ahora .\n",
      "\n",
      "Input text: Do I need to explain it to you again ?\n",
      "Predicted Translation: ¿ Puedo hacerlo por esto ?\n",
      "Actual Translation: ¿ Te lo tengo que explicar de nuevo ?\n",
      "\n",
      "Input text: He did nothing but cry .\n",
      "Predicted Translation: Él no tenía nada llorar .\n",
      "Actual Translation: Él no hacía más que llorar .\n",
      "\n",
      "Input text: Tom is a soldier .\n",
      "Predicted Translation: Tom es un soldado .\n",
      "Actual Translation: Tom es soldado .\n",
      "\n",
      "Input text: It is almost six of the clock .\n",
      "Predicted Translation: Son casi las seis .\n",
      "Actual Translation: Son casi las seis .\n",
      "\n",
      "Input text: I know that you are in love with me .\n",
      "Predicted Translation: Sé que estás enamorada de mí .\n",
      "Actual Translation: Sé que usted está enamorado de mí .\n",
      "\n",
      "Input text: Which pair of shoes did you decide to buy ?\n",
      "Predicted Translation: ¿ Qué de estos zapatos compraste estos zapatos ?\n",
      "Actual Translation: ¿ Por qué zapatos te decidiste ?\n",
      "\n",
      "Input text: You can use that phone .\n",
      "Predicted Translation: Puedes usar tu teléfono .\n",
      "Actual Translation: Puedes ocupar ese teléfono .\n",
      "\n",
      "Input text: Tom wrote a letter to Mary this morning .\n",
      "Predicted Translation: Tom escribió algo una carta en Mary en esta mañana .\n",
      "Actual Translation: Tom escribió una carta a Mary esta mañana .\n",
      "\n",
      "Input text: Is Tom still here ?\n",
      "Predicted Translation: ¿ Todavía está aquí Tom ?\n",
      "Actual Translation: ¿ Tom sigue aquí ?\n",
      "\n",
      "Input text: How can I make a long distance call ?\n",
      "Predicted Translation: ¿ Cómo puedo esperar más que yo ?\n",
      "Actual Translation: ¿ Cómo puedo hacer una llamada a larga distancia ?\n",
      "\n",
      "Input text: Cats are smart .\n",
      "Predicted Translation: Los gatos son inteligentes .\n",
      "Actual Translation: Los gatos son inteligentes .\n",
      "\n",
      "Input text: I could not convince him of his mistake .\n",
      "Predicted Translation: No pude evitar reírme .\n",
      "Actual Translation: No pude sacarle de su error .\n",
      "\n",
      "Input text: Let us begin with that question .\n",
      "Predicted Translation: Veamos el ordenador .\n",
      "Actual Translation: Comencemos por esa pregunta .\n",
      "\n",
      "Input text: I already speak French .\n",
      "Predicted Translation: Yo hablo francés .\n",
      "Actual Translation: Ya hablo francés .\n",
      "\n",
      "Input text: How can you be so optimistic ?\n",
      "Predicted Translation: ¿ Cómo tan tan estar tan ?\n",
      "Actual Translation: ¿ Cómo puedes ser tan optimista ?\n",
      "\n",
      "Input text: That is not what Tom meant .\n",
      "Predicted Translation: Eso no es lo que Tom es lo .\n",
      "Actual Translation: Eso no es lo que Tom quiso decir .\n",
      "\n",
      "Input text: Am I fat ?\n",
      "Predicted Translation: ¿ Estoy gordo ?\n",
      "Actual Translation: ¿ Estoy gorda ?\n",
      "\n",
      "Input text: What do you think of these shoes ?\n",
      "Predicted Translation: ¿ Qué opinas de zapatos ?\n",
      "Actual Translation: ¿ Qué tal te parecen estos zapatos ?\n",
      "\n",
      "Input text: Mary is afraid of men .\n",
      "Predicted Translation: A Tom le dan miedo los hombres .\n",
      "Actual Translation: A Mary los hombres le dan miedo .\n",
      "\n",
      "Input text: I have only a small garden .\n",
      "Predicted Translation: Solo necesito un jardín .\n",
      "Actual Translation: Solo tengo un jardín pequeño .\n",
      "\n",
      "Input text: I was made to wait for a long time .\n",
      "Predicted Translation: Estuve esperar mucho tiempo .\n",
      "Actual Translation: Me hicieron esperar un buen rato .\n",
      "\n",
      "Input text: Our director is a Canadian .\n",
      "Predicted Translation: Nuestro equipo es canadiense .\n",
      "Actual Translation: Nuestro director es canadiense .\n",
      "\n",
      "Input text: Tom gave up hope .\n",
      "Predicted Translation: Tom se dio a esperanzas .\n",
      "Actual Translation: Tom perdió las esperanzas .\n",
      "\n",
      "Input text: It is a quarter past eight .\n",
      "Predicted Translation: Es una de menos tres de octubre .\n",
      "Actual Translation: Son las ocho y cuarto .\n",
      "\n",
      "Input text: Where do you think Tom is ?\n",
      "Predicted Translation: ¿ Dónde crees que Tom Tom ?\n",
      "Actual Translation: ¿ Dónde crees que esté Tom ?\n",
      "\n",
      "Input text: Are you a criminal ?\n",
      "Predicted Translation: ¿ Eres poli ?\n",
      "Actual Translation: ¿ Eres un criminal ?\n",
      "\n",
      "Input text: Why not both ?\n",
      "Predicted Translation: ¿ Por qué no ambas ?\n",
      "Actual Translation: ¿ Por qué no ambos ?\n",
      "\n",
      "Input text: Do you want to know what I would say ?\n",
      "Predicted Translation: ¿ Quieres saber qué quieres decir ?\n",
      "Actual Translation: ¿ Quieres saber lo que yo diría ?\n",
      "\n",
      "Input text: They do not want you to know .\n",
      "Predicted Translation: No quieren saber qué quieren .\n",
      "Actual Translation: No quieren que lo sepas .\n",
      "\n",
      "Validation BLEU Score: 0.257\n"
     ]
    }
   ],
   "source": [
    "from torchmetrics.text import BLEUScore\n",
    "\n",
    "bleu_score = BLEUScore()\n",
    "\n",
    "device = next(seq2seq.model.parameters()).device\n",
    "for batch_index, (in_sequences, out_sequences) in enumerate(test_loader):\n",
    "    in_sentences = unprocess(in_sequences.to(device), in_vocab, specials)\n",
    "    pred_sequences = seq2seq.model.evaluate(in_sequences.to(device))\n",
    "    pred_sentences = unprocess(pred_sequences, out_vocab, specials)\n",
    "    out_sentences = unprocess(out_sequences.to(device), out_vocab, specials)\n",
    "    \n",
    "    bleu_score.update(pred_sentences, [[s] for s in out_sentences])\n",
    "\n",
    "    print(f\"Input text: {in_sentences[0]}\\n\" \n",
    "          + f\"Predicted Translation: {pred_sentences[0]}\\n\"\n",
    "          + f\"Actual Translation: {out_sentences[0]}\\n\")\n",
    "\n",
    "final_bleu = bleu_score.compute()\n",
    "print(f\"Validation BLEU Score: {final_bleu:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py_env_book",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
