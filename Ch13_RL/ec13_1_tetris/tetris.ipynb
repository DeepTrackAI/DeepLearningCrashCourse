{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Teaching a Deep-Q-Learning Agent to Play Tetris\n",
    "\n",
    "<div style=\"background-color: #f0f8ff; border: 2px solid #4682b4; padding: 10px;\">\n",
    "<a href=\"https://colab.research.google.com/github/DeepTrackAI/DeepLearningCrashCourse/blob/main/Ch013_RL/ec13_1_tetris/tetris.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "<strong>If using Colab/Kaggle:</strong> You need to uncomment the code in the cell below this one.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install deeplay  # Uncomment if using Colab/Kaggle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook provides you with a complete code example that trains a train a deep reinforcement learning model to play Tetris."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #f0f8ff; border: 2px solid #4682b4; padding: 10px;\">\n",
    "<strong>Note:</strong> This notebook contains the Code Example 13-1 from the book  \n",
    "\n",
    "**Deep Learning Crash Course**  \n",
    "Benjamin Midtvedt, Jes√∫s Pineda, Henrik Klein Moberg, Harshith Bachimanchi, Joana B. Pereira, Carlo Manzo, Giovanni Volpe  \n",
    "No Starch Press, San Francisco (CA), 2025  \n",
    "ISBN-13: 9781718503922  \n",
    "\n",
    "[https://nostarch.com/deep-learning-crash-course](https://nostarch.com/deep-learning-crash-course)\n",
    "\n",
    "You can find the other notebooks on the [Deep Learning Crash Course GitHub page](https://github.com/DeepTrackAI/DeepLearningCrashCourse).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing a Simplified Tetris"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a class with a simplifed Tetris ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "class Tetris:\n",
    "    \"\"\"Simplified Tetris.\"\"\"\n",
    "\n",
    "    TILES = [\n",
    "        [\n",
    "            [[0, 2]],   # Tile 0, orientation 0.\n",
    "            [[0, 1], [0, 1]],   # Tile 0, orientation 1.\n",
    "        ],\n",
    "        [\n",
    "            [[1, 2], [0, 1]],  # Tile 1, orientation 0.\n",
    "            [[0, 1], [1, 2]],  # Tile 1, orientation 1.\n",
    "        ],\n",
    "        [\n",
    "            [[0, 2], [0, 1]],  # Tile 2, orientation 0.\n",
    "            [[0, 2], [1, 2]],  # Tile 2, orientation 1.\n",
    "            [[1, 2], [0, 2]],  # Tile 2, orientation 2.\n",
    "            [[0, 1], [0, 2]],  # Tile 2, orientation 3.\n",
    "        ],\n",
    "        [\n",
    "            [[0, 2], [0, 2]],  # Tile 3, orientation 0.\n",
    "        ],\n",
    "    ]\n",
    "\n",
    "    UNDEFINED = -1\n",
    "\n",
    "    def __init__(self, rows, cols, max_tiles, random_seed):\n",
    "        \"\"\"Initialize Tetris.\"\"\"\n",
    "        self.rows, self.cols = rows, cols\n",
    "        self.max_tiles = max_tiles\n",
    "        self.random_seed = random_seed\n",
    "\n",
    "        self.restart()\n",
    "\n",
    "    def restart(self):\n",
    "        \"\"\"Restart the game.\"\"\"\n",
    "        self.board = np.full((self.rows, self.cols), Tetris.UNDEFINED)\n",
    "        self.current_tile = Tetris.UNDEFINED\n",
    "        self.tile_x = Tetris.UNDEFINED\n",
    "        self.tile_y = Tetris.UNDEFINED\n",
    "        self.tile_orientation = Tetris.UNDEFINED\n",
    "\n",
    "        self.gameover = False\n",
    "        self.tile_count = 0\n",
    "\n",
    "        # Create predefined tile sequence.\n",
    "        rand_state = random.getstate()\n",
    "        random.seed(self.random_seed)\n",
    "        self.tile_sequence = [random.randint(0, len(Tetris.TILES) - 1)\n",
    "                              for _ in range(self.max_tiles)]\n",
    "        random.setstate(rand_state)\n",
    "\n",
    "        self.reward = 0\n",
    "\n",
    "        self.next_tile()\n",
    "\n",
    "    def next_tile(self):\n",
    "        \"\"\"Get the next tile.\"\"\"\n",
    "        if self.tile_count < self.max_tiles:\n",
    "            if self.random_seed is not None:\n",
    "                self.current_tile = self.tile_sequence[self.tile_count]\n",
    "            else:\n",
    "                self.current_tile = random.randint(0, len(Tetris.TILES) - 1)\n",
    "\n",
    "            self.tile_x = self.cols // 2\n",
    "            self.tile_y = self.rows\n",
    "            self.tile_orientation = 0\n",
    "\n",
    "            self.tile_count += 1\n",
    "        else:\n",
    "            self.gameover = True\n",
    "\n",
    "    def move_left(self):\n",
    "        \"\"\"Move current tile to the left.\"\"\"\n",
    "        if self.tile_x - 1 >= 0:\n",
    "            self.tile_x -= 1\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def move_right(self):\n",
    "        \"\"\"Move current tile to the right.\"\"\"\n",
    "        tilewidth = len(Tetris.TILES[self.current_tile][self.tile_orientation])\n",
    "        if self.tile_x + 1 <= self.cols - tilewidth:\n",
    "            self.tile_x += 1\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def rotate(self):\n",
    "        \"\"\"Rotate current tile.\"\"\"\n",
    "        new_orientation = ((self.tile_orientation + 1)\n",
    "                           % len(Tetris.TILES[self.current_tile]))\n",
    "        tilewidth = len(Tetris.TILES[self.current_tile][new_orientation])\n",
    "        if self.tile_x <= self.cols - tilewidth:\n",
    "            self.tile_orientation = new_orientation\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def drop(self):\n",
    "        \"\"\"Drop current tile and update game board.\"\"\"\n",
    "        tile = Tetris.TILES[self.current_tile][self.tile_orientation]\n",
    "\n",
    "        # Find first location where the tile collides with occupied locations.\n",
    "        self.tile_y = 0\n",
    "        for tile_column in range(len(tile)):\n",
    "            # Tile final y position for this tile column\n",
    "            # if no other columns are taken into account.\n",
    "            tile_y = -1\n",
    "            for y in range(self.rows - 1, -1, -1):\n",
    "                if self.board[y, self.tile_x + tile_column] > 0:\n",
    "                    tile_y = y + 1 - tile[tile_column][0]\n",
    "                    break\n",
    "            # Update tile y position.\n",
    "            if tile_y > self.tile_y:\n",
    "                self.tile_y = tile_y\n",
    "\n",
    "        if self.tile_y + np.max(tile) > self.rows:\n",
    "            self.gameover = True\n",
    "            dreward = -100\n",
    "        else:\n",
    "            # Change board entries at the newly placed tile to occupied.\n",
    "            for tile_column in range(len(tile)):\n",
    "                self.board[self.tile_y + tile[tile_column][0]\n",
    "                        :self.tile_y + tile[tile_column][1],\n",
    "                        tile_column + self.tile_x] = 1\n",
    "\n",
    "            # Remove full lines.\n",
    "            removed_lines = 0\n",
    "            for y in range(self.rows - 1, -1, -1):\n",
    "                if np.sum(self.board[y, :]) == self.cols:\n",
    "                    removed_lines += 1\n",
    "                    for y1 in range(y, self.rows - 1):\n",
    "                        self.board[y1, :] = self.board[y1 + 1, :]\n",
    "                    self.board[self.rows - 1, :] = Tetris.UNDEFINED\n",
    "\n",
    "            dreward = 10 ** (removed_lines - 1) if removed_lines > 0 else 0\n",
    "\n",
    "        self.next_tile()\n",
    "        self.reward += dreward\n",
    "        return dreward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and create an instance of this simplified Tetris."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tetris = Tetris(rows=4, cols=4, max_tiles=50, random_seed=123456)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playing Tetris with the Command Line"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "while not tetris.gameover:\n",
    "    print(f\"Tile {tetris.tile_count}/{tetris.max_tiles}\")\n",
    "    print(f\"Reward: {tetris.reward}\")\n",
    "    print(f\"Current tile {tetris.current_tile} with \"\n",
    "          f\"orientation {tetris.tile_orientation} at position {tetris.tile_x}\")\n",
    "    print(tetris.TILES[tetris.current_tile][tetris.tile_orientation])\n",
    "    print(tetris.board)\n",
    "\n",
    "    cmd = input(\"Please enter your command (L, R, O, D, X): \").upper()\n",
    "    print(f\"Your input: {cmd} \\n\")\n",
    "\n",
    "    if cmd == \"L\":\n",
    "        tetris.move_left()\n",
    "    elif cmd == \"R\":\n",
    "        tetris.move_right()\n",
    "    elif cmd == \"O\":\n",
    "        tetris.rotate()\n",
    "    elif cmd == \"D\":\n",
    "        tetris.drop()\n",
    "    elif cmd == \"X\":\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playing Tetris with a Graphical Interface with Pygame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a function to play Tetris with a GUI ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygame\n",
    "\n",
    "def play_tetris_with_gui(tetris):\n",
    "    \"\"\"Play Tetris with GUI for human players.\"\"\"\n",
    "    TILE_SIZE = 20\n",
    "    BLACK = (0, 0, 0)  # RGB code for black color.\n",
    "    GREY = (128, 128, 128)  # RGB code for grey color.\n",
    "    WHITE = (255, 255, 255)  # RGB code for white color.\n",
    "    RED = (255, 0, 0)  # RGB code for red color.\n",
    "\n",
    "    # Initialize the game engine.\n",
    "    pygame.init()\n",
    "    pygame.display.set_caption(\"TETRIS\")\n",
    "    screen = pygame.display.set_mode((200 + tetris.cols * TILE_SIZE,\n",
    "                                      200 + tetris.rows * TILE_SIZE))\n",
    "    pygame.key.set_repeat(300, 100)  # Set keyboard delay and interval in ms.\n",
    "    font = pygame.font.SysFont(\"Calibri\", 25, True)\n",
    "\n",
    "    # Loop until the window is closed.\n",
    "    running = True\n",
    "    while running:\n",
    "        # Paint game board.\n",
    "        if pygame.display.get_active():\n",
    "            screen.fill(WHITE)\n",
    "\n",
    "            for i in range(tetris.rows):\n",
    "                for j in range(tetris.cols):\n",
    "                    pygame.draw.rect(screen, GREY,\n",
    "                                     [100 + TILE_SIZE * j,\n",
    "                                      80 + TILE_SIZE * (tetris.rows - i),\n",
    "                                      TILE_SIZE, TILE_SIZE], 1)\n",
    "                    if tetris.board[i][j] > 0:\n",
    "                        pygame.draw.rect(screen, BLACK,\n",
    "                                         [101 + TILE_SIZE * j,\n",
    "                                          81 + TILE_SIZE * (tetris.rows - i),\n",
    "                                          TILE_SIZE - 2, TILE_SIZE - 2])\n",
    "\n",
    "            tile = tetris.TILES[tetris.current_tile][tetris.tile_orientation]\n",
    "            for x in range(len(tile)):\n",
    "                for y in range(tile[x][0], tile[x][1]):\n",
    "                    pygame.draw.rect(screen, RED, [\n",
    "                        101 + TILE_SIZE * (x + tetris.tile_x),\n",
    "                        81 + TILE_SIZE * (tetris.rows - (y + tetris.tile_y)),\n",
    "                        TILE_SIZE - 2, TILE_SIZE - 2,\n",
    "                    ])\n",
    "            screen.blit(\n",
    "                font.render(f\"Reward: {tetris.reward}\", True, BLACK), [0, 0],\n",
    "            )\n",
    "            screen.blit(\n",
    "                font.render(f\"Tile {tetris.tile_count}/{tetris.max_tiles}\",\n",
    "                            True, BLACK), [0, 30],\n",
    "            )\n",
    "            if tetris.gameover:\n",
    "                screen.blit(font.render(\"G A M E   O V E R\", True, RED),\n",
    "                            [40, 100 + tetris.rows * TILE_SIZE])\n",
    "                screen.blit(font.render(\"Press ESC to try again\", True, RED),\n",
    "                            [10, 100 + tetris.rows * TILE_SIZE + 30])\n",
    "\n",
    "        pygame.display.flip()\n",
    "\n",
    "        # Get user input.\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                running = False\n",
    "            elif event.type == pygame.KEYDOWN:\n",
    "                if event.key == pygame.K_ESCAPE:\n",
    "                    tetris.restart()\n",
    "                if not tetris.gameover:\n",
    "                    if event.key == pygame.K_LEFT:\n",
    "                        tetris.move_left()\n",
    "                    elif event.key == pygame.K_RIGHT:\n",
    "                        tetris.move_right()\n",
    "                    elif event.key == pygame.K_UP:\n",
    "                        tetris.rotate()\n",
    "                    elif event.key == pygame.K_DOWN:\n",
    "                        tetris.drop()\n",
    "\n",
    "    pygame.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and play Tetris."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "tetris.restart()\n",
    "play_tetris_with_gui(tetris)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making an Agent Play Tetris with Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adapting the Actions of Tetris for Q-Learning\n",
    "\n",
    "Implement a class to play Tetris with Q-learning ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLTetris(Tetris):\n",
    "    \"\"\"Simplified Tetris for Q-learning.\"\"\"\n",
    "\n",
    "    def __init__(self, rows, cols, max_tiles, random_seed):\n",
    "        \"\"\"Initialize Tetris for Q-learning.\"\"\"\n",
    "        super().__init__(rows, cols, max_tiles, random_seed)\n",
    "\n",
    "    def teleport(self, new_x, new_orientation):\n",
    "        \"\"\"Teleport current tile to new position and orientation.\"\"\"\n",
    "        if 0 <= new_orientation < len(Tetris.TILES[self.current_tile]):\n",
    "            tilewidth = len(Tetris.TILES[self.current_tile][new_orientation])\n",
    "            if 0 <= new_x <= self.cols - tilewidth:\n",
    "                self.tile_x = new_x\n",
    "                self.tile_orientation = new_orientation\n",
    "                return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and create an instance of this class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qltetris = QLTetris(rows=4, cols=4, max_tiles=50, random_seed=123456)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing the Q-Learning Agent\n",
    "\n",
    "Implement a class representing a Q-learning agent ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLAgent:\n",
    "    \"\"\"Q-learning agent to play Tetris.\"\"\"\n",
    "\n",
    "    def __init__(self, tetris, games, epsilon, alpha, gamma):\n",
    "        \"\"\"Initialize the agent.\"\"\"\n",
    "        self.tetris = tetris\n",
    "        self.games, self.game = games, 0\n",
    "\n",
    "        self.position_num = self.tetris.cols\n",
    "        self.orientation_num = np.max([len(tile) for tile in Tetris.TILES])\n",
    "        self.action_num = self.position_num * self.orientation_num\n",
    "\n",
    "        self.state_size = (\n",
    "            self.tetris.cols * self.tetris.rows  # Cells in board.\n",
    "            + 1 + np.floor(np.log2(len(Tetris.TILES) - 1)).astype(int)  # Tiles.\n",
    "        )\n",
    "        self.state_num = 2 ** self.state_size\n",
    "\n",
    "        self.epsilon = epsilon\n",
    "        self.Q_table = np.zeros((self.state_num, self.action_num))\n",
    "\n",
    "        self.alpha = alpha  # Learning rate.\n",
    "        self.gamma = gamma  # Discount factor.\n",
    "        self.rewards = np.zeros(games)\n",
    "\n",
    "        self.update_state()\n",
    "\n",
    "    def update_state(self):\n",
    "        \"\"\"Update the state of the agent.\"\"\"\n",
    "        tile = bin(self.tetris.current_tile)[2:]\n",
    "\n",
    "        board = np.copy(self.tetris.board.reshape((-1,))).astype(int)\n",
    "        board[board == Tetris.UNDEFINED] = 0\n",
    "\n",
    "        self.state_binary = np.append(tile, board)\n",
    "        self.state = int(\"\".join(str(i) for i in self.state_binary), 2)\n",
    "\n",
    "    def next_turn(self):\n",
    "        \"\"\"Execute the next turn in the game.\"\"\"\n",
    "        if self.tetris.gameover:\n",
    "            self.rewards[self.game] = self.tetris.reward\n",
    "            if self.game % 100 == 0:\n",
    "                av_reward = np.mean(self.rewards[self.game - 100:self.game])\n",
    "                print(f\"game {self.game}/{self.games} reward {av_reward}\")\n",
    "\n",
    "            if self.game + 1 < self.games:\n",
    "                self.game += 1\n",
    "                self.tetris.restart()\n",
    "                self.update_state()\n",
    "            else:\n",
    "                np.savetxt(\"Q_table.txt\", self.Q_table)\n",
    "                return False  # Finish.\n",
    "        else:\n",
    "            old_state = self.state\n",
    "\n",
    "            if np.random.rand() < self.epsilon:\n",
    "                action = np.random.randint(self.action_num)\n",
    "            else:\n",
    "                action = np.argmax(self.Q_table[self.state, :])\n",
    "\n",
    "            new_x = action // self.position_num\n",
    "            new_orientation = action % self.orientation_num\n",
    "\n",
    "            if self.tetris.teleport(new_x, new_orientation):\n",
    "                reward = self.tetris.drop()\n",
    "\n",
    "                self.update_state()\n",
    "                new_state = self.state\n",
    "\n",
    "                dQ = self.alpha * (\n",
    "                    reward\n",
    "                    + self.gamma * np.max(self.Q_table[new_state, :])\n",
    "                    - self.Q_table[old_state, action]\n",
    "                )\n",
    "\n",
    "                self.Q_table[old_state, action] += dQ\n",
    "            else:  # Penalty for illegal move.\n",
    "                self.Q_table[old_state, action] += -50\n",
    "\n",
    "        return True  # Continue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... a function to observe it play ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def observe_tetris_with_gui(tetris, agent):\n",
    "    \"\"\"Observe a QL agent playing Tetris with GUI.\"\"\"\n",
    "    TILE_SIZE = 20\n",
    "    BLACK = (0, 0, 0)  # RGB code for black color.\n",
    "    GREY = (128, 128, 128)  # RGB code for grey color.\n",
    "    WHITE = (255, 255, 255)  # RGB code for white color.\n",
    "    RED = (255, 0, 0)  # RGB code for red color.\n",
    "\n",
    "    # Initialize the game engine.\n",
    "    pygame.init()\n",
    "    pygame.display.set_caption(\"TETRIS\")\n",
    "    screen = pygame.display.set_mode((200 + tetris.cols * TILE_SIZE,\n",
    "                                      200 + tetris.rows * TILE_SIZE))\n",
    "    pygame.key.set_repeat(300, 100)  # Set keyboard delay and interval in ms.\n",
    "    font = pygame.font.SysFont(\"Calibri\", 25, True)\n",
    "\n",
    "    # Loop until the window is closed.\n",
    "    running = True\n",
    "    while running:\n",
    "        # Paint game board.\n",
    "        if pygame.display.get_active():\n",
    "            screen.fill(WHITE)\n",
    "\n",
    "            for i in range(tetris.rows):\n",
    "                for j in range(tetris.cols):\n",
    "                    pygame.draw.rect(screen, GREY,\n",
    "                                     [100 + TILE_SIZE * j,\n",
    "                                      80 + TILE_SIZE * (tetris.rows - i),\n",
    "                                      TILE_SIZE, TILE_SIZE], 1)\n",
    "                    if tetris.board[i][j] > 0:\n",
    "                        pygame.draw.rect(screen, BLACK,\n",
    "                                         [101 + TILE_SIZE * j,\n",
    "                                          81 + TILE_SIZE * (tetris.rows - i),\n",
    "                                          TILE_SIZE - 2, TILE_SIZE - 2])\n",
    "\n",
    "            tile = tetris.TILES[tetris.current_tile][tetris.tile_orientation]\n",
    "            for x in range(len(tile)):\n",
    "                for y in range(tile[x][0], tile[x][1]):\n",
    "                    pygame.draw.rect(screen, RED, [\n",
    "                        101 + TILE_SIZE * (x + tetris.tile_x),\n",
    "                        81 + TILE_SIZE * (tetris.rows - (y + tetris.tile_y)),\n",
    "                        TILE_SIZE - 2, TILE_SIZE - 2,\n",
    "                    ])\n",
    "            screen.blit(\n",
    "                font.render(f\"Reward: {tetris.reward}\", True, BLACK), [0, 0],\n",
    "            )\n",
    "            screen.blit(\n",
    "                font.render(f\"Tile {tetris.tile_count}/{tetris.max_tiles}\",\n",
    "                            True, BLACK), [0, 30],\n",
    "            )\n",
    "            if tetris.gameover:\n",
    "                screen.blit(font.render(\"G A M E   O V E R\", True, RED),\n",
    "                            [40, 100 + tetris.rows * TILE_SIZE])\n",
    "                screen.blit(font.render(\"Press ESC to try again\", True, RED),\n",
    "                            [10, 100 + tetris.rows * TILE_SIZE + 30])\n",
    "\n",
    "        pygame.display.flip()\n",
    "\n",
    "        # Get user input.\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                running = False\n",
    "            elif event.type == pygame.KEYDOWN:\n",
    "                running = agent.next_turn()\n",
    "\n",
    "    pygame.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and observe the agent play."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "qltetris.restart()\n",
    "agent = QLAgent(qltetris, games=3, epsilon=0, alpha=0.2, gamma=1)\n",
    "observe_tetris_with_gui(qltetris, agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Q-Learning Agent\n",
    "\n",
    "Train the Q-learning agent ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qltetris.restart()\n",
    "agent = QLAgent(qltetris, games=1_000, epsilon=0, alpha=0.2, gamma=1)\n",
    "\n",
    "while agent.next_turn():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... plot the reward during training ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(agent.rewards)\n",
    "plt.xlabel(\"Game\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... observe it play using the trained Q-table ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qltetris.restart()\n",
    "agent = QLAgent(qltetris, games=1, epsilon=0, alpha=0.2, gamma=1)\n",
    "agent.Q_table = np.loadtxt(\"Q_table.txt\")\n",
    "observe_tetris_with_gui(qltetris, agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and analyze the sparsity of the trained Q-table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_table = agent.Q_table\n",
    "non_zero_rows_mask = np.any(Q_table != 0, axis=1)\n",
    "num_non_zero_rows = np.sum(non_zero_rows_mask)\n",
    "print(f\"Non-zero rows = {num_non_zero_rows}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with a Random Tile Sequence\n",
    "\n",
    "Train the Q-learning agent with a random tile sequence ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qltetris = QLTetris(rows=4, cols=4, max_tiles=50, random_seed=None)\n",
    "agent = QLAgent(qltetris, games=200_000, epsilon=0.001, alpha=0.2, gamma=1)\n",
    "\n",
    "while agent.next_turn():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... plot the reward during training ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smoothed_rewards = np.convolve(agent.rewards, np.ones(100) / 100, mode=\"valid\")\n",
    "\n",
    "plt.plot(agent.rewards, label=\"Raw Rewards\")\n",
    "plt.plot(smoothed_rewards, label=\"Smoothed Rewards\")\n",
    "plt.xlabel(\"Game\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and analyze the sparsity of the trained Q-table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_table = agent.Q_table\n",
    "non_zero_rows_mask = np.any(Q_table != 0, axis=1)\n",
    "num_non_zero_rows = np.sum(non_zero_rows_mask)\n",
    "print(f\"Non-zero rows = {num_non_zero_rows}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making an Agent Play Tetris with Deep Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing the Deep-Q-Learning Agent\n",
    "\n",
    "Add the neural networks to the QL agent ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deeplay as dl\n",
    "import torch\n",
    "\n",
    "class DQLAgent(dl.Application):\n",
    "    \"\"\"Deep-Q-learning agent to play Tetris.\"\"\"\n",
    "\n",
    "    def __init__(self, tetris, games, epsilon_max, epsilon_min, epsilon_scale,\n",
    "                 alpha, gamma, hidden_units, replay_buffer_size, batch_size,\n",
    "                 sync_target_game_count, **kwargs):\n",
    "        \"\"\"Initialize the agent.\"\"\"\n",
    "        self.tetris = tetris\n",
    "        self.games, self.game = games, 0\n",
    "\n",
    "        self.position_num = self.tetris.cols\n",
    "        self.orientation_num = np.max([len(tile) for tile in Tetris.TILES])\n",
    "        self.action_num = self.position_num * self.orientation_num\n",
    "\n",
    "        self.state_size = (\n",
    "            self.tetris.cols * self.tetris.rows  # Cells in board.\n",
    "            + 1 + np.floor(np.log2(len(Tetris.TILES) - 1)).astype(int)  # Tiles.\n",
    "        )\n",
    "        self.state_num = 2 ** self.state_size\n",
    "\n",
    "        self.epsilon_max = epsilon_max\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_scale = epsilon_scale\n",
    "        self.epsilon = self.epsilon_max\n",
    "\n",
    "        self.alpha = alpha  # Learning rate.\n",
    "        self.gamma = gamma  # Discount factor.\n",
    "        self.rewards = np.zeros(games)\n",
    "        self.max_reward = 0\n",
    "\n",
    "        self.q_net = self.get_net(hidden_units)\n",
    "        self.target_net = self.get_net(hidden_units)\n",
    "\n",
    "        super().__init__(\n",
    "            loss=torch.nn.MSELoss(),\n",
    "            optimizer=torch.optim.Adam(self.q_net.parameters(), lr=0.001),\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        self.buffer = []\n",
    "        self.replay_buffer_size = replay_buffer_size\n",
    "        self.batch_size = batch_size\n",
    "        self.sync_target_game_count = sync_target_game_count\n",
    "\n",
    "        self.update_state()\n",
    "\n",
    "    def get_net(self, hidden_units):\n",
    "        \"\"\"Create instance of neural network.\"\"\"\n",
    "        net = dl.MultiLayerPerceptron(\n",
    "            in_features=self.state_size,\n",
    "            hidden_features=hidden_units,\n",
    "            out_features=self.action_num,\n",
    "        )\n",
    "        return net.build()\n",
    "\n",
    "    def update_state(self):\n",
    "        \"\"\"Update the state of the agent.\"\"\"\n",
    "        num_bits = int(np.ceil(np.log2(len(Tetris.TILES))))\n",
    "        tile = bin(self.tetris.current_tile)[2:].zfill(num_bits)\n",
    "        tile = np.array([int(i) for i in tile])\n",
    "\n",
    "        board = np.copy(self.tetris.board.reshape((-1,))).astype(int)\n",
    "        board[board == Tetris.UNDEFINED] = 0\n",
    "\n",
    "        self.state_binary = np.append(tile, board)\n",
    "        self.state = torch.tensor(self.state_binary, dtype=torch.float32)\n",
    "\n",
    "    def next_turn(self):\n",
    "        \"\"\"Execute the next turn in the game.\"\"\"\n",
    "        if self.tetris.gameover:\n",
    "            self.rewards[self.game] = self.tetris.reward\n",
    "            if self.game % 100 == 0:\n",
    "                av_reward = np.mean(self.rewards[self.game - 100:self.game])\n",
    "                print(f\"game {self.game}/{self.games} reward {av_reward}\")\n",
    "                if av_reward > self.max_reward:\n",
    "                    self.max_reward = av_reward\n",
    "                    torch.save(self.q_net.state_dict(), \"q_net.pth\")\n",
    "\n",
    "            if self.game + 1 < self.games:\n",
    "                self.game += 1\n",
    "                if ((len(self.buffer) >= self.replay_buffer_size)\n",
    "                    and (self.game % self.sync_target_game_count == 0)):\n",
    "                    self.target_net.load_state_dict(self.q_net.state_dict())\n",
    "                self.tetris.restart()\n",
    "                self.update_state()\n",
    "\n",
    "                self.epsilon = max(\n",
    "                    self.epsilon_min,\n",
    "                    self.epsilon_max - self.game / self.epsilon_scale,\n",
    "                )\n",
    "            else:\n",
    "                return False  # Finish.\n",
    "        else:\n",
    "            old_state = self.state\n",
    "\n",
    "            with torch.no_grad():\n",
    "                if np.random.rand() < self.epsilon:\n",
    "                    action = np.random.randint(self.action_num)\n",
    "                else:\n",
    "                    state = self.state.view(1, self.state_size)\n",
    "                    output = self.q_net(state).detach().numpy()[0]\n",
    "                    action = np.argmax(output)\n",
    "\n",
    "                new_x = action // self.position_num\n",
    "                new_orientation = action % self.orientation_num\n",
    "\n",
    "                self.tetris.teleport(new_x, new_orientation)\n",
    "                reward = self.tetris.drop()\n",
    "\n",
    "                self.update_state()\n",
    "                new_state = self.state\n",
    "\n",
    "                self.buffer.append({\n",
    "                    \"old_state\": old_state,\n",
    "                    \"action\": action,\n",
    "                    \"reward\": reward,\n",
    "                    \"new_state\": new_state,\n",
    "                    \"gameover\": self.tetris.gameover,\n",
    "                })\n",
    "                if len(self.buffer) >= self.replay_buffer_size + 1:\n",
    "                    self.buffer.pop(0)\n",
    "\n",
    "            if len(self.buffer) >= self.replay_buffer_size:\n",
    "                batch = random.sample(self.buffer, self.batch_size)\n",
    "\n",
    "                # Store states in lists.\n",
    "                states, next_states = [], []\n",
    "                for sample in batch:\n",
    "                    states.append(sample[\"old_state\"])\n",
    "                    next_states.append(sample[\"new_state\"])\n",
    "\n",
    "                # Evaluate next state with TargetNet.\n",
    "                with torch.no_grad():\n",
    "                    q_hat = self.target_net(torch.stack(next_states, dim=0))\n",
    "\n",
    "                # Compute targets.\n",
    "                targets = torch.zeros(self.batch_size, self.action_num)\n",
    "                targets_mask = torch.zeros(self.batch_size, self.action_num)\n",
    "                for idx, sample in enumerate(batch):\n",
    "                    if sample[\"gameover\"]:\n",
    "                        target = sample[\"reward\"]\n",
    "                    else:\n",
    "                        target = (sample[\"reward\"]\n",
    "                                  + self.gamma * np.nanmax(q_hat[idx, :]))\n",
    "                    targets[idx, sample[\"action\"]] = target\n",
    "                    targets_mask[idx, sample[\"action\"]] = 1\n",
    "\n",
    "                # Evaluate predictions, apply mask, and update weights.\n",
    "                self.optimizer.zero_grad()\n",
    "                preds = self.q_net(torch.stack(states, dim=0))\n",
    "                masked_preds = preds * targets_mask\n",
    "                loss = self.loss(masked_preds, targets)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "        return True  # Continue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... train the deep-Q-learning agent ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qltetris = QLTetris(rows=4, cols=4, max_tiles=50, random_seed=None)\n",
    "agent = DQLAgent(\n",
    "    qltetris, games=10_000, epsilon_max=1, epsilon_min=0.001,\n",
    "    epsilon_scale=5_000, alpha=0.001, gamma=1, hidden_units=[128, 128],\n",
    "    replay_buffer_size=10_000, batch_size=64, sync_target_game_count=100,\n",
    ")\n",
    "\n",
    "while agent.next_turn():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... plot the reward during training ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smoothed_rewards = np.convolve(agent.rewards, np.ones(100) / 100, mode=\"valid\")\n",
    "\n",
    "plt.plot(agent.rewards, label=\"Raw Rewards\")\n",
    "plt.plot(smoothed_rewards, label=\"Smoothed Rewards\")\n",
    "plt.xlabel(\"Game\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and observe its behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qltetris.restart()\n",
    "agent = DQLAgent(\n",
    "    qltetris, games=10_000, epsilon_max=0, epsilon_min=0, epsilon_scale=5_000,\n",
    "    alpha=0.001, gamma=1, hidden_units=[128, 128], replay_buffer_size=10_000,\n",
    "    batch_size=64, sync_target_game_count=100,\n",
    ")\n",
    "model_state_dict = torch.load(\"q_net.pth\")\n",
    "agent.q_net.load_state_dict(model_state_dict)\n",
    "agent.target_net.load_state_dict(model_state_dict)\n",
    "observe_tetris_with_gui(qltetris, agent)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py_env_book",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
