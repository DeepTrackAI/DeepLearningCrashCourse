{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pygame\n",
    "import gameboardClass\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Define parameters of the board\n",
    "\n",
    "# Game parameters: \n",
    "# - 'N_row' and 'N_col' (integers) define the size of the game board.\n",
    "# - 'tile_size' (2 or 4) determines whether the small tile set (2) or the large tile set (4) should be used.\n",
    "# - 'max_tile_count' (integer) sets the maximum number of tiles to be placed in one game.\n",
    "# - 'stochastic_prob' (float between 0 and 1) determines the probability of taking a random tile. \n",
    "#   When stochastic_prob=0, tiles are taken according to a predefined sequence. \n",
    "#   When stochastic_prob=1, all tiles are random. For values 0<stochastic_prob<1, there is a mixture of deterministic and random tiles.\n",
    "\n",
    "# Training parameters:\n",
    "# - 'alpha' is the learning rate in Q-learning or for the stochastic gradient descent in deep Q-networks.\n",
    "# - 'epsilon' is the probability to choose a random action in the epsilon-greedy policy.\n",
    "# - 'episode_count' is the number of episodes a training session lasts.\n",
    "\n",
    "# Additional training parameters for deep Q-networks:\n",
    "# - 'epsilon_scale' is the scale of the episode number where epsilon_N changes from unity to epsilon.\n",
    "# - 'replay_buffer_size' is the size of the experience replay buffer.\n",
    "# - 'batch_size' is the number of samples taken from the experience replay buffer for each update.\n",
    "# - 'sync_target_episode_count' is the number of episodes between synchronizations of the target network.\n",
    "N_row=4\n",
    "N_col=4\n",
    "tile_size=2\n",
    "max_tile_count=50\n",
    "stochastic_prob=0\n",
    "\n",
    "alpha=0.2\n",
    "epsilon=0\n",
    "episode_count=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HumanPlayer:\n",
    "    def init_board(self,gameboard):\n",
    "        self.episode=0\n",
    "        self.reward_tots=[0]\n",
    "        self.gameboard=gameboard\n",
    "\n",
    "    def get_state(self):\n",
    "        pass\n",
    "\n",
    "    def next_turn(self,pygame):\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                pygame.quit()\n",
    "                raise SystemExit(\"Game terminated\")\n",
    "            if event.type==pygame.KEYDOWN:\n",
    "                if event.key == pygame.K_ESCAPE:\n",
    "                    self.reward_tots=[0]\n",
    "                    self.gameboard.fn_restart()\n",
    "                if not self.gameboard.gameover:\n",
    "                    if event.key == pygame.K_UP:\n",
    "                        self.gameboard.fn_move(self.gameboard.tile_x,(self.gameboard.tile_orientation+1)%len(self.gameboard.tiles[self.gameboard.cur_tile_type]))\n",
    "                    if event.key == pygame.K_LEFT:\n",
    "                        self.gameboard.fn_move(self.gameboard.tile_x-1,self.gameboard.tile_orientation)\n",
    "                    if event.key == pygame.K_RIGHT:\n",
    "                        self.gameboard.fn_move(self.gameboard.tile_x+1,self.gameboard.tile_orientation)\n",
    "                    if (event.key == pygame.K_DOWN) or (event.key == pygame.K_SPACE):\n",
    "                        self.reward_tots[self.episode]+=self.gameboard.fn_drop()\n",
    "\n",
    "\n",
    "#play the game, use arrow keys to play\n",
    "player=HumanPlayer()  \n",
    "gameboard=gameboardClass.TGameBoard(N_row,N_col,tile_size,max_tile_count,player,stochastic_prob)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def Evaluate(gameboard,agent,agent_evaluate=None,strategy_file='',evaluate_agent=True):\n",
    "    if agent_evaluate is None:\n",
    "        evaluate_agent=False\n",
    "\n",
    "    if evaluate_agent:\n",
    "        agent_evaluate.epsilon=0\n",
    "        agent_evaluate.init_board(gameboard)\n",
    "        agent_evaluate.load_strategy(strategy_file)\n",
    "\n",
    "    if isinstance(gameboard.agent,HumanPlayer):\n",
    "        # Define some colors for painting\n",
    "        COLOR_BLACK = (0, 0, 0)\n",
    "        COLOR_GREY = (128, 128, 128)\n",
    "        COLOR_WHITE = (255, 255, 255)\n",
    "        COLOR_RED =  (255, 0, 0)\n",
    "\n",
    "        # Initialize the game engine\n",
    "        pygame.init()\n",
    "        screen=pygame.display.set_mode((200+gameboard.N_col*20,150+gameboard.N_row*20))\n",
    "        clock=pygame.time.Clock()\n",
    "        pygame.key.set_repeat(300,100)\n",
    "        pygame.display.set_caption('Turn-based tetris')\n",
    "        font=pygame.font.SysFont('Calibri',25,True)\n",
    "        fontLarge=pygame.font.SysFont('Calibri',50,True)\n",
    "        framerate=0;\n",
    "\n",
    "        # Loop until the window is closed\n",
    "        while True:\n",
    "            if isinstance(gameboard.agent,HumanPlayer):\n",
    "                gameboard.agent.next_turn(pygame)\n",
    "            else:\n",
    "                pygame.event.pump()\n",
    "                for event in pygame.event.get():\n",
    "                    if event.type==pygame.KEYDOWN:\n",
    "                        if event.key==pygame.K_SPACE:\n",
    "                            if framerate > 0:\n",
    "                                framerate=0\n",
    "                            else:\n",
    "                                framerate=10\n",
    "                        if (event.key==pygame.K_LEFT) and (framerate>1):\n",
    "                            framerate-=1\n",
    "                        if event.key==pygame.K_RIGHT:\n",
    "                            framerate+=1\n",
    "                gameboard.agent.next_turn()\n",
    "\n",
    "            if evaluate_agent:\n",
    "                agent_evaluate.get_state()\n",
    "                agent_evaluate.set_action()\n",
    "\n",
    "            if pygame.display.get_active():\n",
    "                # Paint game board\n",
    "                screen.fill(COLOR_WHITE)\n",
    "\n",
    "                for i in range(gameboard.N_row):\n",
    "                    for j in range(gameboard.N_col):\n",
    "                        pygame.draw.rect(screen,COLOR_GREY,[100+20*j,80+20*(gameboard.N_row-i),20,20],1)\n",
    "                        if gameboard.board[i][j] > 0:\n",
    "                            pygame.draw.rect(screen,COLOR_BLACK,[101+20*j,81+20*(gameboard.N_row-i),18,18])\n",
    "\n",
    "                if gameboard.cur_tile_type is not None:\n",
    "                    curTile=gameboard.tiles[gameboard.cur_tile_type][gameboard.tile_orientation]\n",
    "                    for xLoop in range(len(curTile)):\n",
    "                        for yLoop in range(curTile[xLoop][0],curTile[xLoop][1]):\n",
    "                            pygame.draw.rect(screen,COLOR_RED,[101+20*((xLoop+gameboard.tile_x)%gameboard.N_col),81+20*(gameboard.N_row-(yLoop+gameboard.tile_y)),18,18])\n",
    "\n",
    "                screen.blit(font.render(\"Reward: \"+str(agent.reward_tots[agent.episode]),True,COLOR_BLACK),[0,0])\n",
    "                screen.blit(font.render(\"Tile \"+str(gameboard.tile_count)+\"/\"+str(gameboard.max_tile_count),True,COLOR_BLACK),[0,20])\n",
    "                if framerate>0:\n",
    "                    screen.blit(font.render(\"FPS: \"+str(framerate),True,COLOR_BLACK),[320,0])\n",
    "                screen.blit(font.render(\"Reward: \"+str(agent.reward_tots[agent.episode]),True,COLOR_BLACK),[0,0])\n",
    "                if gameboard.gameover:\n",
    "                    screen.blit(fontLarge.render(\"Game Over\", True,COLOR_RED), [80, 200])\n",
    "                    screen.blit(font.render(\"Press ESC to try again\", True,COLOR_RED), [85, 265])\n",
    "\n",
    "                pygame.display.flip()\n",
    "                clock.tick(framerate)\n",
    "\n",
    "Evaluate(gameboard,player)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define standard Q learning agent\n",
    "\n",
    "def convertToBinary(s,state_size=18):\n",
    "    binary = np.array([int(i) for i in bin(s)[2:]])\n",
    "    binary = np.pad(binary, (state_size - len(binary), 0), 'constant')\n",
    "    return binary\n",
    "\n",
    "class QLAgent:\n",
    "    def __init__(self,alpha,epsilon,episode_count):\n",
    "        # Initialize training parameters\n",
    "        self.alpha=alpha\n",
    "        self.epsilon=epsilon\n",
    "        self.episode=0\n",
    "        self.episode_count=episode_count\n",
    "\n",
    "    def init_board(self,gameboard):\n",
    "        \n",
    "        #Set up and initialize the states, actions and Q-table and storage for the rewards\n",
    "        self.gameboard=gameboard\n",
    "        self.nr_of_rows = gameboard.N_row\n",
    "        self.nr_of_cols = gameboard.N_col\n",
    "        self.len_of_tiles = len(bin(len(gameboard.tiles)-1)[2:])\n",
    "        self.board = gameboard.board\n",
    "        self.tiles = gameboard.tiles\n",
    "        self.tile_sequence = gameboard.tile_sequence\n",
    "        self.reward_tots = np.array([0]*self.episode_count)\n",
    "        \n",
    "        self.nr_of_rotations=4\n",
    "        self.nr_of_moves = self.nr_of_cols-1\n",
    "        self.state_size = gameboard.N_col*gameboard.N_row+self.len_of_tiles\n",
    "        self.nr_of_states = 2**self.state_size\n",
    "        self.action_size = self.nr_of_moves*self.nr_of_rotations\n",
    "        \n",
    "        self.Q_table = np.zeros((self.nr_of_states,self.action_size))\n",
    "        self.Q_target = self.Q_table\n",
    "\n",
    "    def load_strategy(self, strategy_file):\n",
    "        self.Q_table = np.loadtxt(strategy_file)\n",
    "        self.Q_target = self.Q_table\n",
    "\n",
    "    def get_state(self):\n",
    "        #Convert board to binary list\n",
    "        board = np.copy(self.gameboard.board.reshape(self.nr_of_cols*self.nr_of_rows,)).astype(int)\n",
    "        board[board==-1] = 0\n",
    "        #Convert tile to binary list\n",
    "        tile = np.copy(self.gameboard.cur_tile_type)\n",
    "        tile = convertToBinary(tile,self.len_of_tiles)   \n",
    "        #Add binary lists\n",
    "        state = np.append(tile,board)    \n",
    "        #Convert binary list to int\n",
    "        state = int(\"\".join(str(i) for i in state),2)\n",
    "        self.state = state\n",
    "        \n",
    "    def set_action(self):\n",
    "        # Select action using epsilon-greedy policy\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            action = np.random.randint(self.nr_of_moves*self.nr_of_rotations)            \n",
    "        else:\n",
    "            action = np.argmax(self.Q_table[self.state,:]) \n",
    "        #Extract rotation and movement from action parameter \n",
    "        tile_orientation = action % self.nr_of_rotations\n",
    "        tile_x = int(action/self.nr_of_rotations)\n",
    "        self.legalAction = self.gameboard.fn_move(tile_x,tile_orientation)\n",
    "        self.action = action\n",
    "        \n",
    "    def train(self,old_state,reward):\n",
    "        # Update the Q table using state and action stored as attributes in self and using function arguments for the old state and the reward\n",
    "        if self.legalAction == 0:\n",
    "            self.Q_table[old_state,self.action] += self.alpha*(reward+np.max(self.Q_table[self.state,:])-self.Q_table[old_state,self.action])\n",
    "        else:\n",
    "            self.Q_table[old_state,self.action] += -50 #penalty for illegal move\n",
    "\n",
    "    def next_turn(self):\n",
    "        if self.gameboard.gameover:\n",
    "            self.episode+=1\n",
    "            if self.episode%100==0:\n",
    "                self.Q_target = self.Q_table\n",
    "                print('episode '+str(self.episode)+'/'+str(self.episode_count)+' (reward: ',str(np.sum(self.reward_tots[range(self.episode-100,self.episode)])),')')\n",
    "            if self.episode>=self.episode_count:\n",
    "                np.savetxt('Q_table.txt',self.Q_table)\n",
    "                raise SystemExit(\"Training finished\")\n",
    "            else:\n",
    "                self.gameboard.fn_restart()\n",
    "        else:\n",
    "            # Select and execute action (move the tile to the desired column and orientation)\n",
    "            self.set_action()\n",
    "            old_state=np.copy(self.state)\n",
    "            # Drop the tile on the game board\n",
    "            reward=self.gameboard.fn_drop()   \n",
    "            self.reward_tots[self.episode]+=reward\n",
    "            # Read the new state\n",
    "            self.get_state()\n",
    "            # Update the Q-table using the old state and the reward\n",
    "            self.train(old_state,reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standard Q-learning to play Tetris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent=QLAgent(alpha,epsilon,episode_count)\n",
    "gameboard=gameboardClass.TGameBoard(N_row,N_col,tile_size,max_tile_count,agent,stochastic_prob)\n",
    "\n",
    "while True:\n",
    "    gameboard.agent.next_turn()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gameboard=gameboardClass.TGameBoard(N_row,N_col,tile_size,max_tile_count,player,stochastic_prob)\n",
    "plt.plot(agent.reward_tots)\n",
    "plt.ylabel('Reward')\n",
    "plt.xlabel('Episode')\n",
    "Evaluate(gameboard,player,agent,strategy_file='Q_table.txt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We consider the more challenging scenario of a random sequence of tiles. Here, we expect standard Q learning to not converge to an (even close to) optimal solution in finite time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stochastic_prob=1\n",
    "epsilon=0.001\n",
    "episode_count=200000\n",
    "\n",
    "agent=QLAgent(alpha,epsilon,episode_count)\n",
    "gameboard=gameboardClass.TGameBoard(N_row,N_col,tile_size,max_tile_count,agent,stochastic_prob)\n",
    "\n",
    "while True:\n",
    "    gameboard.agent.next_turn()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gameboard=gameboardClass.TGameBoard(N_row,N_col,tile_size,max_tile_count,player,stochastic_prob)\n",
    "plt.plot(agent.reward_tots)\n",
    "running_mean=np.zeros(len(agent.reward_tots))\n",
    "for i in range(len(agent.reward_tots)):\n",
    "    running_mean[i]=np.mean(agent.reward_tots[max(0,i-500):i])\n",
    "plt.plot(running_mean)\n",
    "plt.ylabel('Reward')\n",
    "plt.xlabel('Episode')\n",
    "Evaluate(gameboard,player,agent,strategy_file='Q_table.txt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possibly, we can also let the user play the game again in these more difficult conditions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Evaluate(gameboard,player)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./deeplay')\n",
    "import deeplay as dl\n",
    "import torch\n",
    "import random\n",
    "\n",
    "    \n",
    "class Quadruplet:\n",
    "    def __init__(self, state, action, reward, new_state, terminal):\n",
    "        self.state = state\n",
    "        self.action = action\n",
    "        self.new_state = new_state\n",
    "        self.reward = reward\n",
    "        self.terminal = terminal\n",
    "\n",
    "class DQLAgent(dl.Application):\n",
    "    # Agent for learning to play tetris using deep Q-learning\n",
    "    def __init__(self,q_net,target_net, alpha, epsilon, epsilon_scale, replay_buffer_size, batch_size, sync_target_episode_count, episode_count,**kwargs):\n",
    "        # Initialize training parameters\n",
    "        self.alpha = alpha\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_scale = epsilon_scale\n",
    "        self.replay_buffer_size = replay_buffer_size\n",
    "        self.batch_size = batch_size\n",
    "        self.sync_target_episode_count = sync_target_episode_count\n",
    "\n",
    "        self.q_net = q_net\n",
    "        self.target_net = target_net\n",
    "        self.episode = 0\n",
    "        self.episode_count = episode_count\n",
    "        self.max_reward=0\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def init_board(self, gameboard):\n",
    "\n",
    "        # Initialize board variables\n",
    "        self.gameboard = gameboard\n",
    "        self.nr_of_actions=gameboard.N_col*4\n",
    "        self.len_of_tiles = len(bin(len(gameboard.tiles)-1)[2:])\n",
    "        self.state_size = gameboard.N_col*gameboard.N_row+self.len_of_tiles\n",
    "\n",
    "        # Initialize networks and storage\n",
    "        self.reward_tots = np.zeros(self.episode_count)\n",
    "        self.exp_buffer = []\n",
    "\n",
    "    def load_strategy(self, strategy_file):\n",
    "        #Load model weights\n",
    "        model_state_dict = torch.load(strategy_file)\n",
    "        self.q_net.load_state_dict(model_state_dict)\n",
    "        self.target_net.load_state_dict(model_state_dict)\n",
    "\n",
    "    def get_state(self):\n",
    "        #Convert board to binary list\n",
    "        board = np.copy(self.gameboard.board.reshape(16,)).astype(int)\n",
    "        board[board==-1] = 0\n",
    "        #Convert tile to binary list\n",
    "        tile = np.copy(self.gameboard.cur_tile_type)\n",
    "        tile = convertToBinary(tile,self.len_of_tiles)   \n",
    "        #Combine binary lists and save as state\n",
    "        state = np.append(tile,board)    \n",
    "        self.state = torch.tensor(state, dtype=torch.float32)\n",
    "\n",
    "    def set_action(self):\n",
    "        r = random.random()\n",
    "        with torch.no_grad():\n",
    "            self.output = self.q_net(self.state.view(1, self.state_size)).detach().numpy()[0]\n",
    "            if np.random.rand() < self.epsilon:\n",
    "                action = np.random.randint(self.nr_of_actions)            \n",
    "            else:\n",
    "                action = np.argmax(self.output)\n",
    "            #Extract rotation and movement from action parameter \n",
    "            rotation = action % 4\n",
    "            position = int(action/4)  \n",
    "            self.legalAction = self.gameboard.fn_move(position,rotation)\n",
    "            self.action = action\n",
    " \n",
    "\n",
    "    def training_step(self, batch):\n",
    "        # Store states in a list\n",
    "        states = []\n",
    "        next_states = []\n",
    "        for quadruple in batch:\n",
    "            states.append(quadruple.state)\n",
    "            next_states.append(quadruple.new_state)\n",
    "        # Initialize targets and target mask\n",
    "        targets = torch.zeros(self.batch_size, self.nr_of_actions)\n",
    "        targets_mask = torch.zeros(self.batch_size, self.nr_of_actions)\n",
    "        # Evaluate next state with target network\n",
    "        with torch.no_grad():\n",
    "            q_hat = self.target_net(torch.stack(next_states, dim=0))\n",
    "        # Computes targets\n",
    "        for idx, quadruple in enumerate(batch):\n",
    "            if quadruple.terminal:\n",
    "                y = quadruple.reward\n",
    "            else:\n",
    "                y = quadruple.reward + np.nanmax(q_hat[idx,:])\n",
    "            targets[idx, quadruple.action] = y\n",
    "            targets_mask[idx, quadruple.action] = 1\n",
    "        # Evaluate old states, apply mask and update weights\n",
    "        self.optimizer.zero_grad()\n",
    "        outputs = self.q_net(torch.stack(states, dim=0)) * targets_mask\n",
    "        loss = self.loss(outputs, targets)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def next_turn(self):\n",
    "        if self.gameboard.gameover:\n",
    "            self.episode += 1\n",
    "            self.epsilon = max(0.001, 1 - self.episode / self.epsilon_scale)\n",
    "            if self.episode % 100 == 0:\n",
    "                current_reward = np.sum(self.reward_tots[range(self.episode - 100, self.episode)])\n",
    "                print('episode ' + str(self.episode) + ' / ' + str(self.episode_count) + ' (reward: ', str(current_reward))\n",
    "                if current_reward > self.max_reward:\n",
    "                    self.max_reward = current_reward\n",
    "                    torch.save(self.q_net.state_dict(), 'q_net.pth')\n",
    "            if self.episode >= self.episode_count:\n",
    "                raise SystemExit(\"Training finished\")\n",
    "            else:\n",
    "                if (len(self.exp_buffer) >= self.replay_buffer_size) and (self.episode % self.sync_target_episode_count == 0):\n",
    "                    self.target_net.load_state_dict(self.q_net.state_dict())\n",
    "                self.gameboard.fn_restart()\n",
    "        else:\n",
    "            # Select and execute action (move the tile to the desired column and orientation)\n",
    "            self.set_action()\n",
    "            old_state = self.state\n",
    "            # Drop the tile on the game board\n",
    "            reward = self.gameboard.fn_drop()\n",
    "            self.reward_tots[self.episode] += reward\n",
    "\n",
    "            # Read the new state\n",
    "            self.get_state()\n",
    "            terminal = self.gameboard.gameover\n",
    "            quadruplet = Quadruplet(old_state, self.action, reward, self.state, terminal)\n",
    "            self.exp_buffer.append(quadruplet)\n",
    "            if len(self.exp_buffer) >= self.replay_buffer_size:\n",
    "                batch = random.sample(self.exp_buffer, self.batch_size)\n",
    "                self.training_step(batch)\n",
    "            if len(self.exp_buffer) >= self.replay_buffer_size + 1:\n",
    "                self.exp_buffer.pop(0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha=0.001\n",
    "episode_count=10000\n",
    "\n",
    "epsilon_scale=5000\n",
    "replay_buffer_size=10000\n",
    "batch_size=64\n",
    "sync_target_episode_count=100\n",
    "\n",
    "hidden_units=[128,128]\n",
    "\n",
    "q_net=dl.MultiLayerPerceptron(gameboard.N_row * gameboard.N_col + 2,hidden_units, gameboard.N_col*4)\n",
    "q_net.blocks[:-1].activation = torch.nn.ReLU()\n",
    "q_net=q_net.build()\n",
    "print(q_net)\n",
    "target_net=dl.MultiLayerPerceptron(gameboard.N_row * gameboard.N_col + 2,hidden_units,gameboard.N_col*4)\n",
    "target_net.blocks[:-1].activation = torch.nn.ReLU() #GELU stabilizes training, ReLU works ok as well\n",
    "target_net=target_net.build()\n",
    "\n",
    "agent=DQLAgent(q_net,target_net,alpha,epsilon,epsilon_scale,replay_buffer_size,batch_size,sync_target_episode_count,episode_count,loss=torch.nn.MSELoss(),optimizer=torch.optim.Adam(q_net.parameters(),lr=alpha))\n",
    "gameboard=gameboardClass.TGameBoard(N_row,N_col,tile_size,max_tile_count,agent,stochastic_prob)\n",
    "\n",
    "while True:\n",
    "    gameboard.agent.next_turn()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gameboard=gameboardClass.TGameBoard(N_row,N_col,tile_size,max_tile_count,player,stochastic_prob)\n",
    "plt.plot(agent.reward_tots)\n",
    "running_mean=np.zeros(len(agent.reward_tots))\n",
    "for i in range(len(agent.reward_tots)):\n",
    "    running_mean[i]=np.mean(agent.reward_tots[max(0,i-500):i])\n",
    "plt.plot(running_mean)\n",
    "plt.ylabel('Reward')\n",
    "plt.xlabel('Episode')\n",
    "Evaluate(gameboard,player,agent,strategy_file='q_net.pth')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DLCC",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
