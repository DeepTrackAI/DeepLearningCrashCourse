{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.5.2 (SDL 2.28.3, Python 3.10.13)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pygame\n",
    "import gameboardClass\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Define parameters of the board\n",
    "\n",
    "# Game parameters: \n",
    "# - 'N_row' and 'N_col' (integers) define the size of the game board.\n",
    "# - 'tile_size' (2 or 4) determines whether the small tile set (2) or the large tile set (4) should be used.\n",
    "# - 'max_tile_count' (integer) sets the maximum number of tiles to be placed in one game.\n",
    "# - 'stochastic_prob' (float between 0 and 1) determines the probability of taking a random tile. \n",
    "#   When stochastic_prob=0, tiles are taken according to a predefined sequence. \n",
    "#   When stochastic_prob=1, all tiles are random. For values 0<stochastic_prob<1, there is a mixture of deterministic and random tiles.\n",
    "\n",
    "# Training parameters:\n",
    "# - 'alpha' is the learning rate in Q-learning or for the stochastic gradient descent in deep Q-networks.\n",
    "# - 'epsilon' is the probability to choose a random action in the epsilon-greedy policy.\n",
    "# - 'episode_count' is the number of episodes a training session lasts.\n",
    "\n",
    "# Additional training parameters for deep Q-networks:\n",
    "# - 'epsilon_scale' is the scale of the episode number where epsilon_N changes from unity to epsilon.\n",
    "# - 'replay_buffer_size' is the size of the experience replay buffer.\n",
    "# - 'batch_size' is the number of samples taken from the experience replay buffer for each update.\n",
    "# - 'sync_target_episode_count' is the number of episodes between synchronizations of the target network.\n",
    "N_row=4\n",
    "N_col=4\n",
    "tile_size=2\n",
    "max_tile_count=50\n",
    "stochastic_prob=0\n",
    "\n",
    "alpha=0.2\n",
    "epsilon=0\n",
    "episode_count=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HumanPlayer:\n",
    "    def init_board(self,gameboard):\n",
    "        self.episode=0\n",
    "        self.reward_tots=[0]\n",
    "        self.gameboard=gameboard\n",
    "\n",
    "    def get_state(self):\n",
    "        pass\n",
    "\n",
    "    def next_turn(self,pygame):\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                pygame.quit()\n",
    "                raise SystemExit(\"Game terminated\")\n",
    "            if event.type==pygame.KEYDOWN:\n",
    "                if event.key == pygame.K_ESCAPE:\n",
    "                    self.reward_tots=[0]\n",
    "                    self.gameboard.fn_restart()\n",
    "                if not self.gameboard.gameover:\n",
    "                    if event.key == pygame.K_UP:\n",
    "                        self.gameboard.fn_move(self.gameboard.tile_x,(self.gameboard.tile_orientation+1)%len(self.gameboard.tiles[self.gameboard.cur_tile_type]))\n",
    "                    if event.key == pygame.K_LEFT:\n",
    "                        self.gameboard.fn_move(self.gameboard.tile_x-1,self.gameboard.tile_orientation)\n",
    "                    if event.key == pygame.K_RIGHT:\n",
    "                        self.gameboard.fn_move(self.gameboard.tile_x+1,self.gameboard.tile_orientation)\n",
    "                    if (event.key == pygame.K_DOWN) or (event.key == pygame.K_SPACE):\n",
    "                        self.reward_tots[self.episode]+=self.gameboard.fn_drop()\n",
    "\n",
    "\n",
    "#play the game, use arrow keys to play\n",
    "player=HumanPlayer()  \n",
    "gameboard=gameboardClass.TGameBoard(N_row,N_col,tile_size,max_tile_count,player,stochastic_prob)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "Game terminated",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m Game terminated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/giovannivolpe/miniconda3/envs/py310/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3534: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def Evaluate(gameboard,agent,agent_evaluate=None,strategy_file='',evaluate_agent=True):\n",
    "    if agent_evaluate is None:\n",
    "        evaluate_agent=False\n",
    "\n",
    "    if evaluate_agent:\n",
    "        agent_evaluate.epsilon=0\n",
    "        agent_evaluate.init_board(gameboard)\n",
    "        agent_evaluate.load_strategy(strategy_file)\n",
    "\n",
    "    if isinstance(gameboard.agent,HumanPlayer):\n",
    "        # Define some colors for painting\n",
    "        COLOR_BLACK = (0, 0, 0)\n",
    "        COLOR_GREY = (128, 128, 128)\n",
    "        COLOR_WHITE = (255, 255, 255)\n",
    "        COLOR_RED =  (255, 0, 0)\n",
    "\n",
    "        # Initialize the game engine\n",
    "        pygame.init()\n",
    "        screen=pygame.display.set_mode((200+gameboard.N_col*20,150+gameboard.N_row*20))\n",
    "        clock=pygame.time.Clock()\n",
    "        pygame.key.set_repeat(300,100)\n",
    "        pygame.display.set_caption('Turn-based tetris')\n",
    "        font=pygame.font.SysFont('Calibri',25,True)\n",
    "        fontLarge=pygame.font.SysFont('Calibri',50,True)\n",
    "        framerate=0;\n",
    "\n",
    "        # Loop until the window is closed\n",
    "        while True:\n",
    "            if isinstance(gameboard.agent,HumanPlayer):\n",
    "                gameboard.agent.next_turn(pygame)\n",
    "            else:\n",
    "                pygame.event.pump()\n",
    "                for event in pygame.event.get():\n",
    "                    if event.type==pygame.KEYDOWN:\n",
    "                        if event.key==pygame.K_SPACE:\n",
    "                            if framerate > 0:\n",
    "                                framerate=0\n",
    "                            else:\n",
    "                                framerate=10\n",
    "                        if (event.key==pygame.K_LEFT) and (framerate>1):\n",
    "                            framerate-=1\n",
    "                        if event.key==pygame.K_RIGHT:\n",
    "                            framerate+=1\n",
    "                gameboard.agent.next_turn()\n",
    "\n",
    "            if evaluate_agent:\n",
    "                agent_evaluate.get_state()\n",
    "                agent_evaluate.set_action()\n",
    "\n",
    "            if pygame.display.get_active():\n",
    "                # Paint game board\n",
    "                screen.fill(COLOR_WHITE)\n",
    "\n",
    "                for i in range(gameboard.N_row):\n",
    "                    for j in range(gameboard.N_col):\n",
    "                        pygame.draw.rect(screen,COLOR_GREY,[100+20*j,80+20*(gameboard.N_row-i),20,20],1)\n",
    "                        if gameboard.board[i][j] > 0:\n",
    "                            pygame.draw.rect(screen,COLOR_BLACK,[101+20*j,81+20*(gameboard.N_row-i),18,18])\n",
    "\n",
    "                if gameboard.cur_tile_type is not None:\n",
    "                    curTile=gameboard.tiles[gameboard.cur_tile_type][gameboard.tile_orientation]\n",
    "                    for xLoop in range(len(curTile)):\n",
    "                        for yLoop in range(curTile[xLoop][0],curTile[xLoop][1]):\n",
    "                            pygame.draw.rect(screen,COLOR_RED,[101+20*((xLoop+gameboard.tile_x)%gameboard.N_col),81+20*(gameboard.N_row-(yLoop+gameboard.tile_y)),18,18])\n",
    "\n",
    "                screen.blit(font.render(\"Reward: \"+str(agent.reward_tots[agent.episode]),True,COLOR_BLACK),[0,0])\n",
    "                screen.blit(font.render(\"Tile \"+str(gameboard.tile_count)+\"/\"+str(gameboard.max_tile_count),True,COLOR_BLACK),[0,20])\n",
    "                if framerate>0:\n",
    "                    screen.blit(font.render(\"FPS: \"+str(framerate),True,COLOR_BLACK),[320,0])\n",
    "                screen.blit(font.render(\"Reward: \"+str(agent.reward_tots[agent.episode]),True,COLOR_BLACK),[0,0])\n",
    "                if gameboard.gameover:\n",
    "                    screen.blit(fontLarge.render(\"Game Over\", True,COLOR_RED), [80, 200])\n",
    "                    screen.blit(font.render(\"Press ESC to try again\", True,COLOR_RED), [85, 265])\n",
    "\n",
    "                pygame.display.flip()\n",
    "                clock.tick(framerate)\n",
    "\n",
    "Evaluate(gameboard,player)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define standard Q learning agent\n",
    "\n",
    "def convertToBinary(s,state_size=18):\n",
    "    binary = np.array([int(i) for i in bin(s)[2:]])\n",
    "    binary = np.pad(binary, (state_size - len(binary), 0), 'constant')\n",
    "    return binary\n",
    "\n",
    "class QLAgent:\n",
    "    def __init__(self,alpha,epsilon,episode_count):\n",
    "        # Initialize training parameters\n",
    "        self.alpha=alpha\n",
    "        self.epsilon=epsilon\n",
    "        self.episode=0\n",
    "        self.episode_count=episode_count\n",
    "\n",
    "    def init_board(self,gameboard):\n",
    "        \n",
    "        #Set up and initialize the states, actions and Q-table and storage for the rewards\n",
    "        self.gameboard=gameboard\n",
    "        self.nr_of_rows = gameboard.N_row\n",
    "        self.nr_of_cols = gameboard.N_col\n",
    "        self.len_of_tiles = len(bin(len(gameboard.tiles)-1)[2:])\n",
    "        self.board = gameboard.board\n",
    "        self.tiles = gameboard.tiles\n",
    "        self.tile_sequence = gameboard.tile_sequence\n",
    "        self.reward_tots = np.array([0]*self.episode_count)\n",
    "        \n",
    "        self.nr_of_rotations=4\n",
    "        self.nr_of_moves = self.nr_of_cols-1\n",
    "        self.state_size = gameboard.N_col*gameboard.N_row+self.len_of_tiles\n",
    "        self.nr_of_states = 2**self.state_size\n",
    "        self.action_size = self.nr_of_moves*self.nr_of_rotations\n",
    "        \n",
    "        self.Q_table = np.zeros((self.nr_of_states,self.action_size))\n",
    "        self.Q_target = self.Q_table\n",
    "\n",
    "    def load_strategy(self, strategy_file):\n",
    "        self.Q_table = np.loadtxt(strategy_file)\n",
    "        self.Q_target = self.Q_table\n",
    "\n",
    "    def get_state(self):\n",
    "        #Convert board to binary list\n",
    "        board = np.copy(self.gameboard.board.reshape(self.nr_of_cols*self.nr_of_rows,)).astype(int)\n",
    "        board[board==-1] = 0\n",
    "        #Convert tile to binary list\n",
    "        tile = np.copy(self.gameboard.cur_tile_type)\n",
    "        tile = convertToBinary(tile,self.len_of_tiles)   \n",
    "        #Add binary lists\n",
    "        state = np.append(tile,board)    \n",
    "        #Convert binary list to int\n",
    "        state = int(\"\".join(str(i) for i in state),2)\n",
    "        self.state = state\n",
    "        \n",
    "    def set_action(self):\n",
    "        # Select action using epsilon-greedy policy\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            action = np.random.randint(self.nr_of_moves*self.nr_of_rotations)            \n",
    "        else:\n",
    "            action = np.argmax(self.Q_table[self.state,:]) \n",
    "        #Extract rotation and movement from action parameter \n",
    "        tile_orientation = action % self.nr_of_rotations\n",
    "        tile_x = int(action/self.nr_of_rotations)\n",
    "        self.legalAction = self.gameboard.fn_move(tile_x,tile_orientation)\n",
    "        self.action = action\n",
    "        \n",
    "    def train(self,old_state,reward):\n",
    "        # Update the Q table using state and action stored as attributes in self and using function arguments for the old state and the reward\n",
    "        if self.legalAction == 0:\n",
    "            self.Q_table[old_state,self.action] += self.alpha*(reward+np.max(self.Q_table[self.state,:])-self.Q_table[old_state,self.action])\n",
    "        else:\n",
    "            self.Q_table[old_state,self.action] += -50 #penalty for illegal move\n",
    "\n",
    "    def next_turn(self):\n",
    "        if self.gameboard.gameover:\n",
    "            self.episode+=1\n",
    "            if self.episode%100==0:\n",
    "                self.Q_target = self.Q_table\n",
    "                print('episode '+str(self.episode)+'/'+str(self.episode_count)+' (reward: ',str(np.sum(self.reward_tots[range(self.episode-100,self.episode)])),')')\n",
    "            if self.episode>=self.episode_count:\n",
    "                np.savetxt('Q_table.txt',self.Q_table)\n",
    "                raise SystemExit(\"Training finished\")\n",
    "            else:\n",
    "                self.gameboard.fn_restart()\n",
    "        else:\n",
    "            # Select and execute action (move the tile to the desired column and orientation)\n",
    "            self.set_action()\n",
    "            old_state=np.copy(self.state)\n",
    "            # Drop the tile on the game board\n",
    "            reward=self.gameboard.fn_drop()   \n",
    "            self.reward_tots[self.episode]+=reward\n",
    "            # Read the new state\n",
    "            self.get_state()\n",
    "            # Update the Q-table using the old state and the reward\n",
    "            self.train(old_state,reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standard Q-learning to play Tetris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 100/1000 (reward:  -8906 )\n",
      "episode 200/1000 (reward:  -7313 )\n",
      "episode 300/1000 (reward:  -6154 )\n",
      "episode 400/1000 (reward:  -5479 )\n",
      "episode 500/1000 (reward:  -3031 )\n",
      "episode 600/1000 (reward:  6400 )\n",
      "episode 700/1000 (reward:  6400 )\n",
      "episode 800/1000 (reward:  6400 )\n",
      "episode 900/1000 (reward:  6400 )\n",
      "episode 1000/1000 (reward:  6400 )\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "Training finished",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m Training finished\n"
     ]
    }
   ],
   "source": [
    "agent=QLAgent(alpha,epsilon,episode_count)\n",
    "gameboard=gameboardClass.TGameBoard(N_row,N_col,tile_size,max_tile_count,agent,stochastic_prob)\n",
    "\n",
    "while True:\n",
    "    gameboard.agent.next_turn()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mylabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReward\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpisode\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m \u001b[43mEvaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgameboard\u001b[49m\u001b[43m,\u001b[49m\u001b[43mplayer\u001b[49m\u001b[43m,\u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43mstrategy_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mQ_table.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 47\u001b[0m, in \u001b[0;36mEvaluate\u001b[0;34m(gameboard, agent, agent_evaluate, strategy_file, evaluate_agent)\u001b[0m\n\u001b[1;32m     44\u001b[0m     gameboard\u001b[38;5;241m.\u001b[39magent\u001b[38;5;241m.\u001b[39mnext_turn()\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m evaluate_agent:\n\u001b[0;32m---> 47\u001b[0m     \u001b[43magent_evaluate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m     agent_evaluate\u001b[38;5;241m.\u001b[39mset_action()\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pygame\u001b[38;5;241m.\u001b[39mdisplay\u001b[38;5;241m.\u001b[39mget_active():\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;66;03m# Paint game board\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 47\u001b[0m, in \u001b[0;36mQLAgent.get_state\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m#Convert tile to binary list\u001b[39;00m\n\u001b[1;32m     46\u001b[0m tile \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mcopy(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgameboard\u001b[38;5;241m.\u001b[39mcur_tile_type)\n\u001b[0;32m---> 47\u001b[0m tile \u001b[38;5;241m=\u001b[39m \u001b[43mconvertToBinary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtile\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlen_of_tiles\u001b[49m\u001b[43m)\u001b[49m   \n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m#Add binary lists\u001b[39;00m\n\u001b[1;32m     49\u001b[0m state \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mappend(tile,board)    \n",
      "Cell \u001b[0;32mIn[4], line 5\u001b[0m, in \u001b[0;36mconvertToBinary\u001b[0;34m(s, state_size)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvertToBinary\u001b[39m(s,state_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m18\u001b[39m):\n\u001b[1;32m      4\u001b[0m     binary \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;28mint\u001b[39m(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mbin\u001b[39m(s)[\u001b[38;5;241m2\u001b[39m:]])\n\u001b[0;32m----> 5\u001b[0m     binary \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbinary\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbinary\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mconstant\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m binary\n",
      "File \u001b[0;32m~/miniconda3/envs/py310/lib/python3.10/site-packages/numpy/lib/arraypad.py:808\u001b[0m, in \u001b[0;36mpad\u001b[0;34m(array, pad_width, mode, **kwargs)\u001b[0m\n\u001b[1;32m    806\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m axis, width_pair, value_pair \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(axes, pad_width, values):\n\u001b[1;32m    807\u001b[0m         roi \u001b[38;5;241m=\u001b[39m _view_roi(padded, original_area_slice, axis)\n\u001b[0;32m--> 808\u001b[0m         \u001b[43m_set_pad_area\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwidth_pair\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue_pair\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    810\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mempty\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    811\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m  \u001b[38;5;66;03m# Do nothing as _pad_simple already returned the correct result\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/py310/lib/python3.10/site-packages/numpy/lib/arraypad.py:150\u001b[0m, in \u001b[0;36m_set_pad_area\u001b[0;34m(padded, axis, width_pair, value_pair)\u001b[0m\n\u001b[1;32m    146\u001b[0m left_slice \u001b[38;5;241m=\u001b[39m _slice_at_axis(\u001b[38;5;28mslice\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m, width_pair[\u001b[38;5;241m0\u001b[39m]), axis)\n\u001b[1;32m    147\u001b[0m padded[left_slice] \u001b[38;5;241m=\u001b[39m value_pair[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    149\u001b[0m right_slice \u001b[38;5;241m=\u001b[39m _slice_at_axis(\n\u001b[0;32m--> 150\u001b[0m     \u001b[38;5;28;43mslice\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpadded\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mwidth_pair\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m, axis)\n\u001b[1;32m    151\u001b[0m padded[right_slice] \u001b[38;5;241m=\u001b[39m value_pair[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkcAAAGwCAYAAACjPMHLAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAAA0VElEQVR4nO3deXxU9b3/8fckIUMIybAEskAIQbEsccHEsogFXACrWJVriyAStXgRkNWiCFZAIbYFL49yK168mC5i8Vq1PxVFUBZLAaGQaFhcqCBoEhCEDOtk+/7+wIwzBDJDMslk5ryej8d5PDLnfOfkM99Y5t3v93vOsRljjAAAACBJigh2AQAAAI0J4QgAAMAD4QgAAMAD4QgAAMAD4QgAAMAD4QgAAMAD4QgAAMBDVLALCEWVlZUqLCxUXFycbDZbsMsBAAB+MMbo+PHjSklJUUTEhceHCEe1UFhYqNTU1GCXAQAAauHAgQNq3779BY8TjmohLi5O0tnOjY+PD3I1AADAH06nU6mpqe7v8QshHNVC1VRafHw84QgAgBDja0kMC7IBAAA8EI4AAAA8EI4AAAA8EI4AAAA8EI4AAAA8EI4AAAA8EI4AAAA8EI4AAAA8EI4AAAA8EI4AAAA8EI4AAAA8EI4AAAA88OBZAJZhjFFhyRkZY4JdCgAf2rWI8fmA2PpCOAJgGQ//NU9vf1IU7DIA+OHzp29WdBThCADq1cdfH5MkRUdGKEj/hxRACCAcAbCMqtm0V/6zl3p0aBncYgA0WizIBgAA8EA4AmAZVSNHwVrkCSA0EI4AAAA8EI4AWA7jRgBqQjgCAADwQDgCYBlVN39kyRGAmhCOAAAAPBCOAFhG1UNDbKw6AlADwhEAAIAHwhEAy/jhPkfBrQNA40Y4AgAA8EA4AmAZxr3qCAAujHAEAADggXAEwDJYcwTAH4QjAAAAD4QjAJbBfY4A+INwBAAA4IFwBMAyWHMEwB+EIwAAAA+EIwAWcnboiJEjADUhHAEAAHggHAGwDPeaI65WA1ADwhEAAICHsApHs2bNks1m89qSkpLcx40xmjVrllJSUhQTE6P+/ftr586dQawYQENy3+eIgSMANQircCRJ3bt3V1FRkXsrKChwH/vtb3+rZ599Vv/93/+trVu3KikpSTfddJOOHz8exIoBAEBjEnbhKCoqSklJSe6tTZs2ks6OGi1cuFAzZszQnXfeqYyMDP3pT3/SqVOn9PLLLwe5agANwXy/6IiBIwA1Cbtw9MUXXyglJUXp6ekaNmyYvvzyS0nS3r17VVxcrIEDB7rb2u129evXTxs3bqzxnC6XS06n02sDAADhKazCUc+ePfXnP/9Z7733nl544QUVFxerT58+OnLkiIqLiyVJiYmJXu9JTEx0H7uQnJwcORwO95aamlpvnwFA/WHNEQB/hFU4uvnmmzV06FBdfvnluvHGG7VixQpJ0p/+9Cd3G9s5/yoaY6rtO9f06dNVUlLi3g4cOBD44gEAQKMQVuHoXLGxsbr88sv1xRdfuK9aO3eU6NChQ9VGk85lt9sVHx/vtQEIPVX3OWLVEYCahHU4crlc2r17t5KTk5Wenq6kpCStXr3afby0tFTr169Xnz59glglAABoTKKCXUAgPfLIIxoyZIg6dOigQ4cO6emnn5bT6dSoUaNks9k0adIkzZs3T507d1bnzp01b948NWvWTMOHDw926QAagPtqNQaOANQgrMLR119/rbvvvluHDx9WmzZt1KtXL23evFlpaWmSpGnTpun06dMaO3asjh49qp49e2rVqlWKi4sLcuUAAKCxsBnzwyw8/ON0OuVwOFRSUsL6IyCEXD7rPR0/U641U/upU5vmwS4HQAPz9/s7rNccAQAAXCzCEQDr+H6c3NftOwBYG+EIAADAA+EIgGW475Ad1CoANHaEIwAAAA+EIwCWwX2OAPiDcAQAAOCBcATAMn5Yc8TQEYALIxwBAAB4IBwBsAzjvs9RcOsA0LgRjgBYhhFPSwLgG+EIAADAA+EIgGXwmG0A/iAcAbAc1hwBqAnhCIBlMHAEwB+EIwCWY2PoCEANCEcArIOhIwB+IBwBsBzGjQDUhHAEwDK4zxEAfxCOAFgOS44A1IRwBMAyuM8RAH8QjgBYjo1VRwBqQDgCYBkMHAHwB+EIgOWw5ghATQhHACzDsOgIgB8IRwAsh4EjADUhHAGwDPe4EekIQA0IRwAAAB4IRwAso2rJEZfyA6gJ4QgAAMAD4QiA5XApP4CaEI4AAAA8EI4AWILnPY4YOAJQE8IRAACAB8IRAEvwvDm2jUVHAGpAOAIAAPBAOAJgCZ5PVWPcCEBNCEcAAAAeCEcALMHrajWGjgDUgHAEAADgIazCUU5Ojq655hrFxcWpbdu2uv322/XZZ595tcnOzpbNZvPaevXqFaSKATQU7zVHDB0BuLCwCkfr16/XuHHjtHnzZq1evVrl5eUaOHCgTp486dVu8ODBKioqcm/vvPNOkCoGAACNTVSwCwiklStXer3Ozc1V27ZttW3bNv3kJz9x77fb7UpKSmro8gAEkeFyNQB+CquRo3OVlJRIklq1auW1f926dWrbtq0uu+wyjR49WocOHarxPC6XS06n02sDAADhKWzDkTFGU6ZMUd++fZWRkeHef/PNN2vZsmVas2aNFixYoK1bt+r666+Xy+W64LlycnLkcDjcW2pqakN8BAABZMTVagD8YzPGa7A5bIwbN04rVqzQhg0b1L59+wu2KyoqUlpampYvX64777zzvG1cLpdXeHI6nUpNTVVJSYni4+MDXjuAwHOVV+hHM89OvX8ya6DimzYJckUAGprT6ZTD4fD5/R1Wa46qPPzww3rzzTf14Ycf1hiMJCk5OVlpaWn64osvLtjGbrfLbrcHukwADcjr2WrBKwNACAircGSM0cMPP6w33nhD69atU3p6us/3HDlyRAcOHFBycnIDVAgAABq7sFpzNG7cOL300kt6+eWXFRcXp+LiYhUXF+v06dOSpBMnTuiRRx7Rpk2btG/fPq1bt05DhgxRQkKC7rjjjiBXD6Ch2Fh0BKAGYTVytHjxYklS//79vfbn5uYqOztbkZGRKigo0J///GcdO3ZMycnJGjBggF555RXFxcUFoWIAANDYhFU48rW2PCYmRu+9914DVQOgMWHNEQB/hdW0GgAAQF0RjgBYAvc5AuAvwhEAAIAHwhEAS/Bec8TQEYALIxwBAAB4IBwBsATPa1lZcwSgJoQjAAAAD4QjAJYQps/YBlAPCEcALIFoBMBfhCMAlsOaIwA1IRwBsARm1QD4i3AEwHK4zxGAmhCOAFgDI0cA/EQ4AmA5rDkCUBPCEQBLMAwdAfAT4QiA5TBwBKAmhCMAlsDVagD8RTgCYDk2Fh0BqAHhCIAlMHAEwF+EIwCWw7gRgJoQjgBYAg+eBeAvwhEAy2HJEYCaEI4AWALjRgD8RTgCYDlcrQagJoQjAJbAkiMA/iIcAQAAeCAcAbAEnq0GwF+EIwCWwnIjAL4QjgBYAwNHAPxEOAJgKQwcAfCFcATAEhg4AuAvwhEAS+EeRwB8IRwBsATucwTAX4QjAJbCuBEAXwhHACyB+xwB8BfhCIClsOQIgC+EIwCWwJojAP4iHAGwFBurjgD4QDgCYAnugSOyEQAfCEcAAAAeLBuOnnvuOaWnp6tp06bKzMzUP/7xj2CXBKAeme8XHTFwBMAXS4ajV155RZMmTdKMGTOUl5en6667TjfffLP2798f7NIAAECQWTIcPfvss3rggQf0y1/+Ul27dtXChQuVmpqqxYsXB7s0APWk6mo1LuUH4IvlwlFpaam2bdumgQMHeu0fOHCgNm7ceN73uFwuOZ1Orw0AAIQny4Wjw4cPq6KiQomJiV77ExMTVVxcfN735OTkyOFwuLfU1NSGKBVAPeBSfgC+WC4cVTn3ydzGmAs+rXv69OkqKSlxbwcOHGiIEgEAQBBEBbuAhpaQkKDIyMhqo0SHDh2qNppUxW63y263N0R5AOoJa44A+MtyI0fR0dHKzMzU6tWrvfavXr1affr0CVJVAACgsbDcyJEkTZkyRSNHjlRWVpZ69+6tJUuWaP/+/RozZkywSwNQT4y4zxEA/1gyHP3iF7/QkSNHNGfOHBUVFSkjI0PvvPOO0tLSgl0aAAAIMkuGI0kaO3asxo4dG+wyADSQH9YcMXYEoGaWW3MEAABQE8IRAEv4fuCINUcAfCIcAQAAeCAcAbAE4150FNw6ADR+hCMAAAAPhCMAlsCaIwD+IhwBsISqWTUA8IVwBMBSuM8RAF8IRwAsgqEjAP7x+w7Zd955p98nff3112tVDADUNwaOAPji98iRw+Fwb/Hx8frggw/0r3/9y31827Zt+uCDD+RwOOqlUACoC9YcAfCX3yNHubm57p8fffRR/fznP9fzzz+vyMhISVJFRYXGjh2r+Pj4wFcJAAHCwBEAX2q15ujFF1/UI4884g5GkhQZGakpU6boxRdfDFhxABAoDBwB8FetwlF5ebl2795dbf/u3btVWVlZ56IAoL5wtRoAX/yeVvN033336f7779eePXvUq1cvSdLmzZv1zDPP6L777gtogQAQCKw5AuCvWoWj+fPnKykpSf/1X/+loqIiSVJycrKmTZumqVOnBrRAAAgkxo0A+HLR4ai8vFzLli3Tvffeq2nTpsnpdEoSC7EBNGqGVUcA/HTRa46ioqL00EMPyeVySTobighGAEIFS44A+FKrBdk9e/ZUXl5eoGsBgHrDmiMA/qrVmqOxY8dq6tSp+vrrr5WZmanY2Fiv41dccUVAigOAwGPoCEDNahWOfvGLX0iSJkyY4N5ns9lkjJHNZlNFRUVgqgOAAGHkCIC/ahWO9u7dG+g6AKBBsOYIgC+1CkdpaWmBrgMA6hVXqwHwV63CUZVdu3Zp//79Ki0t9dp/22231akoAKgvDBwB8KVW4ejLL7/UHXfcoYKCAvdaI+mH2/Kz5ghAY8OaIwD+qtWl/BMnTlR6eroOHjyoZs2aaefOnfrwww+VlZWldevWBbhEAAgc1hwB8KVWI0ebNm3SmjVr1KZNG0VERCgiIkJ9+/ZVTk6OJkyYwD2QAABAyKrVyFFFRYWaN28uSUpISFBhYaGkswu1P/vss8BVBwABZmPVEQAfajVylJGRoU8++USdOnVSz5499dvf/lbR0dFasmSJOnXqFOgaAaDOWHMEwF+1CkczZ87UyZMnJUlPP/20br31Vl133XVq3bq1XnnllYAWCACBxJojAL7UKhwNGjTI/XOnTp20a9cufffdd2rZsqX7ijUAaEy4zxEAf9VqzdHq1at16tQpr32tWrUiGAFo9PhXCoAvtRo5Gjp0qFwulzIzM9WvXz/1799f1157rXuRNgA0Nqw5AuCvWo0cHT16VOvWrdNtt92mvLw83XXXXWrVqpV69eqlxx57LNA1AkDAMMINwJdahaPIyEj17t1bjz32mFauXKmNGzdq+PDh2rZtm373u98FukYAqDMGjgD4q1bTart379b69eu1bt06rV+/XhUVFerbt68WLFigfv36BbpGAACABlOrcNS9e3e1adNGkyZN0hNPPKHu3bsHui4ACCjDoiMAfqrVtNqECRPUrl07zZo1S/fff78effRRvfvuuzpx4kSg6wOAgGLJEQBfahWOFi5cqO3bt+vgwYOaOXOmKioq9Otf/1oJCQnq1atXoGsEgDpj3AiAv2oVjqpUVlaqvLxcpaWlcrlcKisr0759+wJUGgAEHiNHAHypVTiaOHGirrzySrVt21b/+Z//qcLCQj344IP6+OOPVVxcHOgaAaDOWHIEwF+1CkfffPONRo8erfz8fB06dEh/+9vfNH78eGVkZAS6Pr/t27dPDzzwgNLT0xUTE6NLLrlETz75pEpLS73a2Wy2atvzzz8fpKoBNDQb98gG4EOtrlb729/+Fug66uzTTz9VZWWl/ud//keXXnqpduzYodGjR+vkyZOaP3++V9vc3FwNHjzY/drhcDR0uQAaHENHAPxTq3AkSX/5y1/0/PPPa+/evdq0aZPS0tK0cOFCpaen62c/+1kga/TL4MGDvQJPp06d9Nlnn2nx4sXVwlGLFi2UlJTk97ldLpdcLpf7tdPprHvBAIKCNUcAfKnVtNrixYs1ZcoU/fSnP9WxY8dUUVEh6WzoWLhwYSDrq5OSkhK1atWq2v7x48crISFB11xzjZ5//nlVVlbWeJ6cnBw5HA73lpqaWl8lA6gnrDkC4K9ahaNFixbphRde0IwZMxQZGenen5WVpYKCgoAVVxf//ve/tWjRIo0ZM8Zr/1NPPaVXX31V77//voYNG6apU6dq3rx5NZ5r+vTpKikpcW8HDhyoz9IB1CMGjgD4Uqtptb1796pHjx7V9tvtdp08ebLORXmaNWuWZs+eXWObrVu3Kisry/26sLBQgwcP1l133aVf/vKXXm1nzpzp/vmqq66SJM2ZM8dr/7nsdrvsdnstqgfQWDBwBMBftQpH6enpys/PV1pamtf+d999V127dg1IYVXGjx+vYcOG1dimY8eO7p8LCws1YMAA9e7dW0uWLPF5/l69esnpdOrgwYNKTEysa7kAGjkbi44A+FCrcPSrX/1K48aN05kzZ2SM0ZYtW/TXv/5V8+bN09KlSwNaYEJCghISEvxq+80332jAgAHKzMxUbm6uIiJ8zxrm5eWpadOmatGiRR0rBdCYseYIgL9qFY7uu+8+lZeXa9q0aTp16pSGDx+udu3aadGiRbruuusCXaNfCgsL1b9/f3Xo0EHz58/Xt99+6z5WdWXaW2+9peLiYvXu3VsxMTFau3atZsyYoQcffJBpM8AiGDcC4EutL+UfPXq0Ro8ercOHD6uyslIVFRWaN2+exo0bp9OnTweyRr+sWrVKe/bs0Z49e9S+fXuvY1VP427SpImee+45TZkyRZWVlerUqZPmzJmjcePGNXi9ABpW1b8DpCMAvlzU1WrHjh3TiBEj1KZNG6WkpOj3v/+9WrVqpT/84Q+69NJLtXnzZr344ov1VWuNsrOzZYw571Zl8ODBysvL0/Hjx3Xy5EkVFBRo4sSJioqqdUYEAABh5qJSweOPP64PP/xQo0aN0sqVKzV58mStXLlSZ86c0TvvvKN+/frVV50AUCdV/zeJgSMAvlxUOFqxYoVyc3N14403auzYsbr00kt12WWXNaobPwIAANTFRU2rFRYWqlu3bpLOPp6jadOm1e4jBACNkXvJEZfyA/DhosJRZWWlmjRp4n4dGRmp2NjYgBcFAAAQLBc1rWaMUXZ2tvuy9zNnzmjMmDHVAtLrr78euAoBIADM96uOGDcC4MtFhaNRo0Z5vb7nnnsCWgwA1BtuAgnATxcVjnJzc+urDgBoECw5AuDLRa05AoBQxcARAH8RjgBYio1VRwB8IBwBsAQePAvAX4QjAJbCmiMAvhCOAFiCYdURAD8RjgAAADwQjgBYAmuOAPiLcATAUni2GgBfCEcALIGBIwD+IhwBsBTGjQD4QjgCYAmGRUcA/EQ4AmApLDkC4AvhCIAlMG4EwF+EIwCWwsgRAF8IRwCsgaEjAH4iHAGwFBvXqwHwgXAEwBJ4thoAfxGOAFgKa44A+EI4AmAJ3OYIgL8IRwAshYEjAL4QjgBYAiNHAPxFOAJgLSw6AuAD4QiAJTBwBMBfhCMAlsK4EQBfCEcALMGw6AiAnwhHACyFJUcAfCEcAbAExo0A+ItwBMBSGDgC4AvhCIAlsOQIgL8IRwDCXnlFpWa/tVOSZGPREQAfCEcAwt6GPYdVVHJGkpTQPDrI1QBo7AhHAMLe9v3HJEn2qAjNu+Py4BYDoNEjHAEIe3n7j0qSZt7SVa2b24NcDYDGLqzCUceOHWWz2by2xx57zKvN/v37NWTIEMXGxiohIUETJkxQaWlpkCoGUN8qK43yDxyTJPXo0DK4xQAICVHBLiDQ5syZo9GjR7tfN2/e3P1zRUWFbrnlFrVp00YbNmzQkSNHNGrUKBljtGjRomCUC6Ce/fvbEzp+plxNm0SoS1JcsMsBEALCLhzFxcUpKSnpvMdWrVqlXbt26cCBA0pJSZEkLViwQNnZ2Zo7d67i4+PP+z6XyyWXy+V+7XQ6A184gHqR9/16oyvat1BUZFgNlgOoJ2H3L8VvfvMbtW7dWldddZXmzp3rNWW2adMmZWRkuIORJA0aNEgul0vbtm274DlzcnLkcDjcW2pqar1+BgCBs6OwRJJ0VWqL4BYCIGSE1cjRxIkTdfXVV6tly5basmWLpk+frr179+p///d/JUnFxcVKTEz0ek/Lli0VHR2t4uLiC553+vTpmjJlivu10+kkIAEhovDY2Uv4O7RqFuRKAISKRh+OZs2apdmzZ9fYZuvWrcrKytLkyZPd+6644gq1bNlS//Ef/+EeTZLOfwM4Y0yNN4az2+2y27nCBQhFxc7TkqRkR9MgVwIgVDT6cDR+/HgNGzasxjYdO3Y87/5evXpJkvbs2aPWrVsrKSlJH330kVebo0ePqqysrNqIEoDQse/wSa3aVazK8zwi5KvDpyRJyY6YBq4KQKhq9OEoISFBCQkJtXpvXl6eJCk5OVmS1Lt3b82dO1dFRUXufatWrZLdbldmZmZgCgbQ4Cb/X7574fX52GxSSgtGjgD4p9GHI39t2rRJmzdv1oABA+RwOLR161ZNnjxZt912mzp06CBJGjhwoLp166aRI0fqd7/7nb777js98sgjGj169AWvVAPCWWl5pd7dUSTn6bJqxzLaOdSjQ0uVV1RqRcH52zQGRtInX59ddH37VSnnvSLtmo4t1aIZjw0B4J+wCUd2u12vvPKKZs+eLZfLpbS0NI0ePVrTpk1zt4mMjNSKFSs0duxYXXvttYqJidHw4cM1f/78IFYOBM8r/zqgJ/6+47zHmjaJ0JYZN2plQbGmvfZJA1d28ZIdTbVwWI9glwEgDIRNOLr66qu1efNmn+06dOigt99+uwEqAhqvklNlWv/Ft/qv1Z9LkrqnxHtdzfXPPYflPFOuFz78Uv/ad/bRG12T49WxdeO84stmk4Ze3T7YZQAIE2ETjgD474n/t0Nvflzofj3zlm7qfUlr9+tJy/P09/xCLVqzx73vsZu7qN9lbRq0TgAIBsIRYDHGGG3892GvfVe0d3i9HtP/Ep0qrdCZ8kpJUlqrZrrWIzwBQDgjHAEW8/XR0zp8wvthy7F2738KuiTFa8m9WQ1ZFgA0GoQjIAydcJXrkwPHznvfn637vpMkXdq2ua5o79DtV7Vr4OoAoHEjHAFhaMxftmnDnsM1tul7aYJm3da9gSoCgNBBOAJCkDFGXxw6oROu8mrHKiuNNn95RJL0o8Q4ne/JOHFNozSiZ4f6LhMAQhLhCAhBr/7ra5/3HmoTZ9fKSdfV+NxAAEB1hCMgyIwxOvDdaSU67LJHRXodq6g02nv4RLW1QysKiiRJrWKjFWv3fo8kRdhsuv/adIIRANQC4QgIsqUb9urpFbvVPSVeKyZc53Vs3LLtWrmz+ILvff6eTP04vVV9lwgAlkI4AoLs5S37JUk7C536+MAxtW5+9hlglZXS2s8OSZJaNmuiiHNGgbomx+uq1BYNWisAWAHhCAiilTuK9OW3J92vf/aHf1Zr44hpom0zb1JEBFNkANAQCEeAB2OMSk6XKdYepZPnuRIs0N76uMj9c2x0pMrPWVwUYbNpVO80ghEANCDCEeBhzEvb9N7Ogw3+e//ywI91XWeeWwYAjUFEsAsAGpN/fFHzjRPrQ+e2zZWVxqJqAGgsGDkCPFScM63173k/rfffGWETl9wDQCNCOAI8nLvmJ5K1PgBgOUyrAd+rrDTVRo4AANZDOAK+V1ZZGewSAACNAOEI+F5ZBaNGAADCEeBWVs7IEQCAcAS4Ma0GAJAIR4Ab02oAAIlwBLgxrQYAkAhHgFs502oAABGOALfScqbVAACEI8CtrIKRIwAA4QhwY1oNACARjgA3ptUAABLhCHBjWg0AIBGOADem1QAAEuEIcGNaDQAgEY4AN6bVAAAS4Qhw2/zlkWCXAABoBAhHwPf+X36h1+tB3RODVAkAIJiigl0A0BgsXvdvnXCVS5JeHt1T+w6f0q1XJge5KgBAMBCOYHk7vinRb1Z+KknqkhSnPpckqM8lQS4KABA0hCOEnO37j+qfXxwO2Pl2FJa4f55/15UBOy8AIDQRjhBSKiqN7svdqpLTZQE/968G/UgZ7RwBPy8AILQQjhAUew+f1NpPD+li7yx07FSpSk6XKaZJpG7v0S5g9cTHROmenmkBOx8AIHQRjhAUD720TZ8WH6/1+zPTWirnzssDWBEAAGeFTThat26dBgwYcN5jW7Zs0TXXXCNJstls1Y4vXrxYY8aMqdf6rOq7k6Va8+khVXg8mqOswriD0ZArU1T9L1KzqEib7r82PYBVAgDwg7AJR3369FFRUZHXvieeeELvv/++srKyvPbn5uZq8ODB7tcOB+tM6ssTf9+hFQVF5z3WKSFWi+7u0cAVAQBQs7AJR9HR0UpKSnK/Lisr05tvvqnx48dXGy1q0aKFV1v4dsJVrg1ffKuyiotbJbTx32evKutzSWs1i45077fZbBres0NAawQAIBBsxpiwfNrma6+9pp///Ofat2+fUlNT3fttNpvatWunM2fOKD09XQ888IAefPBBRURc+GbhLpdLLpfL/drpdCo1NVUlJSWKj4+v18/RWEx//RP9dcuBWr03OipCO2YNUnQUN2QHAASP0+mUw+Hw+f0dNiNH51q6dKkGDRrkFYwk6amnntINN9ygmJgYffDBB5o6daoOHz6smTNnXvBcOTk5mj17dn2X3Kh9c+yMJOmSNrFqE2e/qPfeckUKwQgAEDIa/cjRrFmzfAaTrVu3eq0r+vrrr5WWlqb/+7//09ChQ2t874IFCzRnzhyVlJRcsA0jR9LdSzZr05dHtOjuHhpyZUqwywEA4KKFzcjR+PHjNWzYsBrbdOzY0et1bm6uWrdurdtuu83n+Xv16iWn06mDBw8qMfH8Dxq12+2y2y9utCTclFWcvdqsSeTFXlsGAEBoafThKCEhQQkJCX63N8YoNzdX9957r5o0aeKzfV5enpo2baoWLVrUocrwV1Z5doCxSSTTYwCA8Nbow9HFWrNmjfbu3asHHnig2rG33npLxcXF6t27t2JiYrR27VrNmDFDDz74oOVHhnwpKz87chRFOAIAhLmwC0dLly5Vnz591LVr12rHmjRpoueee05TpkxRZWWlOnXqpDlz5mjcuHFBqDS0MK0GALCKsAtHL7/88gWPDR482Ovmj/Bf+ffTatGMHAEAwhzfdPBLKdNqAACL4JsOfmFaDQBgFYQj+KWcq9UAABbBNx38UnW1GuEIABDu+KaDX8oqmVYDAFgD4Qh+KatgWg0AYA1808GnykqjCtYcAQAsgm86+FQ1pSZJUUyrAQDCHOEIPlVNqUncBBIAEP74poNPVVeqSUyrAQDCH9908KlqWs1mkyIjmFYDAIQ3whF84ko1AICVhN2DZ1E7hcdOa/zL23X0VFm1Y+5HhzBqBACwAMIRJEl/z/9G2/cfq7FNpzbNG6YYAACCiHBkMR/sPqhFa/Yo/8AxpSfEKtYeKUn65uhpSdIDfdN1c0bSed/bLSW+weoEACBYCEcW899rzwYjSdp7+KTXMZtNuiurvbokEYIAANZFOLIQV3mFdn7j9Nr3x/uucf+c7IjRj5LiGrosAAAaFcKRhewsdKq04od7Fl3fpa36/6htECsCAKDxIRyFEVd5hWa/tUs7vinRQecZdU9xeB0vKjkjSbqhS1tNvukydWjdLBhlAgDQqBGOwsgHuw/p5Y/2u18fdB46b7vel7RWRjvHeY8BAGB1hKMwsv2ro16vh17dXj07tfLaF2eP0g1dExuyLAAAQgrhKIzkfX8VWpUx/TqpcyILrAEAuBiEoxDmKq/QH9b+W4ecZ9cSFXxdIknKzb5GcU2jCEYAANQC4SiE/ePzw/r9B1947WsbZ1f/H7WRzcajPgAAqA3CUQg77jr7HLS01s3086xUSdJPOhOMAACoC8JRCCurMJKkTgmxGjfg0iBXAwBAeIgIdgGovbLvb+jYJJI/IwAAgcK3aggrKyccAQAQaHyrhrDyyrPTak0iWWMEAECgEI5CWCnTagAABBzfqiGs/PsF2VGEIwAAAoZv1RBWtSA7mmk1AAAChnAUwphWAwAg8PhWDWFMqwEAEHh8q4YwptUAAAg8wlEIq7pDNtNqAAAEDt+qIaxq5IhpNQAAAodv1RD2w+NDmFYDACBQCEchrGpBdnQUf0YAAAKFb9UQVnUpf1QEf0YAAAIlZL5V586dqz59+qhZs2Zq0aLFedvs379fQ4YMUWxsrBISEjRhwgSVlpZ6tSkoKFC/fv0UExOjdu3aac6cOTLGNMAnCLxyptUAAAi4qGAX4K/S0lLddddd6t27t5YuXVrteEVFhW655Ra1adNGGzZs0JEjRzRq1CgZY7Ro0SJJktPp1E033aQBAwZo69at+vzzz5Wdna3Y2FhNnTq1oT9SnZUxrQYAQMCFTDiaPXu2JOmPf/zjeY+vWrVKu3bt0oEDB5SSkiJJWrBggbKzszV37lzFx8dr2bJlOnPmjP74xz/KbrcrIyNDn3/+uZ599llNmTJFNltwR2C+PnrqotqfcJVLYloNAIBACplw5MumTZuUkZHhDkaSNGjQILlcLm3btk0DBgzQpk2b1K9fP9ntdq8206dP1759+5Senn7ec7tcLrlcLvdrp9NZL5/h+gXrVVpeedHvi2JaDQCAgAmbIYfi4mIlJiZ67WvZsqWio6NVXFx8wTZVr6vanE9OTo4cDod7S01NDXD1Z9mjIi5669i6ma7u0LJe6gEAwIqCOnI0a9Ys93TZhWzdulVZWVl+ne9802LGGK/957apWoxd05Ta9OnTNWXKFPdrp9NZLwGpYNaggJ8TAABcnKCGo/Hjx2vYsGE1tunYsaNf50pKStJHH33kte/o0aMqKytzjw4lJSVVGyE6dOiQJFUbUfJkt9u9puIAAED4Cmo4SkhIUEJCQkDO1bt3b82dO1dFRUVKTk6WdHaRtt1uV2ZmprvN448/rtLSUkVHR7vbpKSk+B3CAABAeAuZNUf79+9Xfn6+9u/fr4qKCuXn5ys/P18nTpyQJA0cOFDdunXTyJEjlZeXpw8++ECPPPKIRo8erfj4eEnS8OHDZbfblZ2drR07duiNN97QvHnzGsWVagAAoHGwmRC5A2J2drb+9Kc/Vdu/du1a9e/fX9LZADV27FitWbNGMTExGj58uObPn+81JVZQUKBx48Zpy5YtatmypcaMGaNf//rXFxWOnE6nHA6HSkpK3MELAAA0bv5+f4dMOGpMCEcAAIQef7+/Q2ZaDQAAoCEQjgAAADwQjgAAADwQjgAAADwQjgAAADwQjgAAADwQjgAAADwQjgAAADwQjgAAADwE9cGzoarqpuJOpzPIlQAAAH9VfW/7ejgI4agWjh8/LklKTU0NciUAAOBiHT9+XA6H44LHebZaLVRWVqqwsFBxcXEX9cBaX5xOp1JTU3XgwAGe2VaP6OeGQ183DPq5YdDPDaM++9kYo+PHjyslJUURERdeWcTIUS1ERESoffv29Xb++Ph4/ofXAOjnhkNfNwz6uWHQzw2jvvq5phGjKizIBgAA8EA4AgAA8EA4akTsdruefPJJ2e32YJcS1ujnhkNfNwz6uWHQzw2jMfQzC7IBAAA8MHIEAADggXAEAADggXAEAADggXAEAADggXDUiDz33HNKT09X06ZNlZmZqX/84x/BLilk5OTk6JprrlFcXJzatm2r22+/XZ999plXG2OMZs2apZSUFMXExKh///7auXOnVxuXy6WHH35YCQkJio2N1W233aavv/66IT9KSMnJyZHNZtOkSZPc++jnwPnmm290zz33qHXr1mrWrJmuuuoqbdu2zX2cvq678vJyzZw5U+np6YqJiVGnTp00Z84cVVZWutvQzxfvww8/1JAhQ5SSkiKbzaa///3vXscD1adHjx7VyJEj5XA45HA4NHLkSB07dqzuH8CgUVi+fLlp0qSJeeGFF8yuXbvMxIkTTWxsrPnqq6+CXVpIGDRokMnNzTU7duww+fn55pZbbjEdOnQwJ06ccLd55plnTFxcnHnttddMQUGB+cUvfmGSk5ON0+l0txkzZoxp166dWb16tdm+fbsZMGCAufLKK015eXkwPlajtmXLFtOxY0dzxRVXmIkTJ7r308+B8d1335m0tDSTnZ1tPvroI7N3717z/vvvmz179rjb0Nd19/TTT5vWrVubt99+2+zdu9e8+uqrpnnz5mbhwoXuNvTzxXvnnXfMjBkzzGuvvWYkmTfeeMPreKD6dPDgwSYjI8Ns3LjRbNy40WRkZJhbb721zvUTjhqJH//4x2bMmDFe+7p06WIee+yxIFUU2g4dOmQkmfXr1xtjjKmsrDRJSUnmmWeecbc5c+aMcTgc5vnnnzfGGHPs2DHTpEkTs3z5cnebb775xkRERJiVK1c27Ado5I4fP246d+5sVq9ebfr16+cOR/Rz4Dz66KOmb9++FzxOXwfGLbfcYu6//36vfXfeeae55557jDH0cyCcG44C1ae7du0ykszmzZvdbTZt2mQkmU8//bRONTOt1giUlpZq27ZtGjhwoNf+gQMHauPGjUGqKrSVlJRIklq1aiVJ2rt3r4qLi7362G63q1+/fu4+3rZtm8rKyrzapKSkKCMjg7/DOcaNG6dbbrlFN954o9d++jlw3nzzTWVlZemuu+5S27Zt1aNHD73wwgvu4/R1YPTt21cffPCBPv/8c0nSxx9/rA0bNuinP/2pJPq5PgSqTzdt2iSHw6GePXu62/Tq1UsOh6PO/c6DZxuBw4cPq6KiQomJiV77ExMTVVxcHKSqQpcxRlOmTFHfvn2VkZEhSe5+PF8ff/XVV+420dHRatmyZbU2/B1+sHz5cm3fvl1bt26tdox+Dpwvv/xSixcv1pQpU/T4449ry5YtmjBhgux2u+699176OkAeffRRlZSUqEuXLoqMjFRFRYXmzp2ru+++WxL/TdeHQPVpcXGx2rZtW+38bdu2rXO/E44aEZvN5vXaGFNtH3wbP368PvnkE23YsKHasdr0MX+HHxw4cEATJ07UqlWr1LRp0wu2o5/rrrKyUllZWZo3b54kqUePHtq5c6cWL16se++9192Ovq6bV155RS+99JJefvllde/eXfn5+Zo0aZJSUlI0atQodzv6OfAC0afnax+IfmdarRFISEhQZGRktaR76NChaskaNXv44Yf15ptvau3atWrfvr17f1JSkiTV2MdJSUkqLS3V0aNHL9jG6rZt26ZDhw4pMzNTUVFRioqK0vr16/X73/9eUVFR7n6in+suOTlZ3bp189rXtWtX7d+/XxL/TQfKr371Kz322GMaNmyYLr/8co0cOVKTJ09WTk6OJPq5PgSqT5OSknTw4MFq5//222/r3O+Eo0YgOjpamZmZWr16tdf+1atXq0+fPkGqKrQYYzR+/Hi9/vrrWrNmjdLT072Op6enKykpyauPS0tLtX79encfZ2ZmqkmTJl5tioqKtGPHDv4O37vhhhtUUFCg/Px895aVlaURI0YoPz9fnTp1op8D5Nprr612O4rPP/9caWlpkvhvOlBOnTqliAjvr8LIyEj3pfz0c+AFqk979+6tkpISbdmyxd3mo48+UklJSd37vU7LuREwVZfyL1261OzatctMmjTJxMbGmn379gW7tJDw0EMPGYfDYdatW2eKiorc26lTp9xtnnnmGeNwOMzrr79uCgoKzN13333eS0fbt29v3n//fbN9+3Zz/fXXW/pyXH94Xq1mDP0cKFu2bDFRUVFm7ty55osvvjDLli0zzZo1My+99JK7DX1dd6NGjTLt2rVzX8r/+uuvm4SEBDNt2jR3G/r54h0/ftzk5eWZvLw8I8k8++yzJi8vz317mkD16eDBg80VV1xhNm3aZDZt2mQuv/xyLuUPN3/4wx9MWlqaiY6ONldffbX7MnT4Jum8W25urrtNZWWlefLJJ01SUpKx2+3mJz/5iSkoKPA6z+nTp8348eNNq1atTExMjLn11lvN/v37G/jThJZzwxH9HDhvvfWWycjIMHa73XTp0sUsWbLE6zh9XXdOp9NMnDjRdOjQwTRt2tR06tTJzJgxw7hcLncb+vnirV279rz/Jo8aNcoYE7g+PXLkiBkxYoSJi4szcXFxZsSIEebo0aN1rt9mjDF1G3sCAAAIH6w5AgAA8EA4AgAA8EA4AgAA8EA4AgAA8EA4AgAA8EA4AgAA8EA4AgAA8EA4AgAA8EA4AmAZ+/btk81mU35+fr39juzsbN1+++31dn4A9Y9wBCBkZGdny2azVdsGDx7s1/tTU1NVVFSkjIyMeq4UQCiLCnYBAHAxBg8erNzcXK99drvdr/dGRkYqKSmpPsoCEEYYOQIQUux2u5KSkry2li1bSpJsNpsWL16sm2++WTExMUpPT9err77qfu+502pHjx7ViBEj1KZNG8XExKhz585ewaugoEDXX3+9YmJi1Lp1az344IM6ceKE+3hFRYWmTJmiFi1aqHXr1po2bZrOfVylMUa//e1v1alTJ8XExOjKK6/U3/72t3rsIQB1RTgCEFaeeOIJDR06VB9//LHuuece3X333dq9e/cF2+7atUvvvvuudu/ercWLFyshIUGSdOrUKQ0ePFgtW7bU1q1b9eqrr+r999/X+PHj3e9fsGCBXnzxRS1dulQbNmzQd999pzfeeMPrd8ycOVO5ublavHixdu7cqcmTJ+uee+7R+vXr668TANSNAYAQMWrUKBMZGWliY2O9tjlz5hhjjJFkxowZ4/Wenj17moceesgYY8zevXuNJJOXl2eMMWbIkCHmvvvuO+/vWrJkiWnZsqU5ceKEe9+KFStMRESEKS4uNsYYk5ycbJ555hn38bKyMtO+fXvzs5/9zBhjzIkTJ0zTpk3Nxo0bvc79wAMPmLvvvrv2HQGgXrHmCEBIGTBggBYvXuy1r1WrVu6fe/fu7XWsd+/eF7w67aGHHtLQoUO1fft2DRw4ULfffrv69OkjSdq9e7euvPJKxcbGuttfe+21qqys1GeffaamTZuqqKjI6/dFRUUpKyvLPbW2a9cunTlzRjfddJPX7y0tLVWPHj0u/sMDaBCEIwAhJTY2VpdeeulFvcdms513/80336yvvvpKK1as0Pvvv68bbrhB48aN0/z582WMueD7LrT/XJWVlZKkFStWqF27dl7H/F1EDqDhseYIQFjZvHlztdddunS5YPs2bdooOztbL730khYuXKglS5ZIkrp166b8/HydPHnS3faf//ynIiIidNlll8nhcCg5Odnr95WXl2vbtm3u1926dZPdbtf+/ft16aWXem2pqamB+sgAAoyRIwAhxeVyqbi42GtfVFSUeyH1q6++qqysLPXt21fLli3Tli1btHTp0vOe69e//rUyMzPVvXt3uVwuvf322+rataskacSIEXryySc1atQozZo1S99++60efvhhjRw5UomJiZKkiRMn6plnnlHnzp3VtWtXPfvsszp27Jj7/HFxcXrkkUc0efJkVVZWqm/fvnI6ndq4caOaN2+uUaNG1UMPAagrwhGAkLJy5UolJyd77fvRj36kTz/9VJI0e/ZsLV++XGPHjlVSUpKWLVumbt26nfdc0dHRmj59uvbt26eYmBhdd911Wr58uSSpWbNmeu+99zRx4kRdc801atasmYYOHapnn33W/f6pU6eqqKhI2dnZioiI0P3336877rhDJSUl7jZPPfWU2rZtq5ycHH355Zdq0aKFrr76aj3++OOB7hoAAWIz5pybcgBAiLLZbHrjjTd4fAeAOmHNEQAAgAfCEQAAgAfWHAEIG6wSABAIjBwBAAB4IBwBAAB4IBwBAAB4IBwBAAB4IBwBAAB4IBwBAAB4IBwBAAB4IBwBAAB4+P9SYmG9CjrG0AAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gameboard=gameboardClass.TGameBoard(N_row,N_col,tile_size,max_tile_count,player,stochastic_prob)\n",
    "plt.plot(agent.reward_tots)\n",
    "plt.ylabel('Reward')\n",
    "plt.xlabel('Episode')\n",
    "Evaluate(gameboard,player,agent,strategy_file='Q_table.txt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We consider the more challenging scenario of a random sequence of tiles. Here, we expect standard Q learning to not converge to an (even close to) optimal solution in finite time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stochastic_prob=1\n",
    "epsilon=0.001\n",
    "episode_count=200000\n",
    "\n",
    "agent=QLAgent(alpha,epsilon,episode_count)\n",
    "gameboard=gameboardClass.TGameBoard(N_row,N_col,tile_size,max_tile_count,agent,stochastic_prob)\n",
    "\n",
    "while True:\n",
    "    gameboard.agent.next_turn()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gameboard=gameboardClass.TGameBoard(N_row,N_col,tile_size,max_tile_count,player,stochastic_prob)\n",
    "plt.plot(agent.reward_tots)\n",
    "running_mean=np.zeros(len(agent.reward_tots))\n",
    "for i in range(len(agent.reward_tots)):\n",
    "    running_mean[i]=np.mean(agent.reward_tots[max(0,i-500):i])\n",
    "plt.plot(running_mean)\n",
    "plt.ylabel('Reward')\n",
    "plt.xlabel('Episode')\n",
    "Evaluate(gameboard,player,agent,strategy_file='Q_table.txt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possibly, we can also let the user play the game again in these more difficult conditions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Evaluate(gameboard,player)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./deeplay')\n",
    "import deeplay as dl\n",
    "import torch\n",
    "import random\n",
    "\n",
    "    \n",
    "class Quadruplet:\n",
    "    def __init__(self, state, action, reward, new_state, terminal):\n",
    "        self.state = state\n",
    "        self.action = action\n",
    "        self.new_state = new_state\n",
    "        self.reward = reward\n",
    "        self.terminal = terminal\n",
    "\n",
    "class DQLAgent(dl.Application):\n",
    "    # Agent for learning to play tetris using deep Q-learning\n",
    "    def __init__(self,q_net,target_net, alpha, epsilon, epsilon_scale, replay_buffer_size, batch_size, sync_target_episode_count, episode_count,**kwargs):\n",
    "        # Initialize training parameters\n",
    "        self.alpha = alpha\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_scale = epsilon_scale\n",
    "        self.replay_buffer_size = replay_buffer_size\n",
    "        self.batch_size = batch_size\n",
    "        self.sync_target_episode_count = sync_target_episode_count\n",
    "\n",
    "        self.q_net = q_net\n",
    "        self.target_net = target_net\n",
    "        self.episode = 0\n",
    "        self.episode_count = episode_count\n",
    "        self.max_reward=0\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def init_board(self, gameboard):\n",
    "\n",
    "        # Initialize board variables\n",
    "        self.gameboard = gameboard\n",
    "        self.nr_of_actions=gameboard.N_col*4\n",
    "        self.len_of_tiles = len(bin(len(gameboard.tiles)-1)[2:])\n",
    "        self.state_size = gameboard.N_col*gameboard.N_row+self.len_of_tiles\n",
    "\n",
    "        # Initialize networks and storage\n",
    "        self.reward_tots = np.zeros(self.episode_count)\n",
    "        self.exp_buffer = []\n",
    "\n",
    "    def load_strategy(self, strategy_file):\n",
    "        #Load model weights\n",
    "        model_state_dict = torch.load(strategy_file)\n",
    "        self.q_net.load_state_dict(model_state_dict)\n",
    "        self.target_net.load_state_dict(model_state_dict)\n",
    "\n",
    "    def get_state(self):\n",
    "        #Convert board to binary list\n",
    "        board = np.copy(self.gameboard.board.reshape(16,)).astype(int)\n",
    "        board[board==-1] = 0\n",
    "        #Convert tile to binary list\n",
    "        tile = np.copy(self.gameboard.cur_tile_type)\n",
    "        tile = convertToBinary(tile,self.len_of_tiles)   \n",
    "        #Combine binary lists and save as state\n",
    "        state = np.append(tile,board)    \n",
    "        self.state = torch.tensor(state, dtype=torch.float32)\n",
    "\n",
    "    def set_action(self):\n",
    "        r = random.random()\n",
    "        with torch.no_grad():\n",
    "            self.output = self.q_net(self.state.view(1, self.state_size)).detach().numpy()[0]\n",
    "            if np.random.rand() < self.epsilon:\n",
    "                action = np.random.randint(self.nr_of_actions)            \n",
    "            else:\n",
    "                action = np.argmax(self.output)\n",
    "            #Extract rotation and movement from action parameter \n",
    "            rotation = action % 4\n",
    "            position = int(action/4)  \n",
    "            self.legalAction = self.gameboard.fn_move(position,rotation)\n",
    "            self.action = action\n",
    " \n",
    "\n",
    "    def training_step(self, batch):\n",
    "        # Store states in a list\n",
    "        states = []\n",
    "        next_states = []\n",
    "        for quadruple in batch:\n",
    "            states.append(quadruple.state)\n",
    "            next_states.append(quadruple.new_state)\n",
    "        # Initialize targets and target mask\n",
    "        targets = torch.zeros(self.batch_size, self.nr_of_actions)\n",
    "        targets_mask = torch.zeros(self.batch_size, self.nr_of_actions)\n",
    "        # Evaluate next state with target network\n",
    "        with torch.no_grad():\n",
    "            q_hat = self.target_net(torch.stack(next_states, dim=0))\n",
    "        # Computes targets\n",
    "        for idx, quadruple in enumerate(batch):\n",
    "            if quadruple.terminal:\n",
    "                y = quadruple.reward\n",
    "            else:\n",
    "                y = quadruple.reward + np.nanmax(q_hat[idx,:])\n",
    "            targets[idx, quadruple.action] = y\n",
    "            targets_mask[idx, quadruple.action] = 1\n",
    "        # Evaluate old states, apply mask and update weights\n",
    "        self.optimizer.zero_grad()\n",
    "        outputs = self.q_net(torch.stack(states, dim=0)) * targets_mask\n",
    "        loss = self.loss(outputs, targets)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def next_turn(self):\n",
    "        if self.gameboard.gameover:\n",
    "            self.episode += 1\n",
    "            self.epsilon = max(0.001, 1 - self.episode / self.epsilon_scale)\n",
    "            if self.episode % 100 == 0:\n",
    "                current_reward = np.sum(self.reward_tots[range(self.episode - 100, self.episode)])\n",
    "                print('episode ' + str(self.episode) + ' / ' + str(self.episode_count) + ' (reward: ', str(current_reward))\n",
    "                if current_reward > self.max_reward:\n",
    "                    self.max_reward = current_reward\n",
    "                    torch.save(self.q_net.state_dict(), 'q_net.pth')\n",
    "            if self.episode >= self.episode_count:\n",
    "                raise SystemExit(\"Training finished\")\n",
    "            else:\n",
    "                if (len(self.exp_buffer) >= self.replay_buffer_size) and (self.episode % self.sync_target_episode_count == 0):\n",
    "                    self.target_net.load_state_dict(self.q_net.state_dict())\n",
    "                self.gameboard.fn_restart()\n",
    "        else:\n",
    "            # Select and execute action (move the tile to the desired column and orientation)\n",
    "            self.set_action()\n",
    "            old_state = self.state\n",
    "            # Drop the tile on the game board\n",
    "            reward = self.gameboard.fn_drop()\n",
    "            self.reward_tots[self.episode] += reward\n",
    "\n",
    "            # Read the new state\n",
    "            self.get_state()\n",
    "            terminal = self.gameboard.gameover\n",
    "            quadruplet = Quadruplet(old_state, self.action, reward, self.state, terminal)\n",
    "            self.exp_buffer.append(quadruplet)\n",
    "            if len(self.exp_buffer) >= self.replay_buffer_size:\n",
    "                batch = random.sample(self.exp_buffer, self.batch_size)\n",
    "                self.training_step(batch)\n",
    "            if len(self.exp_buffer) >= self.replay_buffer_size + 1:\n",
    "                self.exp_buffer.pop(0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha=0.001\n",
    "episode_count=10000\n",
    "\n",
    "epsilon_scale=5000\n",
    "replay_buffer_size=10000\n",
    "batch_size=64\n",
    "sync_target_episode_count=100\n",
    "\n",
    "hidden_units=[128,128]\n",
    "\n",
    "q_net=dl.MultiLayerPerceptron(gameboard.N_row * gameboard.N_col + 2,hidden_units, gameboard.N_col*4)\n",
    "q_net.blocks[:-1].activation = torch.nn.ReLU()\n",
    "q_net=q_net.build()\n",
    "print(q_net)\n",
    "target_net=dl.MultiLayerPerceptron(gameboard.N_row * gameboard.N_col + 2,hidden_units,gameboard.N_col*4)\n",
    "target_net.blocks[:-1].activation = torch.nn.ReLU() #GELU stabilizes training, ReLU works ok as well\n",
    "target_net=target_net.build()\n",
    "\n",
    "agent=DQLAgent(q_net,target_net,alpha,epsilon,epsilon_scale,replay_buffer_size,batch_size,sync_target_episode_count,episode_count,loss=torch.nn.MSELoss(),optimizer=torch.optim.Adam(q_net.parameters(),lr=alpha))\n",
    "gameboard=gameboardClass.TGameBoard(N_row,N_col,tile_size,max_tile_count,agent,stochastic_prob)\n",
    "\n",
    "while True:\n",
    "    gameboard.agent.next_turn()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gameboard=gameboardClass.TGameBoard(N_row,N_col,tile_size,max_tile_count,player,stochastic_prob)\n",
    "plt.plot(agent.reward_tots)\n",
    "running_mean=np.zeros(len(agent.reward_tots))\n",
    "for i in range(len(agent.reward_tots)):\n",
    "    running_mean[i]=np.mean(agent.reward_tots[max(0,i-500):i])\n",
    "plt.plot(running_mean)\n",
    "plt.ylabel('Reward')\n",
    "plt.xlabel('Episode')\n",
    "Evaluate(gameboard,player,agent,strategy_file='q_net.pth')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DLCC",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
