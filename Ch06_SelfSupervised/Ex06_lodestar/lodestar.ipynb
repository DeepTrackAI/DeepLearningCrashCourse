{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-Supervised Learning and LodeSTAR\n",
    "\n",
    "We'll explore self-supervised learning and how LodeSTAR exploits symmetries to locate microscopic particles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Dataset\n",
    "\n",
    "We start by creating the dataset, including a particle ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deeptrack as dt \n",
    "from numpy.random import uniform\n",
    "\n",
    "image_size = 51\n",
    "\n",
    "particle = dt.PointParticle(\n",
    "    position=lambda:uniform(image_size / 2 - 5, image_size / 2 + 5, size=2),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and the imaging optics ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optics = dt.Fluorescence(output_region=(0, 0, image_size, image_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... which we combine in a simulation pipeline ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "simulation = (\n",
    "    optics(particle) \n",
    "    >> dt.NormalizeMinMax(0, 1)\n",
    "    >> dt.Gaussian(sigma=0.1)\n",
    "    >> dt.MoveAxis(-1, 0)\n",
    "    >> dt.pytorch.ToTensor(dtype=torch.float32)\n",
    ")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... which we have used to create a train and a test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dt.pytorch.Dataset(simulation, length=100)\n",
    "test_dataset = dt.pytorch.Dataset(simulation & particle.position, length=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we plot some examples of particles and the relative positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 2))\n",
    "for i in range(5):\n",
    "\timage, position = test_dataset[i]\n",
    "\tplt.subplot(1, 5, i + 1)\n",
    "\tplt.imshow(image[0], cmap=\"gray\", origin=\"lower\")\n",
    "\tplt.scatter(position[1], position[0], c=\"r\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learn from Translations\n",
    "\n",
    "We implement a cobvolutional neural network with a dense top, with two outputs corresponding to the coordinates of the particle position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deeplay as dl\n",
    "import torch.nn as nn\n",
    "\n",
    "backbbone = dl.ConvolutionalNeuralNetwork(in_channels=1,\n",
    "                                          hidden_channels=[16, 32, 64],\n",
    "                                          out_channels=128,\n",
    "                                          pool=nn.MaxPool2d(2))\n",
    "\n",
    "# We use LazyLinear not to have to calculate the output size of the CNN.\n",
    "model = dl.Sequential(backbbone,\n",
    "                      nn.Flatten(),\n",
    "                      nn.LazyLinear(2)).create()\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the transformation that we will apply and with which we will teach the neural network to be consistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kornia.geometry.transform import translate\n",
    "\n",
    "def image_translation(batch, translation):\n",
    "    \"\"\"Translate a batch of images.\"\"\"\n",
    "    # Flip the translation to match the image coordinate system.\n",
    "    xy_flipped_translation = translation[:, [1, 0]]\n",
    "    return translate(batch, xy_flipped_translation, padding_mode=\"reflection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also define the inverse transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_translation(predicted_position, applied_translation):\n",
    "    \"\"\"Invert transaltion of output positions.\"\"\"\n",
    "    return predicted_position - applied_translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParticleLocalizer(dl.Application):\n",
    "    \"\"\"LodeSTAR implementation with translations.\"\"\"\n",
    "    \n",
    "    def __init__(self, model, n_transforms=8, **kwargs):\n",
    "        \"\"\"Initialize the ParticleLocalizer.\"\"\"\n",
    "        self.model = model\n",
    "        self.n_transforms = n_transforms\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass through the model.\"\"\"\n",
    "        return self.model(x)\n",
    "    \n",
    "    def random_arguments(self):\n",
    "        \"\"\"Generate random arguments for transformations.\"\"\"\n",
    "        translation = (torch.rand(self.n_transforms, 2).float().to(self.device) \n",
    "                       * 5 - 2.5)\n",
    "        return {\"translation\": translation}\n",
    "\n",
    "    def forward_transform(self, x, translation):\n",
    "        \"\"\"Apply forward translation to the image.\"\"\"\n",
    "        return image_translation(x, translation)\n",
    "\n",
    "    def inverse_transform(self, x, translation):\n",
    "        \"\"\"Apply inverse translation to the image.\"\"\"\n",
    "        return inverse_translation(x, translation)\n",
    "\n",
    "    def training_step(self, image, batch_idx):\n",
    "        \"\"\"Perform a single training step.\"\"\"\n",
    "\n",
    "        # Create a batch of images by applying random translations.\n",
    "        image, *_ = image  # Take the first image from the input batch.\n",
    "        batch = image.repeat(self.n_transforms, 1, 1, 1)\n",
    "\n",
    "        # Get arguments for the random transforms.\n",
    "        kwargs = self.random_arguments()\n",
    "        transformed_batch = self.forward_transform(batch, **kwargs)\n",
    "\n",
    "        # Predict the position of the particle.\n",
    "        pred_position = self(transformed_batch)\n",
    "        # Invert the translation to get the predicted position in the original image.\n",
    "        pred_position = self.inverse_transform(pred_position, **kwargs)\n",
    "\n",
    "        # Get average predicted position.\n",
    "        average_pred_position = (pred_position\n",
    "                                 .mean(dim=0, keepdim=True)\n",
    "                                 .repeat(self.n_transforms, 1))  # Repeat the average position to match the batch size.\n",
    "\n",
    "        # Calculate the loss. \n",
    "        # We minimize the distance between each prediction and their average\n",
    "        # which effectively minimizes the variance of the predictions on the original image.\n",
    "        loss = self.loss(pred_position, average_pred_position)\n",
    "        self.log(\"loss\", loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then instantiate the `ParticleLocalizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "localizer = ParticleLocalizer(\n",
    "    model, n_transforms=8, loss=nn.L1Loss(), optimizer=dl.Adam(lr=5e-4)\n",
    ").create()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the dataloader and trainer, and finally we train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataloader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "trainer = dl.Trainer(max_epochs=100)\n",
    "trainer.fit(localizer, dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Performance\n",
    "\n",
    "We start by evaulating the positions of some particles in the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, positions = zip(*test_dataset)\n",
    "images = torch.stack(images)\n",
    "positions = torch.stack(positions)\n",
    "\n",
    "predictions = localizer(images).detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We write a function to plot the predicted positions as a function of the real ones, and add it to `fnc_lodestar.py`.\n",
    "\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_position_comparison(positions, predictions):\n",
    "    \"\"\"Plot comparison between predicted and real particle positions.\"\"\"\n",
    "\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    grid = plt.GridSpec(4, 7, wspace=.2, hspace=.1)\n",
    "\n",
    "    plt.subplot(grid[1:, :3])\n",
    "    plt.scatter(positions[:, 0], predictions[:, 0], alpha=.5) \n",
    "    plt.axline((25, 25), slope=1, color=\"black\")\n",
    "    plt.xlabel(\"True Horizontal Position\")\n",
    "    plt.ylabel(\"Predicted Horizontal Position\")\n",
    "    plt.axis(\"equal\")    \n",
    "\n",
    "    plt.subplot(grid[1:, 4:])\n",
    "    plt.scatter(positions[:, 1], predictions[:, 1], alpha=.5)\n",
    "    plt.axline((25, 25), slope=1, color=\"black\")\n",
    "    plt.xlabel(\"True Vertical Position\")\n",
    "    plt.ylabel(\"Predicted Vertical Position\")\n",
    "    plt.axis(\"equal\")\n",
    "\n",
    "    plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def plot_position_comparison(positions, predictions):\n",
    "    \"\"\"Plot comparison between predicted and real particle positions.\"\"\"\n",
    "\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    grid = plt.GridSpec(4, 7, wspace=.2, hspace=.1)\n",
    "\n",
    "    plt.subplot(grid[1:, :3])\n",
    "    plt.scatter(positions[:, 0], predictions[:, 0], alpha=.5) \n",
    "    plt.axline((25, 25), slope=1, color=\"black\")\n",
    "    plt.xlabel(\"True Horizontal Position\")\n",
    "    plt.ylabel(\"Predicted Horizontal Position\")\n",
    "    plt.axis(\"equal\")    \n",
    "\n",
    "    plt.subplot(grid[1:, 4:])\n",
    "    plt.scatter(positions[:, 1], predictions[:, 1], alpha=.5)\n",
    "    plt.axline((25, 25), slope=1, color=\"black\")\n",
    "    plt.xlabel(\"True Vertical Position\")\n",
    "    plt.ylabel(\"Predicted Vertical Position\")\n",
    "    plt.axis(\"equal\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fnc_lodestar import plot_position_comparison\n",
    "\n",
    "plot_position_comparison(positions, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reflected_images = images.flip(dims=(2, 3))\n",
    "\n",
    "direct_preds = localizer(images).detach().numpy()\n",
    "reflected_preds = localizer(reflected_images).detach().numpy()\n",
    "\n",
    "predictions_with_difference = ((direct_preds - reflected_preds) / 2 \n",
    "               + image_size / 2 - 0.5)\n",
    "\n",
    "plot_position_comparison(positions, predictions_with_difference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learn also from Reflections\n",
    "\n",
    "We now add also reflections to the learning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flip_transform(image, should_flip, dim):\n",
    "    \"\"\"Conditionally flip images along a specified dimension.\"\"\"\n",
    "    should_flip = should_flip.view(-1, 1, 1, 1)  # Reshape the should_flip tensor for broadcasting.\n",
    "    return torch.where(should_flip, image.flip(dims=(dim,)), image)  # Flip the images where should_flip is True.\n",
    "\n",
    "def inverse_flip_transform(x, should_flip, dim):\n",
    "    \"\"\"Conditionally inverse flip transformation based on the should_flip condition.\"\"\"\n",
    "    should_flip_mask = torch.zeros_like(x).bool()\n",
    "    should_flip_mask[should_flip, dim] = 1\n",
    "    return torch.where(should_flip_mask, -x, x)   # Apply the inverse flip where should_flip_mask is True\n",
    "\n",
    "\n",
    "class ParticleLocalizerWithReflections(ParticleLocalizer):\n",
    "    \"\"\"ParticleLocalizer with additional reflection (flip) transformations.\"\"\"\n",
    "    \n",
    "    def forward_transform(self, batch, translation, should_flip_x, should_flip_y):\n",
    "        \"\"\"Apply forward translations and reflections to the batch.\"\"\"\n",
    "        x = image_translation(batch, translation)\n",
    "        x = flip_transform(x, should_flip_x, dim=3)\n",
    "        x = flip_transform(x, should_flip_y, dim=2)\n",
    "        return x\n",
    "    \n",
    "    def inverse_transform(self, x, translation, should_flip_x, should_flip_y):\n",
    "        \"\"\"Apply the inverse transformations to the predictions.\"\"\"\n",
    "        x = inverse_flip_transform(x, should_flip_y, dim=0)\n",
    "        x = inverse_flip_transform(x, should_flip_x, dim=1)\n",
    "        x = inverse_translation(x, translation)\n",
    "        return x\n",
    "    \n",
    "    def random_arguments(self):\n",
    "        \"\"\"Generate random arguments for translation and flips.\"\"\"\n",
    "        return {\n",
    "            \"translation\": torch.rand(self.n_transforms, 2).float().to(self.device) * 5 - 2.5,\n",
    "            \"should_flip_x\": torch.rand(self.n_transforms).float().to(self.device) > .5,\n",
    "            \"should_flip_y\": torch.rand(self.n_transforms).float().to(self.device) > .5,\n",
    "        }"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "backbbone = dl.ConvolutionalNeuralNetwork(in_channels=1,\n",
    "                                          hidden_channels=[16, 32, 64],\n",
    "                                          out_channels=128,\n",
    "                                          pool=nn.MaxPool2d(2))\n",
    "model = dl.Sequential(backbbone,\n",
    "                      nn.Flatten(),\n",
    "                      nn.LazyLinear(2)).create()\n",
    "\n",
    "localizer = ParticleLocalizerWithReflection(model, \n",
    "                                            n_transforms=8, \n",
    "                                            loss=nn.L1Loss(),\n",
    "                                            optimizer=dl.Adam(lr=1e-3)).create()\n",
    "\n",
    "trainer = dl.Trainer(max_epochs=100)\n",
    "trainer.fit(localizer, dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "localizer_with_reflections = ParticleLocalizerWithReflections(\n",
    "    model, n_transforms=8, loss=nn.L1Loss(), optimizer=dl.Adam(lr=1e-3)\n",
    ").create()\n",
    "\n",
    "trainer = dl.Trainer(max_epochs=100)\n",
    "trainer.fit(localizer, dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = localizer(images).detach().numpy() + image_size / 2 - 0.5\n",
    "\n",
    "plot_position_comparison(positions, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improve Performance with LodeSTAR\n",
    "\n",
    "We can improve the performance using the `LodeSTART` model from `deeplay`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "lodestar = dl.LodeSTAR(optimizer=dl.Adam(lr=1e-4)).build()\n",
    "\n",
    "trainer = dl.Trainer(max_epochs=100)\n",
    "trainer.fit(lodestar, dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lodestar_predictions = lodestar.pooled(images).detach().numpy() \n",
    "\n",
    "plot_position_comparison(positions, lodestar_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
