{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell Detection and Localization\n",
    "\n",
    "We'll demonstrate how self-supervised learning can be used to detect and localize cells in an image.\n",
    "We'll use LodeSTAR that permits us to train the neural network using a single crop of a cell without the need for ground truth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the Dataset\n",
    "\n",
    "We will use the BF-C2DL-HSC dataset from the Cell Tracking Challenge.\n",
    "This is a series of videos of proliferating mouse hematopoietic stem cells. \n",
    "Importantly, we can use the annotations provided for the challenge to evaluate the detection performance.\n",
    "This dataset is available at http://data.celltrackingchallenge.net/training-datasets/BF-C2DL-HSC.zip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torchvision.datasets.utils import download_url, _extract_zip\n",
    "\n",
    "dataset_path = os.path.join(\".\", \"cell_detection_dataset\")\n",
    "if not os.path.exists(dataset_path):\n",
    "    url = (\"http://data.celltrackingchallenge.net/training-datasets/\"\n",
    "           \"BF-C2DL-HSC.zip\")\n",
    "    download_url(url, \".\")\n",
    "    _extract_zip(\"BF-C2DL-HSC.zip\", dataset_path, None)\n",
    "    os.remove(\"BF-C2DL-HSC.zip\")\n",
    "\n",
    "dir = os.path.join(dataset_path, \"BF-C2DL-HSC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deeptrack as dt\n",
    "import glob\n",
    "from skimage.measure import regionprops\n",
    "\n",
    "dt.config.disable_image_wrapper()\n",
    "\n",
    "sources = dt.sources.Source(\n",
    "    image_path=sorted(glob.glob(os.path.join(dir, \"02\", \"*.tif\"))),\n",
    "    label_path=sorted(glob.glob(os.path.join(dir, \"02_GT\", \"TRA\", \"*.tif\"))),\n",
    ")\n",
    "\n",
    "image = dt.LoadImage(sources.image_path)[:, :300] / 256\n",
    "label = dt.LoadImage(sources.label_path)[:, :300] >> regionprops\n",
    "\n",
    "pipeline = image & label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import skimage.io\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "for plt_index, data_index in enumerate([0, 300, 600, 900, 1200, 1500]):\n",
    "    image, *props = pipeline(sources[data_index])\n",
    "    \n",
    "    plt.subplot(1, 6, plt_index + 1)\n",
    "    plt.imshow(image, cmap=\"gray\")\n",
    "    for prop in props:\n",
    "        plt.scatter(prop.centroid[1], prop.centroid[0], s=5, color=\"red\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.patches as patches\n",
    "\n",
    "crop_frame_index = 282\n",
    "crop_size = 50\n",
    "crop_x0 = 595 - crop_size // 2\n",
    "crop_y0 = 115 - crop_size // 2\n",
    "\n",
    "image, *props = pipeline(sources[crop_frame_index])\n",
    "crop = image[crop_x0 : crop_x0 + crop_size, crop_y0 : crop_y0 + crop_size]\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(2.5, 10))\n",
    "plt.imshow(image, cmap=\"gray\")\n",
    "plt.gca().add_patch(\n",
    "    patches.Rectangle(\n",
    "        (crop_y0, crop_x0),\n",
    "        crop_size,\n",
    "        crop_size,\n",
    "        linewidth=1,\n",
    "        edgecolor=\"r\",\n",
    "        facecolor=\"none\",\n",
    "    )\n",
    ")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "fig.add_subplot(2, 2, 2)\n",
    "plt.imshow(crop, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "training_pipeline = (\n",
    "    dt.Value(crop)\n",
    "    >> dt.Multiply(lambda: np.random.uniform(0.9, 1.1))\n",
    "    >> dt.Add(lambda: np.random.uniform(-.1, .1))\n",
    "    >> dt.MoveAxis(-1, 0)\n",
    "    >> dt.pytorch.ToTensor(dtype=torch.float32)\n",
    ")\n",
    "\n",
    "training_dataset = dt.pytorch.Dataset(training_pipeline, length=400, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deeplay as dl \n",
    "dataloader = dl.DataLoader(training_dataset, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = dl.LodeSTAR(n_transforms=4, optimizer=dl.Adam(lr=1e-4)).build()\n",
    "\n",
    "trainer = dl.Trainer(max_epochs=20)\n",
    "trainer.fit(model, dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_index = 1500\n",
    "\n",
    "image, *props = pipeline(sources[image_index])\n",
    "\n",
    "torch_image = torch.from_numpy(image).permute(2, 0, 1).unsqueeze(0).float()\n",
    "\n",
    "prediction = model(torch_image)[0].detach().numpy()\n",
    "\n",
    "x_feature = prediction[0]\n",
    "y_feature = prediction[1]\n",
    "mass_feature = prediction[-1]\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(image, cmap=\"gray\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(mass_feature, cmap=\"gray\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(image, cmap=\"gray\")\n",
    "plt.scatter(\n",
    "\ty_feature.flatten(), \n",
    "\tx_feature.flatten(), \n",
    "\talpha=mass_feature.flatten() / mass_feature.max(),\n",
    "\ts=5,\n",
    ")\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "for plot_idx, frame_idx in enumerate([0, 300, 600, 900, 1200, 1500]):\n",
    "    image, *props = pipeline(sources[frame_idx])\n",
    "    torch_image = torch.from_numpy(image).permute(2, 0, 1).unsqueeze(0).float()\n",
    "    y = model(torch_image.to(model.device))\n",
    "    y_pred, weights = y[:, :-1], y[:, -1:]\n",
    "    detections = model.detect(torch_image,\n",
    "                              alpha=0.1,\n",
    "                              beta=0.9,\n",
    "                              mode=\"constant\",\n",
    "                              cutoff=.5)[0]\n",
    "\n",
    "    plt.subplot(1, 6, plot_idx + 1)\n",
    "    plt.imshow(image, cmap=\"gray\")\n",
    "    plt.scatter(\n",
    "        detections[:, 1],\n",
    "        detections[:, 0],\n",
    "        s=5,\n",
    "        color=\"red\",\n",
    "    )\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm \n",
    "import scipy\n",
    "\n",
    "distance_th = 10\n",
    "\n",
    "TP = 0\n",
    "FP = 0\n",
    "FN = 0\n",
    "\n",
    "for source in tqdm.tqdm(sources[::10]):\n",
    "    image, *props = pipeline(source)\n",
    "    torch_image = torch.from_numpy(image).permute(2, 0, 1).unsqueeze(0).float()\n",
    "    detections = model.detect(torch_image,\n",
    "                              alpha=0.1,\n",
    "                              beta=0.9,\n",
    "                              mode=\"constant\",\n",
    "                              cutoff=.5)[0]\n",
    "    \n",
    "    centroids = np.array([prop.centroid[:2] for prop in props])\n",
    "\n",
    "    distance_matrix = scipy.spatial.distance_matrix(detections, centroids)\n",
    "    row_idx, col_idx = scipy.optimize.linear_sum_assignment(distance_matrix)\n",
    "\n",
    "    filtered_row_ind = row_idx[distance_matrix[row_idx, col_idx] < distance_th]\n",
    "    filtered_col_ind = col_idx[distance_matrix[row_idx, col_idx] < distance_th]\n",
    "\n",
    "    TP += len(filtered_row_ind)\n",
    "    FP += len(detections) - len(filtered_row_ind)\n",
    "    FN += len(centroids) - len(filtered_col_ind)\n",
    "\n",
    "f1 = 2 * TP / (2 * TP + FP + FN)\n",
    "\n",
    "print(f\"\"\"\n",
    "TP: {TP}\n",
    "FP: {FP}\n",
    "FN: {FN}\n",
    "F1: {f1}  \n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dt_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
