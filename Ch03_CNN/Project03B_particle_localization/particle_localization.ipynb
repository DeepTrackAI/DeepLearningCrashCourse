{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Particle Localization\n",
    "\n",
    "We'll build a neural network to determine the position of particles within an image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "We'll use a dataset including two videos of optically trapped particles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "if not os.path.exists(\"particle_dataset\"):\n",
    "    os.system(\"git clone -b cm https://github.com/DeepTrackAI/particle_dataset\")\n",
    "\n",
    "train_path = os.path.join(\"particle_dataset\", \"particle_dataset\")\n",
    "train_video_path = glob.glob(os.path.join(train_path, \"*.avi\"))\n",
    "\n",
    "print(f\"{len(train_video_path)} training videos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll define a function to load the videos ...\n",
    "\n",
    "```python\n",
    "def load_video(path, frames_to_load=100, image_size=51):\n",
    "    video = cv2.VideoCapture(path)\n",
    "    data = []\n",
    "    for _ in range(frames_to_load):\n",
    "        (_, frame) = video.read()\n",
    "        frame = cv2.normalize(frame, None, 0, 255, cv2.NORM_MINMAX)\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY) / 255\n",
    "        frame = cv2.resize(frame, (image_size, image_size))\n",
    "        data.append(frame)\n",
    "    return np.array(data)\n",
    "```\n",
    "\n",
    "... and use it to read the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fnc_particle_loc import load_video\n",
    "\n",
    "low_noise_data = load_video(train_video_path[1])\n",
    "high_noise_data = load_video(train_video_path[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll plot the first 6 frames of each video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(2, 6, figsize=(25, 7))\n",
    "for i in range(6):\n",
    "    im = ax[0, i].imshow(low_noise_data[i], vmin=0, vmax=1, cmap=\"gray\")\n",
    "    ax[0, i].text(0, 5, \"Frame {}\".format(i), color=\"white\", fontsize=12)\n",
    "\n",
    "    ax[0, i].axis(\"off\")\n",
    "    ax[1, i].imshow(high_noise_data[i], vmin=0, vmax=1, cmap=\"gray\")\n",
    "    ax[1, i].text(0, 5, \"Frame {}\".format(i), color=\"white\", fontsize=12)\n",
    "    ax[1, i].axis(\"off\")\n",
    "plt.subplots_adjust(wspace=0.1, hspace=0.1)\n",
    "plt.colorbar(im, ax=ax.ravel().tolist())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data manual annotation\n",
    "\n",
    "We'll define a class to be able to interactively select the centroid of particles in some of the video frames.\n",
    "\n",
    "```python\n",
    "class ParticleCenter:\n",
    "\n",
    "    def __init__(self, images):\n",
    "        self.images = images\n",
    "        self.positions = []\n",
    "        self.i = 0\n",
    "        self.fig, self.ax = plt.subplots(1, 1, figsize=(5, 5))\n",
    "        self.fig.canvas.header_visible = False\n",
    "        self.fig.canvas.footer_visible = False\n",
    "\n",
    "    def start(self):\n",
    "        self.im = self.ax.imshow(self.images[self.i], cmap=\"gray\", vmin=0, vmax=1)\n",
    "        self.text = self.ax.text(\n",
    "            3,\n",
    "            5,\n",
    "            \"Frame \" + str(self.i + 1) + \" of \" + str(len(self.images)),\n",
    "            color=\"white\",\n",
    "            fontsize=12,\n",
    "        )\n",
    "        self.ax.axis(\"off\")\n",
    "        self.cursor = Cursor(self.ax, useblit=True, color=\"red\", linewidth=1)\n",
    "        self.cid = self.fig.canvas.mpl_connect(\"button_press_event\", self.onclick)\n",
    "        self.next_image()\n",
    "        plt.show()\n",
    "\n",
    "    def next_image(self):\n",
    "        im = self.images[self.i]\n",
    "        self.im.set_data(im)\n",
    "        self.text.set_text(\"Frame \" + str(self.i + 1) + \" of \" + str(len(self.images)))\n",
    "        self.fig.canvas.draw_idle()\n",
    "\n",
    "    def onclick(self, event):\n",
    "        self.positions.append([event.xdata, event.ydata])\n",
    "        if self.i < len(self.images) - 1:\n",
    "            self.i += 1\n",
    "            self.next_image()\n",
    "        else:\n",
    "            self.fig.canvas.mpl_disconnect(self.cid)\n",
    "            plt.close()\n",
    "            return\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll randomly select `number_of_images_to_annotate` frames a pinpoint the particle center."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from fnc_particle_loc import ParticleCenter\n",
    "from numpy.random import choice\n",
    "\n",
    "%matplotlib ipympl\n",
    "# Needed for using the interactive features of matplotlib in Notebooks\n",
    "\n",
    "number_of_images_to_annotate = 100\n",
    "\n",
    "dataset = np.concatenate([low_noise_data, high_noise_data], axis=0)\n",
    "np.random.shuffle(dataset)\n",
    "\n",
    "images_to_annotate = choice(\n",
    "  np.arange(dataset.shape[0]),\n",
    "  number_of_images_to_annotate,\n",
    "  replace=False\n",
    ")\n",
    "\n",
    "pc = ParticleCenter(dataset[images_to_annotate])\n",
    "pc.start()\n",
    "\n",
    "annotated_data = pc.images\n",
    "labels = pc.positions\n",
    "\n",
    "%matplotlib inline\n",
    "# Back to normal matplotlib backend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll save the annotated images and the corresponding centroid positions in two files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\n",
    "    os.path.join(train_path, \"annotated_data.npy\"),\n",
    "    np.array(annotated_data),\n",
    ")\n",
    "\n",
    "np.save(\n",
    "    os.path.join(train_path, \"annotated_data_labels.npy\"),\n",
    "    np.array(labels),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset preprocessing\n",
    "\n",
    "We'll use the annotated data to train and test a CNN. First, we define a `Dataset` class to provide the data in the needed format.\n",
    "\n",
    "```python\n",
    "class ParticleDataset(Dataset):\n",
    "    import torch\n",
    "    import numpy as np\n",
    "\n",
    "    def __init__(self, file, dir):\n",
    "        self.im = np.load(os.path.join(dir, file))\n",
    "        name, ext = os.path.splitext(file)\n",
    "        self.pos = np.load(os.path.join(dir, name + \"_labels\" + ext))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.im.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = torch.tensor(self.im[idx, np.newaxis, :, :]).float()\n",
    "        labels = torch.tensor(self.pos[idx] / self.im.shape[1] - 0.5).float()\n",
    "        sample = [img, labels]\n",
    "        return sample\n",
    "```\n",
    "The `ParticleDataset` class reads the annotated data, transform the images into torch tensors and rescales the centroid positions in [-1.1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from fnc_particle_loc import ParticleDataset\n",
    "\n",
    "annotated_dataset = ParticleDataset(\"annotated_data.npy\", train_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll split the annotated data and define two data loaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deeplay as dl\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "train_ann_dataset, test_ann_dataset = random_split(annotated_dataset, [0.8, 0.2])\n",
    "\n",
    "train_ann_dataloader = dl.DataLoader(train_ann_dataset, batch_size=1)\n",
    "test_ann_dataloader = dl.DataLoader(test_ann_dataset, batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network\n",
    "\n",
    "We define a CNN with a dense top to analyze the images using `deeplay`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn\n",
    "from fnc_particle_loc import Flatten\n",
    "\n",
    "CNN = dl.Sequential(\n",
    "    dl.ConvolutionalNeuralNetwork(\n",
    "        in_channels=1,\n",
    "        hidden_channels=[16, 32],\n",
    "        out_channels=64,\n",
    "        pool=torch.nn.MaxPool2d(\n",
    "            kernel_size=2,\n",
    "        ),\n",
    "        out_activation=torch.nn.ReLU,\n",
    "    ),\n",
    "    dl.Layer(torch.nn.MaxPool2d, kernel_size=2),\n",
    "    dl.Layer(Flatten),\n",
    "    dl.MultiLayerPerceptron(\n",
    "        in_features=6 * 6 * 64,\n",
    "        hidden_features=[32, 32],\n",
    "        out_features=2,\n",
    "        out_activation=torch.nn.Identity,\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then compile the CNN as a regressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchmetrics as tm\n",
    "\n",
    "regressor_template = dl.Regressor(\n",
    "    model=CNN,\n",
    "    loss=torch.nn.MSELoss(),\n",
    "    optimizer=dl.Adam(),\n",
    "    metrics=[tm.MeanAbsoluteError()],\n",
    ")\n",
    "\n",
    "CNN_regressor = regressor_template.create()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "\n",
    "We train the CNN regressor..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN_trainer = dl.Trainer(\n",
    "    max_epochs=50,  # How many times to run through the entire dataset\n",
    "    accelerator=\"auto\",  # Use GPU if available\n",
    ")\n",
    "\n",
    "CNN_trainer.fit(CNN_regressor, train_ann_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and evaluate its performance over the training set. We also calculate the mean absolute error in pixel units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = CNN_trainer.test(CNN_regressor, test_ann_dataloader)\n",
    "\n",
    "IMAGE_SIZE = test_ann_dataset[0][0].shape[1]\n",
    "print(\n",
    "    \"Mean pixel error: {:.3f} pixels\".format(\n",
    "        test_results[0][\"testMeanAbsoluteError_epoch\"] * IMAGE_SIZE\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OLD IMPLEMENTATION\n",
    "# def mean_pixel_error(y_true, y_pred):\n",
    "#     return np.mean(np.abs(np.array(y_true) - np.array(y_pred))) * IMAGE_SIZE\n",
    "\n",
    "\n",
    "# test_lab, test_pred = [], []\n",
    "# for td_im, td_lab in test_dataset:\n",
    "#     test_pred.append(CNN_regressor(td_im.unsqueeze(0))[0].detach())\n",
    "#     IMAGE_SIZE = test_dataset[idx][0].shape[1]\n",
    "#     test_lab.append(td_lab)\n",
    "\n",
    "\n",
    "# print(\"Mean pixel error: {:.3f} pixels\".format(mean_pixel_error(test_lab, test_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "We compare annotated and predicted particle positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "fig, axes = plt.subplots(2, 5, figsize=(10, 4))\n",
    "\n",
    "random_idx = np.random.choice(np.arange(len(test_ann_dataset)), 10, replace=False)\n",
    "\n",
    "for idx, ax in zip(random_idx, axes.flatten()):\n",
    "    test_pred = CNN_regressor(test_ann_dataset[idx][0].unsqueeze(0))[0].detach()\n",
    "\n",
    "    ax.imshow(test_ann_dataset[idx][0].detach().numpy().squeeze(), cmap=\"gray\")\n",
    "    ax.scatter(\n",
    "        test_pred[0] * IMAGE_SIZE + IMAGE_SIZE / 2,\n",
    "        test_pred[1] * IMAGE_SIZE + IMAGE_SIZE / 2,\n",
    "        marker=\"x\",\n",
    "        c=\"r\",\n",
    "        s=100,\n",
    "        label=\"prediction\",\n",
    "    )\n",
    "    ax.scatter(\n",
    "        test_ann_dataset[idx][1][0] * IMAGE_SIZE + IMAGE_SIZE / 2,\n",
    "        test_ann_dataset[idx][1][1] * IMAGE_SIZE + IMAGE_SIZE / 2,\n",
    "        marker=\"+\",\n",
    "        c=\"g\",\n",
    "        s=100,\n",
    "        label=\"annotation\",\n",
    "    )\n",
    "    ax.set_axis_off()\n",
    "ax.legend(loc=(0.2, 0.8), framealpha=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image simulations\n",
    "We will use the `deeptrack2.0` library to simulate particle images for which the ground truth is known. In these way we avoid the manual annotation and can have more data available for training.\n",
    "\n",
    "The particle is defined using the deeptrack object `MieSphere`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deeptrack.scatterers import MieSphere\n",
    "\n",
    "particle = MieSphere(\n",
    "    position=(25, 25),\n",
    "    z=0,\n",
    "    radius=500e-9,\n",
    "    refractive_index=1.37,\n",
    "    position_unit=\"pixel\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we specify the optical device to image the particle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deeptrack.optics import Brightfield\n",
    "\n",
    "brightfield_microscope = Brightfield(\n",
    "    wavelength=630e-9,\n",
    "    NA=0.8,\n",
    "    resolution=1e-6,\n",
    "    magnification=15,\n",
    "    refractive_index_medium=1.33,\n",
    "    output_region=(0, 0, IMAGE_SIZE, IMAGE_SIZE),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create the image of the particle and display it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imaged_particle = brightfield_microscope(particle)\n",
    "\n",
    "fig = plt.figure()\n",
    "output_image = imaged_particle.resolve()\n",
    "plt.imshow(np.squeeze(output_image), cmap=\"gray\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create particles with random positions arounf the center and display them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "particle = MieSphere(\n",
    "    position=lambda: np.random.uniform(IMAGE_SIZE / 2 - 5, IMAGE_SIZE / 2 + 5, 2),\n",
    "    z=lambda: np.random.uniform(-1, -1),\n",
    "    radius=lambda: np.random.uniform(500, 600) * 1e-9,\n",
    "    refractive_index=lambda: np.random.uniform(1.37, 1.42),\n",
    "    position_unit=\"pixel\",\n",
    ")\n",
    "imaged_particle = brightfield_microscope(particle)\n",
    "\n",
    "fig, axes = plt.subplots(2, 5, figsize=(10, 4))\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    output_image = imaged_particle.update().resolve()\n",
    "    ax.imshow(np.squeeze(output_image), cmap=\"gray\")\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    plt.subplots_adjust(wspace=0.05, hspace=0.05)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also add some Poisson noise to make the images more realistic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deeptrack import Poisson\n",
    "\n",
    "noise = Poisson(\n",
    "    min_snr=5,\n",
    "    max_snr=20,\n",
    "    snr=lambda min_snr, max_snr: min_snr + np.random.rand() * (max_snr - min_snr),\n",
    "    background=1,\n",
    ")\n",
    "\n",
    "noisy_imaged_particle = imaged_particle >> noise\n",
    "\n",
    "fig, axes = plt.subplots(2, 5, figsize=(10, 4))\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    output_image = noisy_imaged_particle.update().resolve()\n",
    "    ax.imshow(np.squeeze(output_image), cmap=\"gray\")\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    plt.subplots_adjust(wspace=0.05, hspace=0.05)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we create a pipeline that generates noisy particle images and normalize the pixel values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deeptrack import NormalizeMinMax\n",
    "\n",
    "normalization = NormalizeMinMax(\n",
    "    lambda: np.random.rand() * 0.2, lambda: 0.8 + np.random.rand() * 0.2\n",
    ")\n",
    "data_pipeline = noisy_imaged_particle >> normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also define a function to exctract the particle centroid position from the image\n",
    "```python\n",
    "def get_label(image):\n",
    "    from numpy import array\n",
    "    position = array(image.get_property(\"position\"))\n",
    "    return position\n",
    "```\n",
    "We display some images with the corresponding ground truth position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fnc_particle_loc import get_label\n",
    "\n",
    "fig, axes = plt.subplots(2, 5, figsize=(10, 4))\n",
    "\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    output_image = data_pipeline.update().resolve()\n",
    "    ax.imshow(np.squeeze(output_image), cmap=\"gray\")\n",
    "\n",
    "    particle_position = get_label(output_image)\n",
    "    ax.scatter(\n",
    "        particle_position[1],\n",
    "        particle_position[0],\n",
    "        s=60,\n",
    "        facecolors=\"none\",\n",
    "        edgecolor=\"g\",\n",
    "        linewidth=4,\n",
    "    )\n",
    "\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    plt.subplots_adjust(wspace=0.05, hspace=0.05)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to use the simulations for the CNN training. Thus, we modify the class `ParticleDataset` to now work with the simulation pipeline, creating the new class `ParticleDatasetSimul`\n",
    "```python\n",
    "class ParticleDatasetSimul(Dataset):\n",
    "    import torch\n",
    "    import numpy as np\n",
    "\n",
    "    def __init__(self, pipeline, data_size):\n",
    "        im = [pipeline.update().resolve() for _ in range(data_size)]\n",
    "        self.pos = np.array([get_label(image) for image in im])\n",
    "        self.im = np.array(im).squeeze()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.im.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = torch.tensor(self.im[idx, np.newaxis, :, :]).float()\n",
    "        labels = torch.tensor(self.pos[idx] / self.im.shape[1] - 0.5).float()\n",
    "        sample = [img, labels]\n",
    "        return sample\n",
    "```\n",
    "The class can be used with the data loader and passed to the CNN for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fnc_particle_loc import ParticleDatasetSimul\n",
    "\n",
    "train_dataloader = dl.DataLoader(\n",
    "    ParticleDatasetSimul(pipeline=data_pipeline, data_size=1000), batch_size=32\n",
    ")\n",
    "\n",
    "CNN_simul_regressor = regressor_template.create()\n",
    "\n",
    "CNN_simul_trainer = dl.Trainer(\n",
    "    max_epochs=50,\n",
    "    accelerator=\"auto\",\n",
    ")\n",
    "\n",
    "CNN_simul_trainer.fit(CNN_simul_regressor, train_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We check the network performance on a simulated test dataset..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = dl.DataLoader(\n",
    "    ParticleDatasetSimul(pipeline=data_pipeline, data_size=100), batch_size=32\n",
    ")\n",
    "\n",
    "test_simul_results = CNN_simul_trainer.test(CNN_simul_regressor, test_dataloader)\n",
    "\n",
    "print(\n",
    "    \"Mean pixel error: {:.3f} pixels\".format(\n",
    "        test_simul_results[0][\"testMeanAbsoluteError_epoch\"] * IMAGE_SIZE\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and plot the predictions versus the ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2)\n",
    "pred, gt = [], []\n",
    "for batch in iter(test_dataloader):\n",
    "    gt.append(batch[1])\n",
    "    pred.append(CNN_simul_regressor(batch[0]))\n",
    "gt = torch.cat(gt, dim=0).numpy()\n",
    "pred = torch.cat(pred, dim=0).detach().numpy()\n",
    "\n",
    "for i, l in enumerate([\"x\", \"y\"]):\n",
    "    label = gt[:][:, i]\n",
    "    prediction = pred[:][:, i]\n",
    "    axs[i].scatter(label, prediction, alpha=0.2)\n",
    "    axs[i].plot([np.min(label), np.max(label)], [np.min(label), np.max(label)], c=\"k\")\n",
    "    axs[i].set_xlabel(\"Prediction\")\n",
    "    axs[i].set_ylabel(\"Ground truth\")\n",
    "    axs[i].set_xlim([-0.065, 0.065])\n",
    "    axs[i].set_ylim([-0.065, 0.065])\n",
    "    axs[i].set_aspect(\"equal\")\n",
    "    axs[i].set_title(\"{i}-coordinates\".format(i=l))\n",
    "for ax in fig.get_axes():\n",
    "    ax.label_outer()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check now the CNN predictions on the annotated test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ann_results = CNN_simul_trainer.test(CNN_simul_regressor, test_ann_dataloader)\n",
    "\n",
    "print(\n",
    "    \"Mean pixel error: {:.3f} pixels\".format(\n",
    "        test_ann_results[0][\"testMeanAbsoluteError_epoch\"] * IMAGE_SIZE\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last, we plot the predictions in comparison to the annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 5, figsize=(10, 4))\n",
    "\n",
    "for idx, ax in zip(random_idx, axes.flatten()):\n",
    "    test_pred = CNN_simul_regressor(test_ann_dataset[idx][0].unsqueeze(0))[0].detach()\n",
    "\n",
    "    ax.imshow(test_ann_dataset[idx][0].detach().numpy().squeeze(), cmap=\"gray\")\n",
    "    ax.scatter(\n",
    "        test_pred[0] * IMAGE_SIZE + IMAGE_SIZE / 2,\n",
    "        test_pred[1] * IMAGE_SIZE + IMAGE_SIZE / 2,\n",
    "        marker=\"x\",\n",
    "        c=\"r\",\n",
    "        s=100,\n",
    "        label=\"prediction\",\n",
    "    )\n",
    "    ax.scatter(\n",
    "        test_ann_dataset[idx][1][0] * IMAGE_SIZE + IMAGE_SIZE / 2,\n",
    "        test_ann_dataset[idx][1][1] * IMAGE_SIZE + IMAGE_SIZE / 2,\n",
    "        marker=\"+\",\n",
    "        c=\"g\",\n",
    "        s=100,\n",
    "        label=\"annotation\",\n",
    "    )\n",
    "    ax.set_axis_off()\n",
    "ax.legend(loc=(0.2, 0.8), framealpha=1)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dt_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
