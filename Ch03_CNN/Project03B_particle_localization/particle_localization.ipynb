{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Particle Localization\n",
    "\n",
    "We build a neural network to determine the position of microscopic particles within an image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "We use a dataset including two videos of optically trapped particles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.path.exists(\"particle_dataset\"):\n",
    "    os.system(\"git clone https://github.com/DeepTrackAI/particle_dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the `load_video` function to load the videos, save it in `fnc_particle_localization.py` ...\n",
    "\n",
    "```python\n",
    "def load_video(path, frames_to_load=100, image_size=51):\n",
    "    video = cv2.VideoCapture(path)\n",
    "    data = []\n",
    "    for _ in range(frames_to_load):\n",
    "        (_, frame) = video.read()\n",
    "        frame = cv2.normalize(frame, None, 0, 255, cv2.NORM_MINMAX)\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY) / 255\n",
    "        frame = cv2.resize(frame, (image_size, image_size))\n",
    "        data.append(frame)\n",
    "    return np.array(data)\n",
    "```\n",
    "\n",
    "... and use it to read the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_video(path, frames_to_load, image_size):\n",
    "    import cv2\n",
    "    import numpy as np\n",
    "\n",
    "    video = cv2.VideoCapture(path)\n",
    "\n",
    "    data = []\n",
    "    for _ in range(frames_to_load):\n",
    "        (_, frame) = video.read()\n",
    "        frame = cv2.normalize(frame, None, 0, 255, cv2.NORM_MINMAX)\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY) / 255\n",
    "        frame = cv2.resize(frame, (image_size, image_size))\n",
    "        data.append(frame)\n",
    "\n",
    "    return np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from fnc_particle_localization import load_video\n",
    "\n",
    "image_size = 51\n",
    "video_low_noise = load_video(os.path.join(\"particle_dataset\", \"high_noise.avi\"),\n",
    "                             frames_to_load=100, image_size=image_size)\n",
    "video_high_noise = load_video(os.path.join(\"particle_dataset\", \"low_noise.avi\"),\n",
    "                              frames_to_load=100, image_size=image_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot the first 5 frames of each video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axs = plt.subplots(2, 4, figsize=(24, 13))\n",
    "for i in range(4):\n",
    "    axs[0, i].imshow(video_low_noise[i], cmap=\"gray\", vmin=0, vmax=1)\n",
    "    axs[0, i].text(0, 5, f\"Frame {i}\", color=\"white\", fontsize=24)\n",
    "    axs[0, i].axis(\"off\")\n",
    "    \n",
    "    axs[1, i].imshow(video_high_noise[i], cmap=\"gray\", vmin=0, vmax=1)\n",
    "    axs[1, i].text(0, 5, f\"Frame {i}\", color=\"white\", fontsize=24)\n",
    "    axs[1, i].axis(\"off\")\n",
    "\n",
    "plt.subplots_adjust(wspace=0.1, hspace=0.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annotate Data Manually\n",
    "\n",
    "We define a class to be able to interactively select the centroid of particles in some of the video frames.\n",
    "\n",
    "```python\n",
    "class ParticleCenter:\n",
    "\n",
    "    def __init__(self, images):\n",
    "        self.images = images\n",
    "        self.positions = []\n",
    "        self.i = 0\n",
    "        self.fig, self.ax = plt.subplots(1, 1, figsize=(5, 5))\n",
    "        self.fig.canvas.header_visible = False\n",
    "        self.fig.canvas.footer_visible = False\n",
    "\n",
    "    def start(self):\n",
    "        self.im = self.ax.imshow(self.images[self.i], cmap=\"gray\", vmin=0, vmax=1)\n",
    "        self.text = self.ax.text(\n",
    "            3,\n",
    "            5,\n",
    "            \"Frame \" + str(self.i + 1) + \" of \" + str(len(self.images)),\n",
    "            color=\"white\",\n",
    "            fontsize=12,\n",
    "        )\n",
    "        self.ax.axis(\"off\")\n",
    "        self.cursor = Cursor(self.ax, useblit=True, color=\"red\", linewidth=1)\n",
    "        self.cid = self.fig.canvas.mpl_connect(\"button_press_event\", self.onclick)\n",
    "        self.next_image()\n",
    "        plt.show()\n",
    "\n",
    "    def next_image(self):\n",
    "        im = self.images[self.i]\n",
    "        self.im.set_data(im)\n",
    "        self.text.set_text(\"Frame \" + str(self.i + 1) + \" of \" + str(len(self.images)))\n",
    "        self.fig.canvas.draw_idle()\n",
    "\n",
    "    def onclick(self, event):\n",
    "        self.positions.append([event.xdata, event.ydata])\n",
    "        if self.i < len(self.images) - 1:\n",
    "            self.i += 1\n",
    "            self.next_image()\n",
    "        else:\n",
    "            self.fig.canvas.mpl_disconnect(self.cid)\n",
    "            plt.close()\n",
    "            return\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.widgets import Cursor\n",
    "\n",
    "class ManualAnnotation:\n",
    "    def __init__(self, images):\n",
    "        self.images = images\n",
    "        self.positions = []\n",
    "        self.i = 0\n",
    "        self.fig, self.ax = plt.subplots(1, 1, figsize=(5, 5))\n",
    "        self.fig.canvas.header_visible = False\n",
    "        self.fig.canvas.footer_visible = False\n",
    "\n",
    "    def start(self):\n",
    "        self.im = self.ax.imshow(self.images[self.i], cmap=\"gray\", vmin=0, vmax=1)\n",
    "        self.text = self.ax.text(\n",
    "            3, \n",
    "            5, \n",
    "            f\"Frame {self.i + 1} of {len(self.images)}\", \n",
    "            color=\"white\", \n",
    "            fontsize=12,\n",
    "        )\n",
    "        self.ax.axis(\"off\")\n",
    "        self.cursor = Cursor(self.ax, useblit=True, color=\"red\", linewidth=1)\n",
    "        self.cid = self.fig.canvas.mpl_connect(\"button_press_event\", self.onclick)\n",
    "        self.next_image()\n",
    "        plt.show()\n",
    "\n",
    "    def next_image(self):\n",
    "        self.im.set_data(self.images[self.i])\n",
    "        self.text.set_text(f\"Frame {self.i + 1} of {len(self.images)}\")\n",
    "        self.fig.canvas.draw_idle()\n",
    "\n",
    "    def onclick(self, event):\n",
    "        self.positions.append([event.xdata, event.ydata])\n",
    "        if self.i < len(self.images) - 1:\n",
    "            self.i += 1\n",
    "            self.next_image()\n",
    "        else:\n",
    "            self.fig.canvas.mpl_disconnect(self.cid)\n",
    "            plt.close()\n",
    "            return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We randomly select `number_of_images_to_annotate` frames and ask you to manually pinpoint the particle center."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib ipympl \n",
    "# Needed for using the interactive features of matplotlib in Notebooks\n",
    "\n",
    "import numpy as np\n",
    "#from fnc_particle_localization import ManualAnnotation\n",
    "\n",
    "number_of_images_to_annotate = 100\n",
    "\n",
    "dataset = np.concatenate([video_low_noise, video_high_noise], axis=0)\n",
    "np.random.shuffle(dataset)\n",
    "\n",
    "images_to_annotate = np.random.choice(\n",
    "  np.arange(dataset.shape[0]),\n",
    "  number_of_images_to_annotate,\n",
    "  replace=False,\n",
    ")\n",
    "\n",
    "manual_annotation = ManualAnnotation(dataset[images_to_annotate])\n",
    "\n",
    "manual_annotation.start()\n",
    "\n",
    "annotated_images = manual_annotation.images\n",
    "manual_positions = manual_annotation.positions\n",
    "\n",
    "# Back to normal matplotlib backend\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We save the annotated images and the corresponding centroid positions in two files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_images = os.path.join(\"particle_dataset\", \"annotated_images.npy\")\n",
    "#np.save(file_images, np.array(annotated_images))\n",
    "\n",
    "file_positions = os.path.join(\"particle_dataset\", \"manual_positions.npy\")\n",
    "#np.save(file_positions, np.array(manual_positions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Dataset\n",
    "\n",
    "We use the annotated data to train and test a CNN. First, we define a `Dataset` class to provide the data in the needed format.\n",
    "\n",
    "```python\n",
    "class ParticleDataset(Dataset):\n",
    "    import torch\n",
    "    import numpy as np\n",
    "\n",
    "    def __init__(self, file, dir):\n",
    "        self.im = np.load(os.path.join(dir, file))\n",
    "        name, ext = os.path.splitext(file)\n",
    "        self.pos = np.load(os.path.join(dir, name + \"_labels\" + ext))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.im.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = torch.tensor(self.im[idx, np.newaxis, :, :]).float()\n",
    "        labels = torch.tensor(self.pos[idx] / self.im.shape[1] - 0.5).float()\n",
    "        sample = [img, labels]\n",
    "        return sample\n",
    "```\n",
    "The `ParticleDataset` class reads the annotated data, transform the images into torch tensors and rescales the centroid positions in [-1.1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class ParticleDataset(Dataset):\n",
    "    def __init__(self, file_images, file_positions):\n",
    "        self.images = np.load(file_images)\n",
    "        self.positions = np.load(file_positions)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.images.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        im = torch.tensor(self.images[idx, np.newaxis, :, :]).float()\n",
    "        pos = torch.tensor(self.positions[idx] / im.shape[-1] - .5).float()\n",
    "        sample = [im, pos]\n",
    "        return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from fnc_particle_localization import ParticleDataset\n",
    "\n",
    "ann_dataset = ParticleDataset(file_images, file_positions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split the annotated data and define two data loaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deeplay as dl\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "train_ann_dataset, test_ann_dataset = random_split(ann_dataset, [0.8, 0.2])\n",
    "\n",
    "train_ann_dataloader = dl.DataLoader(train_ann_dataset, batch_size=1)\n",
    "test_ann_dataloader = dl.DataLoader(test_ann_dataset, batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network\n",
    "\n",
    "We define a CNN with a dense top to analyze the images using `deeplay`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "#from fnc_particle_localization import Flatten\n",
    "\n",
    "cnn = dl.Sequential(\n",
    "    dl.ConvolutionalNeuralNetwork(\n",
    "        in_channels=1,\n",
    "        hidden_channels=[16, 32],\n",
    "        out_channels=64,\n",
    "        pool=nn.MaxPool2d(kernel_size=2),\n",
    "        out_activation=nn.ReLU,\n",
    "    ),\n",
    "    dl.Layer(nn.MaxPool2d, kernel_size=2),\n",
    "    dl.Layer(nn.Flatten),\n",
    "    dl.MultiLayerPerceptron(\n",
    "        in_features=6 * 6 * 64,\n",
    "        hidden_features=[32, 32],\n",
    "        out_features=2,\n",
    "        out_activation=nn.Identity,\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(cnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then compile the CNN as a regressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchmetrics as tm\n",
    "\n",
    "regressor_template = dl.Regressor(\n",
    "    model=cnn,\n",
    "    loss=nn.MSELoss(),\n",
    "    optimizer=dl.Adam(),\n",
    "    metrics=[tm.MeanAbsoluteError()],\n",
    ")\n",
    "\n",
    "cnn_ann_regressor = regressor_template.create()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "\n",
    "We train the CNN regressor..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_ann_trainer = dl.Trainer(max_epochs=50, accelerator=\"auto\")\n",
    "cnn_ann_trainer.fit(cnn_ann_regressor, train_ann_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and evaluate its performance over the training set. We also calculate the mean absolute error in pixel units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ann_results = cnn_ann_trainer.test(cnn_ann_regressor, test_ann_dataloader)\n",
    "\n",
    "MAE_ann = test_ann_results[0][\"testMeanAbsoluteError_epoch\"] * image_size\n",
    "print(f\"Mean pixel error (MAE): {MAE_ann:.3f} pixels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Predictions\n",
    "\n",
    "We compare annotated and predicted particle positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.random.choice(np.arange(len(test_ann_dataset)), 4, replace=False)\n",
    "images = [test_ann_dataset[index][0] for index in indices]\n",
    "annotations = [test_ann_dataset[index][1] for index in indices]\n",
    "predictions = cnn_ann_regressor(torch.stack(images))\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(25, 12))\n",
    "for ax, im, ann, pred in zip(axes, images, annotations, predictions):\n",
    "    ax.imshow(im.numpy().squeeze(), cmap=\"gray\")\n",
    "    \n",
    "    ann = ann * image_size + image_size / 2\n",
    "    ax.scatter(ann[0], ann[1], marker=\"+\", c=\"g\", s=500, label=\"Annotation\")\n",
    "    \n",
    "    pred = pred.detach().numpy() * image_size + image_size / 2\n",
    "    ax.scatter(pred[0], pred[1], marker=\"x\", c=\"r\", s=500, label=\"Prediction\")\n",
    "\n",
    "    ax.set_axis_off()\n",
    "\n",
    "ax.legend(loc=(0.5, 0.8), framealpha=1, fontsize='x-large')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image simulations\n",
    "\n",
    "We will use the `deeptrack` library to simulate particle images for which the ground truth is known. In these way, we avoid the manual annotation and can have more data available for training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The particle is defined using the deeptrack object `MieSphere`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deeptrack.scatterers import MieSphere\n",
    "\n",
    "particle = MieSphere(\n",
    "    position=(25, 25),\n",
    "    z=0,\n",
    "    radius=500e-9,\n",
    "    refractive_index=1.37,\n",
    "    position_unit=\"pixel\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we specify the optical device to image the particle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deeptrack.optics import Brightfield\n",
    "\n",
    "brightfield_microscope = Brightfield(\n",
    "    wavelength=630e-9,\n",
    "    NA=0.8,\n",
    "    resolution=1e-6,\n",
    "    magnification=15,\n",
    "    refractive_index_medium=1.33,\n",
    "    output_region=(0, 0, image_size, image_size),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create the image of the particle and display it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imaged_particle = brightfield_microscope(particle)\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.imshow(np.squeeze(imaged_particle.resolve()), cmap=\"gray\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create particles with random positions around the center and display them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "particle = MieSphere(\n",
    "    position=lambda: np.random.uniform(image_size / 2 - 5, image_size / 2 + 5, 2),\n",
    "    z=lambda: np.random.uniform(-1, -1),\n",
    "    radius=lambda: np.random.uniform(500, 600) * 1e-9,\n",
    "    refractive_index=lambda: np.random.uniform(1.37, 1.42),\n",
    "    position_unit=\"pixel\",\n",
    ")\n",
    "imaged_particle = brightfield_microscope(particle)\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(25, 12))\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    output_image = imaged_particle.update().resolve()\n",
    "    ax.imshow(np.squeeze(output_image), cmap=\"gray\")\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also add some Poisson noise to make the images more realistic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deeptrack import Poisson\n",
    "\n",
    "noise = Poisson(\n",
    "    min_snr=5,\n",
    "    max_snr=20,\n",
    "    snr=lambda min_snr, max_snr: min_snr + np.random.rand() * (max_snr - min_snr),\n",
    "    background=1,\n",
    ")\n",
    "\n",
    "noisy_imaged_particle = imaged_particle >> noise\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(25, 12))\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    output_image = noisy_imaged_particle.update().resolve()\n",
    "    ax.imshow(np.squeeze(output_image), cmap=\"gray\")\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we create a pipeline that generates noisy particle images and normalize the pixel values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deeptrack import NormalizeMinMax\n",
    "\n",
    "normalization = NormalizeMinMax(\n",
    "    lambda: np.random.rand() * 0.2, lambda: 0.8 + np.random.rand() * 0.2\n",
    ")\n",
    "data_pipeline = noisy_imaged_particle >> normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also define a function to exctract the particle centroid position from the image\n",
    "```python\n",
    "def get_label(image):\n",
    "    from numpy import array\n",
    "    position = array(image.get_property(\"position\"))\n",
    "    return position\n",
    "```\n",
    "We display some images with the corresponding ground truth position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label(image):\n",
    "    from numpy import array\n",
    "\n",
    "    position = array(image.get_property(\"position\"))\n",
    "    \n",
    "    return position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from fnc_particle_localization import get_label\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(25, 12))\n",
    "\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    output_image = data_pipeline.update().resolve()\n",
    "    ax.imshow(np.squeeze(output_image), cmap=\"gray\")\n",
    "\n",
    "    particle_position = get_label(output_image)\n",
    "    ax.scatter(\n",
    "        particle_position[1],\n",
    "        particle_position[0],\n",
    "        s=60,\n",
    "        facecolors=\"none\",\n",
    "        edgecolor=\"g\",\n",
    "        linewidth=4,\n",
    "    )\n",
    "\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to use the simulations for the CNN training. Thus, we modify the class `ParticleDataset` to now work with the simulation pipeline, creating the new class `ParticleDatasetSimul`\n",
    "```python\n",
    "class ParticleDatasetSimul(Dataset):\n",
    "    import torch\n",
    "    import numpy as np\n",
    "\n",
    "    def __init__(self, pipeline, data_size):\n",
    "        im = [pipeline.update().resolve() for _ in range(data_size)]\n",
    "        self.pos = np.array([get_label(image) for image in im])\n",
    "        self.im = np.array(im).squeeze()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.im.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = torch.tensor(self.im[idx, np.newaxis, :, :]).float()\n",
    "        labels = torch.tensor(self.pos[idx] / self.im.shape[1] - 0.5).float()\n",
    "        sample = [img, labels]\n",
    "        return sample\n",
    "```\n",
    "The class can be used with the data loader and passed to the CNN for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimulatedDataset(Dataset):\n",
    "    def __init__(self, pipeline, data_size):\n",
    "        im = [pipeline.update().resolve() for _ in range(data_size)]\n",
    "        self.pos = np.array([get_label(image) for image in im])[:, [1, 0]]\n",
    "        self.im = np.array(im).squeeze()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.im.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = torch.tensor(self.im[idx, np.newaxis, :, :]).float()\n",
    "        labels = torch.tensor(self.pos[idx] / img.shape[-1] - 0.5).float()\n",
    "        sample = [img, labels]\n",
    "        return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from fnc_particle_localization import SimulatedDataset\n",
    "\n",
    "train_sim_dataloader = dl.DataLoader(\n",
    "    SimulatedDataset(pipeline=data_pipeline, data_size=10000), batch_size=32\n",
    ")\n",
    "\n",
    "cnn_sim_regressor = regressor_template.create()\n",
    "cnn_sim_trainer = dl.Trainer(max_epochs=50, accelerator=\"auto\")\n",
    "cnn_sim_trainer.fit(cnn_sim_regressor, train_sim_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We check the network performance on a simulated test dataset..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sim_dataloader = dl.DataLoader(\n",
    "    SimulatedDataset(pipeline=data_pipeline, data_size=100), batch_size=32\n",
    ")\n",
    "\n",
    "test_sim_results = cnn_sim_trainer.test(cnn_sim_regressor, test_sim_dataloader)\n",
    "\n",
    "MAE_sim = test_sim_results[0][\"testMeanAbsoluteError_epoch\"] * image_size\n",
    "print(f\"Mean pixel error (MAE): {MAE_sim:.3f} pixels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and plot the predictions versus the ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred, gt = [], []\n",
    "for batch in iter(test_sim_dataloader):\n",
    "    pred.append(cnn_sim_regressor(batch[0]))\n",
    "    gt.append(batch[1])\n",
    "pred = torch.cat(pred, dim=0).detach().numpy()\n",
    "gt = torch.cat(gt, dim=0).numpy()\n",
    "\n",
    "fig, axs = plt.subplots(1, 2)\n",
    "for i, ax, coordinate in zip([0, 1], axs, [\"x\", \"y\"]):\n",
    "    label = gt[:][:, i]\n",
    "    prediction = pred[:][:, i]\n",
    "    ax.scatter(label, prediction, alpha=0.2)\n",
    "    ax.plot([np.min(label), np.max(label)], [np.min(label), np.max(label)], c=\"k\")\n",
    "    ax.set_title(f\"{coordinate}-coordinates\")\n",
    "    ax.set_xlabel(\"Prediction\"), ax.set_ylabel(\"Ground truth\")\n",
    "    ax.set_aspect(\"equal\"), ax.set_xlim([-.07, .07]), ax.set_ylim([-.07, .07])\n",
    "    ax.label_outer()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check now the CNN predictions on the annotated test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ann_results_with_cnn_sim = cnn_sim_trainer.test(cnn_sim_regressor, \n",
    "                                                     test_ann_dataloader)\n",
    "\n",
    "MAE_ann_with_cnn_sim = (test_ann_results_with_cnn_sim[0]\n",
    "                        [\"testMeanAbsoluteError_epoch\"] * image_size)\n",
    "print(f\"Mean pixel error (MAE): {MAE_ann_with_cnn_sim:.3f} pixels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last, we plot the predictions in comparison to the annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = [test_ann_dataset[index][0] for index in indices]\n",
    "annotations = [test_ann_dataset[index][1] for index in indices]\n",
    "predictions = cnn_sim_regressor(torch.stack(images))\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(25, 12))\n",
    "for ax, im, ann, pred in zip(axes, images, annotations, predictions):\n",
    "    ax.imshow(im.numpy().squeeze(), cmap=\"gray\")\n",
    "    \n",
    "    ann = ann * image_size + image_size / 2\n",
    "    ax.scatter(ann[0], ann[1], marker=\"+\", c=\"g\", s=500, label=\"Annotation\")\n",
    "    \n",
    "    pred = pred.detach().numpy() * image_size + image_size / 2\n",
    "    ax.scatter(pred[0], pred[1], marker=\"x\", c=\"r\", s=500, label=\"Prediction\")\n",
    "\n",
    "    ax.set_axis_off()\n",
    "\n",
    "ax.legend(loc=(0.5, 0.8), framealpha=1, fontsize='x-large')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dt_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
