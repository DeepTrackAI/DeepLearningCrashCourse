{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Particle Localization\n",
    "\n",
    "We'll compare the performance of a dense neural network and of a convolutional neural network for the classification of blood smears in the Malaria dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "We'll load a dataset including two videos of optically trapped particles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "if not os.path.exists(\"particle_dataset\"):\n",
    "    os.system(\"git clone -b cm https://github.com/DeepTrackAI/particle_dataset\")\n",
    "\n",
    "train_path = os.path.join(\"particle_dataset\", \"particle_dataset\")\n",
    "train_video_path = glob.glob(os.path.join(train_path,'*.avi'))\n",
    "\n",
    "print(f\"{len(train_video_path)} training videos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_video_path[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def load_video(path, frames_to_load=100, image_size=51):\n",
    "    video = cv2.VideoCapture(path) \n",
    "    data = []\n",
    "    for _ in range(frames_to_load):\n",
    "        (_, frame) = video.read()\n",
    "        frame = cv2.normalize(frame, None, 0, 255, cv2.NORM_MINMAX) \n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY) / 255 \n",
    "        frame = cv2.resize(frame, (image_size, image_size)) \n",
    "        data.append(frame) \n",
    "    return np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_noise_data = load_video(train_video_path[1])\n",
    "high_noise_data = load_video(train_video_path[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(2, 6, figsize=(25, 7))\n",
    "for i in range(6):\n",
    "    im = ax[0, i].imshow(low_noise_data[i], vmin=0, vmax=1, cmap=\"gray\")\n",
    "    ax[0, i].text(0, 5, \"Frame {}\".format(i), color=\"white\", fontsize=12)\n",
    "\n",
    "    ax[0, i].axis(\"off\") \n",
    "    ax[1, i].imshow(high_noise_data[i], vmin=0, vmax=1, cmap=\"gray\") \n",
    "    ax[1, i].text(0, 5, \"Frame {}\".format(i), color=\"white\", fontsize=12) \n",
    "    ax[1, i].axis(\"off\")\n",
    "plt.subplots_adjust(wspace=.1, hspace=.1) \n",
    "plt.colorbar(im, ax=ax.ravel().tolist())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.widgets import Cursor\n",
    "\n",
    "class ParticleCenter():\n",
    "    def __init__(self, images):\n",
    "        self.images = images\n",
    "        self.positions = []\n",
    "        self.i = 0\n",
    "        self.fig, self.ax = plt.subplots(1, 1, figsize=(5, 5))\n",
    "        self.fig.canvas.header_visible = False\n",
    "        self.fig.canvas.footer_visible = False\n",
    "    def start(self):\n",
    "        self.im = self.ax.imshow(self.images[self.i], cmap=\"gray\", vmin=0, vmax=1)\n",
    "        self.text = self.ax.text(3, 5, \"Frame \" + str(self.i+1) + \" of \" + str(len(self.images)), color=\"white\", fontsize=12) \n",
    "        self.ax.axis(\"off\")\n",
    "        self.cursor = Cursor(self.ax, useblit=True, color=\"red\", linewidth=1)\n",
    "        self.cid = self.fig.canvas.mpl_connect(\"button_press_event\", self.onclick)\n",
    "        self.next_image()\n",
    "        plt.show()\n",
    "    def next_image(self):\n",
    "        im = self.images[self.i]\n",
    "        self.im.set_data(im)\n",
    "        self.text.set_text(\"Frame \" + str(self.i+1) + \" of \" + str(len(self.images)))\n",
    "        self.fig.canvas.draw_idle()\n",
    "    def onclick(self, event):\n",
    "        self.positions.append([event.xdata, event.ydata])\n",
    "        if self.i < len(self.images)-1:\n",
    "            self.i += 1\n",
    "            self.next_image()\n",
    "        else:\n",
    "            self.fig.canvas.mpl_disconnect(self.cid)\n",
    "            plt.close()\n",
    "            return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "from numpy.random import choice\n",
    "\n",
    "number_of_images_to_annotate = 50\n",
    "\n",
    "dataset = np.concatenate([low_noise_data, high_noise_data], axis=0)\n",
    "np.random.shuffle(dataset)\n",
    "\n",
    "images_to_annotate = choice(\n",
    "  np.arange(dataset.shape[0]),\n",
    "  number_of_images_to_annotate,\n",
    "  replace=False\n",
    ")\n",
    "\n",
    "pc = ParticleCenter(dataset[images_to_annotate])\n",
    "pc.start()\n",
    "\n",
    "annotated_data = pc.images \n",
    "labels = pc.positions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_ratio = 0.8\n",
    "train_data = annotated_data[:int(split_ratio * len(annotated_data))]\n",
    "train_labels = labels[:int(split_ratio * len(labels))]\n",
    "np.save(os.path.join(train_path, \"train_data.npy\"), np.array(train_data))\n",
    "np.save(os.path.join(train_path, \"train_labels.npy\"), np.array(train_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remaining_images = np.delete(dataset, images_to_annotate, axis=0)\n",
    "np.save(os.path.join(train_path, \"test_data.npy\"), np.array(remaining_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deeplay as dl\n",
    "\n",
    "CNN = dl.Sequential(\n",
    "    dl.ConvolutionalNeuralNetwork(in_channels = 1, hidden_channels = [16, 32, 64], out_channels = 64),\n",
    "    #dl.MultiLayerPerceptron(in_features = 64, hidden_features = [], out_features = 1, out_activation = torch.nn.Sigmoid)\n",
    ")\n",
    "#CNN[0].blocks[2].pool.configure(torch.nn.MaxPool2d, kernel_size = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll define the path to the directories containing the `Infected` and `Parasitized` images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Data\n",
    "\n",
    "We'll then visualize some of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from blood_smears import plot_examples\n",
    "plot_examples(uninfected_files,infected_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "We'll define a popeline to resize and normalize the data ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as tt\n",
    "\n",
    "pipeline = tt.Compose([tt.Resize((28,28)),\n",
    "    tt.ToTensor()]) # automatically converts images to [0,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... subset the full dataset and split it into `train` and `test` sets ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import Lambda\n",
    "from torch.utils.data import random_split, Subset\n",
    "\n",
    "dataset = ImageFolder(base_dir, pipeline, target_transform= Lambda(lambda y: torch.tensor(abs(1-y)).float().unsqueeze(-1)))\n",
    "\n",
    "subset_size = 5000\n",
    "subset_indices = torch.randperm(len(dataset))[:subset_size]\n",
    "subset = Subset(dataset, subset_indices)\n",
    "\n",
    "train_size = int(0.8 * len(subset))\n",
    "test_size = len(subset) - train_size\n",
    "train, test = random_split(subset, [train_size, test_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and define the dataloaders for both sets. For the training, we'll set `batch_size = 32`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test, batch_size=batch_size * 5, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fully-connected Neural Network\n",
    "\n",
    "We'll define a Fully-connected Neural Network (FCNN) using `deeplay`. The FCNN has 2 layers with 128 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deeplay as dl\n",
    "\n",
    "FCNN = dl.MultiLayerPerceptron(in_features = 28 * 28 * 3,\n",
    "                                hidden_features = [128, 128],\n",
    "                                out_features = 1,\n",
    "                                out_activation = torch.nn.Sigmoid,\n",
    ")\n",
    "FCNN.blocks.activation.configure(torch.nn.Sigmoid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll define a classifier based on the FCNN architecture, including loss function, evaluation metrics and othe hyperparameters ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchmetrics as tm\n",
    "\n",
    "FCNN_classifier_template = dl.BinaryClassifier(\n",
    "        model=FCNN,\n",
    "        optimizer=dl.RMSprop(lr=.001),\n",
    "        )\n",
    "\n",
    "FCNN_classifier = FCNN_classifier_template.create()\n",
    "print(FCNN_classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and a trainer including other hyperparameters ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FCNN_trainer = dl.Trainer(\n",
    "    max_epochs=20, # How many times to run through the entire dataset\n",
    "    accelerator=\"auto\", # Use GPU if available\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start the training and visualize the evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FCNN_trainer.fit(FCNN_classifier, train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll calculate the performance over the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = FCNN_trainer.test(FCNN_classifier, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from blood_smears import plot_ROC_AUC\n",
    "_,_,_,_ = plot_ROC_AUC(classifier = FCNN_classifier, dataset=test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network with a dense top\n",
    "We'll now build a convolutional neural network (CNN) with a FCNN at the end ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... define a classifier using the CNN ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN_classifier_template = dl.BinaryClassifier(\n",
    "    model=CNN, \n",
    "    optimizer=dl.RMSprop(lr=.001),\n",
    ")\n",
    "\n",
    "CNN_classifier = CNN_classifier_template.create()\n",
    "print(CNN_classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... train it ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN_trainer = dl.Trainer(\n",
    "    max_epochs=20, # How many times to run through the entire dataset\n",
    "    accelerator=\"auto\", # Use GPU if available\n",
    ")\n",
    "\n",
    "CNN_trainer.fit(CNN_classifier, train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... evaluate the performance over the test set ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = CNN_trainer.test(CNN_classifier, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and display the ROC curve with the AUC value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, gt, pred, _ = plot_ROC_AUC(classifier=CNN_classifier, dataset=test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Failure analysis\n",
    "We'll use a function to visualize some of the wrongly classified cells, looking for common patterns.\n",
    "```python\n",
    "def plot_failure(images, gt, pred, threshold = 0.5, num_of_plots = 5):\n",
    "    from matplotlib import pyplot as plt \n",
    "    from numpy import array, squeeze   \n",
    "    \n",
    "    pred = array(pred).squeeze()\n",
    "    gt = array(gt).squeeze()\n",
    "    images = array(images)\n",
    "\n",
    "    pred_class = pred > threshold\n",
    "\n",
    "    false_positives = (pred_class == 1) & (gt == 0)\n",
    "    false_positives_images = images[false_positives]\n",
    "\n",
    "    false_negatives = (pred_class == 0) & (gt == 1)\n",
    "    false_negatives_images = images[false_negatives]\n",
    "\n",
    "    plt.figure(figsize=(num_of_plots*2, 5))\n",
    "    for i in range(num_of_plots):\n",
    "\n",
    "        # false positives\n",
    "        plt.subplot(2, num_of_plots, i + 1)\n",
    "        plt.imshow(false_positives_images[i].transpose(1, 2, 0))\n",
    "        if i == 0:\n",
    "            plt.title(\"False positives\", fontsize=16, y=1.1)\n",
    "\n",
    "        # false negatives\n",
    "        plt.subplot(2, num_of_plots, i + num_of_plots + 1)\n",
    "        plt.imshow(false_negatives_images[i].transpose(1, 2, 0))\n",
    "        if i == 0:\n",
    "            plt.title(\"False negatives\", fontsize=16, y=1.1)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from blood_smears import plot_failure\n",
    "plot_failure(images=images, gt=gt, pred=pred, threshold = 0.5, num_of_plots = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filters\n",
    "We can access and visualize the filters used by the network at a specific layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = CNN_classifier.model[0].input_block.layer.weight\n",
    "w = weights.clone().detach()\n",
    "\n",
    "from blood_smears import plot_filters_activations\n",
    "plot_filters_activations(input = w, n_rows=4, label = 'Filters', normalize = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activations and Grad-CAM\n",
    "To visualize the network feautures, we'll use `hooks`, functions that allows us to access the information that the model sees during forward and backward passes, such as activations and gradients, respectively. We'll define them as context manager classes, so that we can use them with the `with` statement:\n",
    "```python\n",
    "class fwd_hook():\n",
    "    def __init__(self, m):\n",
    "        self.hook = m.register_forward_hook(self.hook_func)   \n",
    "    def hook_func(self, m, i, o):\n",
    "        print('Forward hook running...') \n",
    "        self.stored = o.detach().clone()\n",
    "        print(f'Activations size: {self.stored.size()}')\n",
    "    def __enter__(self, *args): \n",
    "        return self\n",
    "    def __exit__(self, *args): \n",
    "        self.hook.remove()\n",
    "\n",
    "class bwd_hook():\n",
    "    def __init__(self, m):\n",
    "        self.hook = m.register_full_backward_hook(self.hook_func)\n",
    "    def hook_func(self, m, gi, go):\n",
    "        print('Backward hook running...')\n",
    "        self.stored = go[0].detach().clone()\n",
    "        print(f'Gradients size: {self.stored.size()}')\n",
    "    def __enter__(self, *args): \n",
    "        return self\n",
    "    def __exit__(self, *args): \n",
    "        self.hook.remove()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll randomly pick the image of an infected smear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "ind_infect = np.where((torch.cat(gt)==1).tolist())[0]\n",
    "ind=np.random.choice(ind_infect,1)[0]\n",
    "\n",
    "test_image = images[ind]\n",
    "test_image_hr=mpimg.imread(dataset.imgs[subset.indices[test.indices[ind]]][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activations and gradients at a specific layer can be obtained from the forward and backward pass, respectively ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from blood_smears import fwd_hook, bwd_hook\n",
    "\n",
    "test_layer = CNN_classifier.model[0].blocks[3].layer\n",
    "\n",
    "with bwd_hook(test_layer) as bh:\n",
    "    with fwd_hook(test_layer) as fh:\n",
    "        out = CNN_classifier.model(test_image.unsqueeze(0)).backward()\n",
    "activations = fh.stored\n",
    "gradients = bh.stored"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... we can plot the activations ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_filters_activations(input = activations.permute(1,0,2,3),n_rows=8,label = 'Feature maps', normalize = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... or combine gradients and activations to calculate Grad-CAM and inspect on which part of an image the CNN focuses on to predict its outputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pooled_grad = gradients[0].mean(dim=[1,2], keepdim = True)\n",
    "grad_cam = torch.nn.functional.relu((pooled_grad*activations[0]).sum(0)).detach().numpy()\n",
    "\n",
    "from blood_smears import plot_gradcam\n",
    "plot_gradcam(grad_cam, test_image_hr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dt_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
