{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Style Transfer\n",
    "\n",
    "We'll perform style transfer between images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select Imput Image\n",
    "\n",
    "We begin by loading the example image titled `neuraltissue_with_colorlabels.png`, of which we crop a patch of 256 by 256 pixels.\n",
    "\n",
    "This image is sourced from the Drosophila ssTEM dataset, which is publicly available on Figshare: [Segmented anisotropic ssTEM dataset of neural tissue](https://figshare.com/articles/dataset/Segmented_anisotropic_ssTEM_dataset_of_neural_tissue/856713). This dataset provides a detailed view of neural tissue, aiding in the study of neural structures and patterns. The image can also be downloaded from the corresponding GitHub repository at [this link](http://github.com/unidesigner/groundtruth-drosophila-vnc), which offers additional resources and information related to the Drosophila ssTEM dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "content = Image.open(\"neuraltissue_with_colorlabels.png\"\n",
    "                   ).convert('RGB').crop((100, 170, 100 + 256, 170 + 256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.imshow(content)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Select Style Image\n",
    "\n",
    "We use the style of the _trencadis_ lizard by Antoni Gaudi  in Parc Guell (Barcelona) in the image `lizard.png`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "style = Image.open(\"style.png\").convert('RGB').resize((256, 256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(style)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Pretrained Neural Network\n",
    "\n",
    "We import the VGG16 model, a pretrained neural network known for its proficiency in image recognition tasks, with weights initialized from the ImageNet dataset. We then set the model to evaluation mode and freeze all weights to prevent further changes during our operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "from torchvision.models import VGG16_Weights\n",
    "\n",
    "model = models.vgg16(weights=VGG16_Weights.IMAGENET1K_V1)\n",
    "\n",
    "model.eval()\n",
    "model.requires_grad_(False)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Style Transfer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll define a function that minimize the style and content losses with respect to the reference images. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a funtion for calculating the Gram matrix between all the features of a specific layer.\n",
    "The Gram matrix represents the correlations between different feature maps (or channels) of the output of a convolutional layer.\n",
    "\n",
    "This is the `gram()` function and add it to `fnc_style_transfer.py`:\n",
    "\n",
    "```python\n",
    "def gram(tensor):\n",
    "    from torch import bmm\n",
    "    \n",
    "    batch_size, num_channels, height, width = tensor.size()\n",
    "    features = tensor.view(batch_size, num_channels, height * width)\n",
    "    gram = bmm(features, features.transpose(1, 2)) / (height * width)\n",
    "\n",
    "    return gram\n",
    "```\n",
    "\n",
    "This function:\n",
    "- Unpacks the dimensions of the input tensor\n",
    "- Reshapes the tensor so it's a 2D matrix, with channels as features and height*width as observations\n",
    "- Computes the Gram matrix as the product of the matrix by its transpose, normalizing by the number of elements in each feature map (`height * width`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also add the `image_to_tensor()` function to `fnc_style_transfer.py` (the same function as for the DeepDream project).\n",
    "\n",
    "```python\n",
    "def image_to_tensor(im, mean, std):\n",
    "    import torchvision.transforms as tt\n",
    "\n",
    "    normalize = tt.Compose([tt.ToTensor(), tt.Normalize(mean, std)])\n",
    "\n",
    "    return normalize(im).unsqueeze(0).requires_grad_(True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also add the `tensor_to_image()` function to `fnc_style_transfer.py` (the same function as for the DeepDream project).\n",
    "\n",
    "```python\n",
    "def tensor_to_image(image, mean, std):\n",
    "    import torchvision.transforms as tt\n",
    "    import numpy as np\n",
    "    from PIL import Image\n",
    "\n",
    "    denormalize = tt.Normalize(mean=-mean / std, std=1 / std)\n",
    "\n",
    "    im_array = denormalize(image.data.clone().detach().squeeze()).numpy()\n",
    "    im_array = np.clip(im_array.transpose(1, 2, 0) * 255, 0, 255).astype(np.uint8)\n",
    "    return Image.fromarray(im_array, 'RGB')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also add the `fwd_hooks()` function to `fnc_style_transfer.py` (the same function as for the DeepDream project).\n",
    "\n",
    "```python\n",
    "class fwd_hooks():\n",
    "    def __init__(self, layers):\n",
    "        self.hooks = []\n",
    "        self.activations_list = []\n",
    "        for layer in layers:\n",
    "            self.hooks.append(layer.register_forward_hook(self.hook_func))\n",
    "\n",
    "    def hook_func(self, layer, input, output):\n",
    "        self.activations_list.append(output)\n",
    "\n",
    "    def __enter__(self, *args): \n",
    "        return self\n",
    "    \n",
    "    def __exit__(self, *args): \n",
    "        for hook in self.hooks:\n",
    "            hook.remove()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from fnc_style_transfer import gram, image_to_tensor, tensor_to_image, fwd_hooks\n",
    "\n",
    "def style_transfer(image, content, style, \n",
    "                   content_layers, style_layers, \n",
    "                   lr=1, beta=1e3, iter_num=100):\n",
    "    # Normalization parameters typically used with pretrained models\n",
    "    mean = np.array([0.485, 0.456, 0.406], dtype=np.float32)\n",
    "    std = np.array([0.229, 0.224, 0.225], dtype=np.float32)\n",
    "\n",
    "    # input image \n",
    "    image_tensor = image_to_tensor(image, mean, std)#.requires_grad_(True)\n",
    "    \n",
    "    # content\n",
    "    with fwd_hooks(content_layers) as fh:\n",
    "        _ = model(image_to_tensor(content, mean, std))\n",
    "    content_features = [activations.detach() for activations in fh.activations_list]\n",
    "\n",
    "    # style\n",
    "    with fwd_hooks(style_layers) as fh:\n",
    "        _ = model(image_to_tensor(style, mean, std))\n",
    "    style_features = [activations.detach() for activations in fh.activations_list]\n",
    "    gram_targets = [gram(s) for s in style_features]\n",
    "\n",
    "    optimizer = torch.optim.LBFGS([image_tensor], lr=lr)\n",
    "    mse_loss = torch.nn.MSELoss(reduction=\"sum\")\n",
    "\n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # content\n",
    "        with fwd_hooks(content_layers) as fh:\n",
    "            _ = model(image_tensor)\n",
    "        image_content_features = fh.activations_list\n",
    "\n",
    "        content_loss = 0\n",
    "        for icf, cf in zip(image_content_features, content_features):\n",
    "            n_f = icf.shape[1]\n",
    "            content_loss += mse_loss(icf, cf) / n_f ** 2\n",
    "        content_loss /= len(image_content_features)\n",
    "\n",
    "        # style\n",
    "        with fwd_hooks(style_layers) as fh:\n",
    "            _ = model(image_tensor)\n",
    "        image_style_features = fh.activations_list\n",
    "        gram_image = [gram(i) for i in image_style_features]\n",
    "\n",
    "        style_loss = 0\n",
    "        for gi, gt in zip(gram_image, gram_targets):\n",
    "            n_g = gi.shape[1]\n",
    "            style_loss += mse_loss(gi, gt) / n_g ** 2\n",
    "        style_loss /= len(gram_image)\n",
    "\n",
    "        print(f\"content_loss={content_loss} style_loss={style_loss}\")\n",
    "\n",
    "        total_loss = content_loss + beta * style_loss\n",
    "        total_loss.backward()\n",
    "        return total_loss\n",
    "\n",
    "    for i in range(iter_num):\n",
    "        print(f\"iteration {i}\")\n",
    "\n",
    "        optimizer.step(closure)\n",
    "\n",
    "        image = tensor_to_image(image_tensor, mean, std)\n",
    "        \n",
    "        if i <= 5 or i % 10 == 0:\n",
    "            plt.imshow(image)\n",
    "            plt.title(f\"Iteration {i}\")\n",
    "            plt.axis(\"off\")\n",
    "            plt.show()\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll apply the style transfer using as an input the same image used to get the content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_style(content, style, image):\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    plt.figure(figsize=(15, 5)) \n",
    "    \n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(content)\n",
    "    plt.title('Content image')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.imshow(style)\n",
    "    plt.title('Style image') \n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.imshow(image)\n",
    "    plt.title('Output image') \n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from style_transfer import plot_style\n",
    "\n",
    "#ind = [0, 2, 5, 7, 10, 14]\n",
    "#layers = [model.features[i] for i in ind] \n",
    "#ind_c = [5]\n",
    "#ind_s = [0, 1, 2, 3, 4]\n",
    "\n",
    "#image_out  =  style_transfer(content, content, style, layers, ind_c, ind_s, lr=1, beta=1e5, iter_num=50)\n",
    "image_out = style_transfer(\n",
    "    content, content, style, \n",
    "    content_layers=[model.features[l] for l in [14]], \n",
    "    style_layers=[model.features[l] for l in [0, 2, 5, 7, 10]],\n",
    "    lr=1, beta=1e5, iter_num=50,\n",
    ")\n",
    "\n",
    "plot_style(content, style, image_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also apply the style transfer using as an input a random image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imarray = np.random.rand(256, 256, 3) * 255\n",
    "image_in = Image.fromarray(imarray.astype('uint8')).convert('RGB')\n",
    "\n",
    "#image_out  =  style_transfer(image_in, content, style, layers, ind_c, ind_s, lr = 1, beta = 1e4, iter_num=50)\n",
    "image_out = style_transfer(\n",
    "    image_in, content, style, \n",
    "    content_layers=[model.features[l] for l in [14]], \n",
    "    style_layers=[model.features[l] for l in [0, 2, 5, 7, 10]],\n",
    "    lr=1, beta=1e5, iter_num=50,\n",
    ")\n",
    "\n",
    "plot_style(content, style, image_out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplay_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
