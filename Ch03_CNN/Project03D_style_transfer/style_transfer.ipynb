{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Style Transfer\n",
    "\n",
    "We'll describe an implementation of DeepDream."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "We'll load the example image of the Drosophila ssTEM dataset from https://figshare.com/articles/dataset/Segmented_anisotropic_ssTEM_dataset_of_neural_tissue/856713. Alternatively, you can download an image from the corresponding GiHub repository: http://github.com/unidesigner/groundtruth-drosophila-vnc."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from PIL import Image\n",
    "from urllib.request import urlopen\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "image_size = (256, 256)\n",
    "\n",
    "url = 'https://s3-eu-west-1.amazonaws.com/pfigshare-u-previews/1288336/preview.jpg'\n",
    "\n",
    "width = image_size[0]\n",
    "height = image_size[1]\n",
    "left = 100\n",
    "top = 170\n",
    "\n",
    "im_c = Image.open(urlopen(url)).crop((left,top,left + width, top + height))\n",
    "\n",
    "from style_transfer import plot_example\n",
    "plot_example(im_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "image = Image.open(\"neuraltissue_with_colorlabels.png\").convert('RGB').resize((256, 256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.imshow(image)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the style of the _trencadis_ lizard by Antoni Gaudi  in Parc Guell (Barcelona)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "url = 'https://upload.wikimedia.org/wikipedia/commons/d/da/Reptil_Parc_Guell_Barcelona.jpg'\n",
    "\n",
    "width = 512\n",
    "height = 512\n",
    "left = 800\n",
    "top = 250\n",
    "\n",
    "im_s = Image.open(urlopen(url)).crop((left,top,left + width, top + height)).resize(image_size)\n",
    "\n",
    "plot_example(im_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "style = Image.open(\"style.png\").convert('RGB').resize((256, 256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(style)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll define a funtion for calculating the Gram matrix between all the features of a specific layer.\n",
    "\n",
    "Gramm matrix represents the correlations between different feature maps of a layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import bmm\n",
    "\n",
    "def gram_matrix(tensor):\n",
    "    # Unpack the dimensions of the input tensor\n",
    "    batch_size, num_channels, height, width = tensor.size()\n",
    "\n",
    "    # Reshape the tensor so it's a 2D matrix, with channels as features and height*width as observations\n",
    "    features = tensor.view(batch_size, num_channels, height * width)\n",
    "\n",
    "    # Compute the Gram matrix as the product of the matrix by its transpose\n",
    "    # Normalizing by the number of elements in each feature map (height*width)\n",
    "    gram = bmm(features, features.transpose(1, 2)) / (height * width)\n",
    "\n",
    "    return gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll download a pretrained model (VGG16) and freeze all the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "from torchvision.models import VGG16_Weights\n",
    "\n",
    "model = models.vgg16(weights=VGG16_Weights.IMAGENET1K_V1)\n",
    "\n",
    "model.eval()\n",
    "model.requires_grad_(False)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The VGG16 is pretrained on the ImageNet dataset and the inputs are normalized wih respect to the mean and standard deviation of the channels of this dataset."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "mean_ds = np.array([0.485, 0.456, 0.406], dtype=np.float32)\n",
    "std_ds = np.array([0.229, 0.224, 0.225], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll define a function that minimize the style and content losses with respect to the reference images. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def preprocess(image, mean_ds, std_ds):\n",
    "    import torch \n",
    "    from torchvision import transforms\n",
    "    normalize = transforms.Compose([transforms.ToTensor(),\n",
    "                            transforms.Normalize(mean=mean_ds,\n",
    "                                 std=std_ds),\n",
    "                            ])\n",
    "    tensor = normalize(image).unsqueeze(0).requires_grad_(True)\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def image_to_tensor(im, mean, std):\n",
    "    import torchvision.transforms as tt\n",
    "\n",
    "    normalize = tt.Compose([tt.ToTensor(), tt.Normalize(mean, std)])\n",
    "\n",
    "    return normalize(im).unsqueeze(0).requires_grad_(True)\n",
    "```"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def deprocess(tensor, mean_ds, std_ds):\n",
    "    import numpy as np\n",
    "    from torchvision import transforms\n",
    "    denormalize = transforms.Normalize(mean = -mean_ds/std_ds, \n",
    "                                    std = 1/std_ds )\n",
    "    im = np.array(denormalize(tensor.detach().squeeze())).transpose(1,2,0)\n",
    "    return im"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def tensor_to_image(image, mean, std):\n",
    "    import torchvision.transforms as tt\n",
    "    import numpy as np\n",
    "    from PIL import Image\n",
    "\n",
    "    denormalize = tt.Normalize(mean=-mean / std, std=1 / std)\n",
    "\n",
    "    im_array = denormalize(image.data.clone().detach().squeeze()).numpy()\n",
    "    im_array = np.clip(im_array.transpose(1, 2, 0) * 255, 0, 255).astype(np.uint8)\n",
    "    return Image.fromarray(im_array, 'RGB')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "class fwd_hooks():\n",
    "    def __init__(self, layers):\n",
    "        self.hooks = []\n",
    "        self.activations_list = []\n",
    "        for layer in layers:\n",
    "            self.hooks.append(layer.register_forward_hook(self.hook_func))\n",
    "\n",
    "    def hook_func(self, layer, input, output):\n",
    "        self.activations_list.append(output)\n",
    "\n",
    "    def __enter__(self, *args): \n",
    "        return self\n",
    "    \n",
    "    def __exit__(self, *args): \n",
    "        for hook in self.hooks:\n",
    "            hook.remove()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms as tt\n",
    "from PIL import Image\n",
    "from fnc_style_transfer import image_to_tensor, tensor_to_image, fwd_hooks\n",
    "\n",
    "def style_transfer(im_in, im_c, im_s, layers, ind_c, ind_s, lr = 1, beta = 1e3, iter_num=100):\n",
    "    # Normalization parameters typically used with pretrained models\n",
    "    mean_ds = np.array([0.485, 0.456, 0.406], dtype=np.float32)\n",
    "    std_ds = np.array([0.229, 0.224, 0.225], dtype=np.float32)\n",
    "\n",
    "    # image\n",
    "    image_c = image_to_tensor(im_c, mean_ds, std_ds)\n",
    "\n",
    "    with fwd_hooks(layers) as fh:\n",
    "        _ = model(image_c)\n",
    "    content_features = [fh.activations_list[i].detach() for i in ind_c]\n",
    "\n",
    "    # style\n",
    "    image_s = image_to_tensor(im_s, mean_ds, std_ds)\n",
    "\n",
    "    with fwd_hooks(layers) as fh:\n",
    "        _ = model(image_s)\n",
    "    style_features = [fh.activations_list[i].detach() for i in ind_s]\n",
    "    gram_targets = [gram_matrix(s) for s in style_features]\n",
    "\n",
    "    #input\n",
    "    try: im_in.verify()\n",
    "    except: \n",
    "        print('Input image not provided. Using a random input.')\n",
    "        imarray = np.random.rand(256, 256, 3) * 255                             ### GET IMAGE SIZE!\n",
    "        im_in = Image.fromarray(imarray.astype('uint8')).convert('RGB')\n",
    "    \n",
    "    image_in = image_to_tensor(im_in, mean_ds, std_ds)#.requires_grad_(True)\n",
    "    \n",
    "    optimizer = torch.optim.LBFGS([image_in], lr=lr)\n",
    "    mse_loss = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "    l_c = []\n",
    "    l_s = []\n",
    "\n",
    "    for it in range(iter_num):\n",
    "        def closure():\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # content & style\n",
    "            with fwd_hooks(layers) as fh:\n",
    "                out = model(image_in)\n",
    "            content_features_in = [fh.activations_list[i] for i in ind_c]\n",
    "            style_features_in = [fh.activations_list[i] for i in ind_s]\n",
    "            gram_in = [gram_matrix(i) for i in style_features_in]\n",
    "\n",
    "            c_loss = 0\n",
    "            for i,c in enumerate(content_features_in):\n",
    "                n_f = c.shape[1]\n",
    "                c_loss += mse_loss(c,content_features[i])/n_f**2\n",
    "            c_loss /= len(content_features_in)\n",
    "\n",
    "            s_loss = 0\n",
    "            for i,g in enumerate(gram_in):\n",
    "                n_g = g.shape[1]\n",
    "                s_loss += mse_loss(g,gram_targets[i]) / n_g ** 2\n",
    "            s_loss /= len(gram_in)\n",
    "\n",
    "            loss = c_loss + beta * s_loss\n",
    "            l_c.append(c_loss)\n",
    "            l_s.append(s_loss)\n",
    "\n",
    "            loss.backward()\n",
    "            return loss\n",
    "\n",
    "        optimizer.step(closure)\n",
    "        print('Step {}: Content Loss: {:.8f} Style Loss: {:.8f}'.format(it, l_c[-1], l_s[-1]))\n",
    "\n",
    "        # if (it)%10 == 0:\n",
    "        #     im_out = Image.fromarray(np.uint8(np.clip(deprocess(image_in.data.clone().detach(), mean_ds, std_ds)*255,0,255)), 'RGB') \n",
    "        #     plot_style(im_c, im_s, im_out)\n",
    "                                                                                ###return Image.fromarray(np.uint8(np.clip(deprocess(image_in.data.clone(), mean_ds, std_ds)*255,0,255)), 'RGB')\n",
    "    return tensor_to_image(image_in, mean_ds, std_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll apply the style transfer using as an input the same image used to get the content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_style(im_c, im_s, im_out):\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.figure(figsize=(15, 5)) \n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(im_c)\n",
    "    plt.title('Content image')\n",
    "    plt.axis('off')\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.imshow(im_s)\n",
    "    plt.title('Style image') \n",
    "    plt.axis('off')\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.imshow(im_out)\n",
    "    plt.title('Output image') \n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from style_transfer import plot_style\n",
    "\n",
    "ind = [0, 2, 5, 7, 10, 14]\n",
    "layers = [model.features[i] for i in ind ] \n",
    "ind_c = [5]\n",
    "ind_s = [0, 1, 2, 3, 4]\n",
    "\n",
    "im_out  =  style_transfer(image, image, style, layers, ind_c, ind_s, lr=1, beta=1e5, iter_num=200)\n",
    "plot_style(image, style, im_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also apply the style transfer using as an input a random image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_out  =  style_transfer([], image, style, layers, ind_c, ind_s, lr = 1, beta = 1e4, iter_num=200)\n",
    "plot_style(image, style, im_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplay_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
