{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blood Smears Classification\n",
    "\n",
    "We'll compare the performance of a dense neural network and of a convolutional neural network with a dense top for the classification of blood smears in a dataser of blood smears containing blood cells with and without malaria. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Malaria Dataset\n",
    "\n",
    "We'll load and uncompress the single cell dataset from \n",
    "https://lhncbc.nlm.nih.gov/LHC-research/LHC-projects/image-processing/malaria-datasheet.html\n",
    "\n",
    "The Malaria dataset was published in S. Rajaraman, S. K. Antani, M. Poost- chi, K. Silamut, Md A. Hossain, R. J. Maude, S. Jaeger, and G. R. Thoma. Pre-trained convolutional neural net- works as feature extractors toward improved malaria parasite detection in thin blood smear images. PeerJ, 6:e4568, 2018.\n",
    "It is available at https://data.lhncbc.nlm.nih.gov/public/Malaria/cell_images.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torchvision.datasets.utils import download_url, _extract_zip\n",
    "\n",
    "dataset_path = os.path.join(\".\", \"blood_smears_dataset\")\n",
    "if not os.path.exists(dataset_path):\n",
    "    url = \"https://data.lhncbc.nlm.nih.gov/public/Malaria/cell_images.zip\"\n",
    "    download_url(url, \".\")\n",
    "    _extract_zip(\"cell_images.zip\", dataset_path, None)\n",
    "    os.remove(\"cell_images.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll define the path to the directories containing the `Infected` and `Parasitized` images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "base_dir = os.path.join(dataset_path, \"cell_images\")\n",
    "uninfected_files = glob.glob(os.path.join(base_dir, \"Uninfected\", \"*.png\"))\n",
    "parasitized_files = glob.glob(os.path.join(base_dir, \"Parasitized\", \"*.png\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Data\n",
    "\n",
    "We'll then visualize some of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_blood_smears(title, files):\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    fig, axs = plt.subplots(3, 3, figsize=(8, 8))\n",
    "    for i, ax in enumerate(axs.ravel()):\n",
    "        image = plt.imread(files[i])\n",
    "        ax.imshow(image)\n",
    "        \n",
    "    fig.suptitle(title, fontsize=16)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from fnc_blood_smears import plot_blood_smears\n",
    "\n",
    "plot_blood_smears(\"Uninfected\", uninfected_files)\n",
    "plot_blood_smears(\"Parasitized\", parasitized_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "We'll define a pipeline to resize the images to 28 by 28 pixels and convert them to PyTorch tensors (note that `ToTensor()` also normalizes their values between 0 and 1) ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import Compose, Resize, ToTensor\n",
    "\n",
    "input_transform = Compose([Resize((28, 28)), ToTensor()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... we set up a dataset where images are loaded from a structured directory (`base_dir`) ..."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import Lambda\n",
    "\n",
    "from torch import tensor\n",
    "\n",
    "dataset = ImageFolder(base_dir, transform=pipeline, target_transform= Lambda(lambda y: tensor(abs(1-y)).float().unsqueeze(-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import tensor\n",
    "\n",
    "def target_transform(target):\n",
    "    return tensor(abs(1 - target)).float().unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "dataset = ImageFolder(base_dir, \n",
    "                      transform=input_transform, \n",
    "                      target_transform=target_transform)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(f\"Default mapping: {dataset.class_to_idx}\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from torch import tensor\n",
    "\n",
    "dataset.class_to_idx = {'Parasitized': 1, 'Uninfected': 0}\n",
    "dataset.targets = [abs(1 - target) for target in dataset.targets]\n",
    "#dataset.targets = [tensor(abs(1 - target)).float().unsqueeze(-1) \n",
    "#                   for target in dataset.targets]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... creates a subset the full dataset and split it into `train` and `test` sets ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import randperm\n",
    "from torch.utils.data import random_split, Subset\n",
    "\n",
    "images_num = 5000\n",
    "images_idx = randperm(len(dataset))[:images_num]\n",
    "images = Subset(dataset, images_idx)\n",
    "\n",
    "train_size = int(0.8 * len(images))\n",
    "test_size = len(images) - train_size\n",
    "train, test = random_split(images, [train_size, test_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and define the dataloaders for both sets. For the training, we'll set `batch_size = 32`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test, batch_size=124, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dense Neural Network\n",
    "\n",
    "We'll define a dense neural network using `deeplay`. The FCNN has 2 layers with 128 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deeplay as dl\n",
    "from torch.nn import Sigmoid\n",
    "\n",
    "dnn = dl.MultiLayerPerceptron(in_features=28 * 28 * 3,\n",
    "                               hidden_features=[128, 128],\n",
    "                               out_features=1,\n",
    "                               out_activation=Sigmoid,\n",
    ")\n",
    "#dnn.blocks.activation.configure(Sigmoid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll define a classifier based on the FCNN architecture, including loss function, evaluation metrics and othe hyperparameters ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchmetrics as tm\n",
    "\n",
    "dnn_classifier_template = dl.BinaryClassifier(model=dnn,\n",
    "                                              optimizer=dl.RMSprop(lr=.001))\n",
    "\n",
    "dnn_classifier = dnn_classifier_template.create()\n",
    "\n",
    "print(dnn_classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and a trainer including other hyperparameters ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn_trainer = dl.Trainer(max_epochs=20, accelerator=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start the training and visualize the evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn_trainer.fit(dnn_classifier, train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll calculate the performance over the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = dnn_trainer.test(dnn_classifier, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC curve and AUC\n",
    "\n",
    "We'll use the function `plot_ROC_AUC` to get the ground truth and predictions for all the images in the test set, calculate the ROC and AUC, and visualize the results.\n",
    "\n",
    "```python\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_AUROC(classifier, dataset):\n",
    "    from torch import tensor, stack\n",
    "    from sklearn.metrics import roc_curve, auc\n",
    "    from matplotlib import pyplot as plt\n",
    "\n",
    "    # calculate predictions\n",
    "    images, gt = zip(*dataset)\n",
    "    pred = classifier(tensor(stack(images))).tolist()\n",
    "    \n",
    "    # calculate the ROC curve\n",
    "    fpr, tpr, thresholds = roc_curve(gt, pred, pos_label=1) \n",
    "    auroc = auc(fpr, tpr) \n",
    "\n",
    "    # plot the ROC curve\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.plot(fpr, tpr, label=f\"ROC curve (AUC = {auroc:.3f})\", linewidth=2)\n",
    "    plt.title(\"ROC curve\")\n",
    "    plt.xlabel(\"False positive rate\")\n",
    "    plt.ylabel(\"True positive rate\")\n",
    "    plt.axis(\"square\")\n",
    "    plt.xlim(0, 1)\n",
    "    plt.ylim(0, 1)\n",
    "    plt.legend(loc = 'center right')\n",
    "    plt.show()\n",
    "\n",
    "    return images, gt, pred, auroc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from fnc_blood_smears import plot_ROC_AUC\n",
    "\n",
    "_, _, _, _ = plot_AUROC(classifier=dnn_classifier, dataset=test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network with a Dense Top\n",
    "\n",
    "We'll now build a convolutional neural network a dense top using `deeplay` ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import AdaptiveAvgPool2d, MaxPool2d\n",
    "\n",
    "CNN = dl.Sequential(\n",
    "    dl.ConvolutionalNeuralNetwork(\n",
    "        in_channels=3, \n",
    "        hidden_channels=[32,32,64], \n",
    "        out_channels = 64\n",
    "    ),\n",
    "    dl.Layer(AdaptiveAvgPool2d, output_size=1),\n",
    "    dl.MultiLayerPerceptron(\n",
    "        in_features=64, \n",
    "        hidden_features=[], \n",
    "        out_features=1,\n",
    "        out_activation = Sigmoid)\n",
    ")\n",
    "CNN[0].blocks[2].pool.configure(MaxPool2d, kernel_size=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... define a classifier using the CNN ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN_classifier_template = dl.BinaryClassifier(\n",
    "    model=CNN, \n",
    "    optimizer=dl.RMSprop(lr=.001),\n",
    ")\n",
    "\n",
    "CNN_classifier = CNN_classifier_template.create()\n",
    "\n",
    "print(CNN_classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... train it ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN_trainer = dl.Trainer(max_epochs=20, accelerator=\"auto\")\n",
    "\n",
    "CNN_trainer.fit(CNN_classifier, train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... evaluate the performance over the test set ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = CNN_trainer.test(CNN_classifier, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and display the ROC curve with the AUC value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, gt, pred, _ = plot_AUROC(classifier=CNN_classifier, dataset=test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Failure analysis\n",
    "\n",
    "We'll use a function to visualize some of the wrongly classified cells, looking for common patterns.\n",
    "\n",
    "```python\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_failures(images, gt, pred, threshold=.5, plot_num=5):\n",
    "    from matplotlib import pyplot as plt \n",
    "    from numpy import array, squeeze\n",
    "    \n",
    "    pred = array(pred).squeeze()\n",
    "    gt = array(gt).squeeze()\n",
    "    images = array(images)\n",
    "\n",
    "    false_positives = (pred > threshold) & (gt == 0)\n",
    "    false_positives_images = images[false_positives]\n",
    "\n",
    "    false_negatives = (pred < threshold) & (gt == 1)\n",
    "    false_negatives_images = images[false_negatives]\n",
    "\n",
    "    plt.figure(figsize=(plot_num * 2, 5))\n",
    "    for i in range(plot_num):\n",
    "        # false positives\n",
    "        plt.subplot(2, plot_num, i + 1)\n",
    "        plt.imshow(false_positives_images[i].transpose(1, 2, 0))\n",
    "        if i == 0:\n",
    "            plt.title(\"False positives\", fontsize=16, y=1.1)\n",
    "\n",
    "        # false negatives\n",
    "        plt.subplot(2, plot_num, plot_num + i + 1)\n",
    "        plt.imshow(false_negatives_images[i].transpose(1, 2, 0))\n",
    "        if i == 0:\n",
    "            plt.title(\"False negatives\", fontsize=16, y=1.1)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from fnc_blood_smears import plot_failure\n",
    "\n",
    "plot_failures(images, gt, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filters\n",
    "\n",
    "We can access and visualize the filters used by the CNN at a specific layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = CNN_classifier.model[0].input.layer.weight\n",
    "w = weights.clone().detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_filters(weights, cols=8):\n",
    "    from matplotlib import pyplot as plt\n",
    "\n",
    "    rows = -(weights.shape[0] // -cols)\n",
    "\n",
    "    fig, axs = plt.subplots(rows, cols, figsize=(2 * cols, 2 * rows))\n",
    "    for i, ax in enumerate(axs.ravel()):\n",
    "        ax.axis('off')\n",
    "        if i < weights.shape[0]:\n",
    "            p  = weights[i].permute(1, 2, 0).numpy()\n",
    "            ax.imshow(p)\n",
    "            ax.set_title(i)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from fnc_blood_smears import plot_filters_activations\n",
    "\n",
    "plot_filters(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activations and Grad-CAM\n",
    "\n",
    "To visualize the network features, we'll use `hooks`, functions that allows us to access the information that the model sees during forward and backward passes, such as activations and gradients, respectively. We'll define them as context manager classes, so that we can use them with the `with` statement:\n",
    "\n",
    "```python\n",
    "\n",
    "```\n",
    "\n",
    "```python\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll randomly pick the image of an infected smear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class fwd_hook():\n",
    "    def __init__(self, m):\n",
    "        self.hook = m.register_forward_hook(self.hook_func)\n",
    "\n",
    "    def hook_func(self, m, i, o):\n",
    "        print('Forward hook running...') \n",
    "        self.stored = o.detach().clone()\n",
    "        print(f'Activations size: {self.stored.size()}')\n",
    "\n",
    "    def __enter__(self, *args): \n",
    "        return self\n",
    "    \n",
    "    def __exit__(self, *args): \n",
    "        self.hook.remove()\n",
    "\n",
    "\n",
    "class bwd_hook():\n",
    "    def __init__(self, m):\n",
    "        self.hook = m.register_full_backward_hook(self.hook_func)\n",
    "\n",
    "    def hook_func(self, m, gi, go):\n",
    "        print('Backward hook running...')\n",
    "        self.stored = go[0].detach().clone()\n",
    "        print(f'Gradients size: {self.stored.size()}')\n",
    "\n",
    "    def __enter__(self, *args): \n",
    "        return self\n",
    "    \n",
    "    def __exit__(self, *args): \n",
    "        self.hook.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "ind_infect = np.where((torch.cat(gt)==1).tolist())[0]\n",
    "ind = np.random.choice(ind_infect, 1)[0]\n",
    "\n",
    "test_image = images[ind]\n",
    "test_image_hr = mpimg.imread(dataset.imgs[subset.indices[test.indices[ind]]][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activations and gradients at a specific layer can be obtained from the forward and backward pass, respectively ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from fnc_blood_smears import fwd_hook, bwd_hook\n",
    "\n",
    "test_layer = CNN_classifier.model[0].blocks[3].layer\n",
    "\n",
    "with bwd_hook(test_layer) as bh:\n",
    "    with fwd_hook(test_layer) as fh:\n",
    "        out = CNN_classifier.model(test_image.unsqueeze(0)).backward()\n",
    "activations = fh.stored\n",
    "gradients = bh.stored"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... we can plot the activations ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from fnc_blood_smears import plot_activations\n",
    "\n",
    "plot_activations(input = activations.permute(1,0,2,3),n_rows=8,label = 'Feature maps', normalize = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... or combine gradients and activations to calculate Grad-CAM and inspect on which part of an image the CNN focuses on to predict its outputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gradcam(gcam, img):\n",
    "    from matplotlib import pyplot as plt\n",
    "    from skimage.transform import resize\n",
    "    from skimage.exposure import rescale_intensity\n",
    "\n",
    "    gcam = resize(gcam, img.shape, order = 2)\n",
    "    gcam = rescale_intensity(gcam,out_range=(0.25,1))\n",
    "\n",
    "    plt.figure(figsize=(12, 5)) \n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(img, interpolation = 'bilinear')\n",
    "    plt.title('Original image')\n",
    "    plt.axis('off')\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.imshow(gcam.mean(axis=-1), interpolation = 'bilinear')\n",
    "    plt.title('Grad-CAM')\n",
    "    plt.axis('off')\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.imshow(img*gcam)\n",
    "    plt.title('Overlay')\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pooled_grad = gradients[0].mean(dim=[1,2], keepdim = True)\n",
    "grad_cam = torch.nn.functional.relu((pooled_grad*activations[0]).sum(0)).detach().numpy()\n",
    "\n",
    "#from fnc_blood_smears import plot_gradcam\n",
    "\n",
    "plot_gradcam(grad_cam, test_image_hr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dt_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
