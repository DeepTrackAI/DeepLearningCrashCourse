{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blood Smear Classification\n",
    "\n",
    "We'll compare the performance of a dense neural network and of a convolutional neural network with a dense top for the classification of blood smears in a dataset of blood smears containing blood cells with and without malaria. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Malaria Dataset\n",
    "\n",
    "We load and uncompress the malaria dataset available at https://data.lhncbc.nlm.nih.gov/public/Malaria/cell_images.zip.\n",
    "\n",
    "This dataset was origianlly published in S. Rajaraman, S. K. Antani, M. Poost- chi, K. Silamut, Md A. Hossain, R. J. Maude, S. Jaeger, and G. R. Thoma. _Pre-trained convolutional neural networks as feature extractors toward improved malaria parasite detection in thin blood smear images._ PeerJ 6, e4568, 2018."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torchvision.datasets.utils import download_url, _extract_zip\n",
    "\n",
    "dataset_path = os.path.join(\".\", \"blood_smears_dataset\")\n",
    "if not os.path.exists(dataset_path):\n",
    "    url = \"https://data.lhncbc.nlm.nih.gov/public/Malaria/cell_images.zip\"\n",
    "    download_url(url, \".\")\n",
    "    _extract_zip(\"cell_images.zip\", dataset_path, None)\n",
    "    os.remove(\"cell_images.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now obtain the paths to the files containing the `Infected` and `Parasitized` images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "base_dir = os.path.join(dataset_path, \"cell_images\")\n",
    "uninfected_files = glob.glob(os.path.join(base_dir, \"Uninfected\", \"*.png\"))\n",
    "parasitized_files = glob.glob(os.path.join(base_dir, \"Parasitized\", \"*.png\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Data\n",
    "\n",
    "We impleemnt the function `plot_blood_smears`, save it in `fnc_blood_smears.py` ...\n",
    "\n",
    "```python\n",
    "def plot_blood_smears(title, files):\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    fig, axs = plt.subplots(3, 3, figsize=(8, 8))\n",
    "    for i, ax in enumerate(axs.ravel()):\n",
    "        image = plt.imread(files[i])\n",
    "        ax.imshow(image)\n",
    "        \n",
    "    fig.suptitle(title, fontsize=16)\n",
    "    plt.show()\n",
    "```\n",
    "\n",
    "... and use it to visualize some of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fnc_blood_smears import plot_blood_smears\n",
    "\n",
    "plot_blood_smears(\"Uninfected\", uninfected_files)\n",
    "plot_blood_smears(\"Parasitized\", parasitized_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Data\n",
    "\n",
    "We define a transformation to resize the images to 28 by 28 pixels and convert them to PyTorch tensors (note that `ToTensor()` also normalizes their values between 0 and 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import Compose, Resize, ToTensor\n",
    "\n",
    "input_transform = Compose([Resize((28, 28)), ToTensor()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then define a transformation to make the target equal to 0 for the uninfected cells and to 1 for the parasitized ones, achieved through `abs(1 - target)`. This step is followed by converting it into a PyTorch tensor using `tensor()`, ensuring the data is in the correct format for PyTorch operations. Next, we convert the tensor into a floating-point type with `float()`, enhancing its compatibility with PyTorch's computational requirements. Finally, we add a new dimension to the tensor using `unsqueeze(-1)`, preparing it for batch processing in neural network models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import tensor\n",
    "\n",
    "def target_transform(target):\n",
    "    return tensor(abs(1 - target)).float().unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then set up a dataset where images are loaded from a structured directory (`base_dir`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "dataset = ImageFolder(base_dir, \n",
    "                      transform=input_transform, \n",
    "                      target_transform=target_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We extract a subset with 5,000 of the ca. 27,000 cell images for computational efficiency, and split it into `train` and `test` sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import randperm\n",
    "from torch.utils.data import random_split, Subset\n",
    "\n",
    "images_num = 5000\n",
    "images_idx = randperm(len(dataset))[:images_num]\n",
    "subset = Subset(dataset, images_idx)\n",
    "\n",
    "train_size = int(0.8 * len(subset))\n",
    "test_size = len(subset) - train_size\n",
    "train, test = random_split(subset, [train_size, test_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we define the dataloaders for both sets. For the training, we'll set `batch_size=32`, while we'll set a larger batch for the testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test, batch_size=124, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dense Neural Network\n",
    "\n",
    "We define a dense neural network (DNN) using `deeplay` with 2 layers with 128 neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deeplay as dl\n",
    "from torch.nn import Sigmoid\n",
    "\n",
    "dnn = dl.MultiLayerPerceptron(in_features=28 * 28 * 3,\n",
    "                               hidden_features=[128, 128],\n",
    "                               out_features=1,\n",
    "                               out_activation=Sigmoid)\n",
    "\n",
    "print(dnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then define a classifier using the DNN ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchmetrics as tm\n",
    "\n",
    "dnn_classifier_template = dl.BinaryClassifier(model=dnn,\n",
    "                                              optimizer=dl.RMSprop(lr=.001))\n",
    "\n",
    "dnn_classifier = dnn_classifier_template.create()\n",
    "\n",
    "print(dnn_classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and a trainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn_trainer = dl.Trainer(max_epochs=20, accelerator=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train\n",
    "\n",
    "We train the DNN classifier ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn_trainer.fit(dnn_classifier, train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and evaluate its performance over the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = dnn_trainer.test(dnn_classifier, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot ROC Curve\n",
    "\n",
    "We implement the `plot_roc`, save it in `fnc_blood_smears.py` ...\n",
    "\n",
    "```python\n",
    "def plot_roc(classifier, dataset):\n",
    "    from torch import tensor, stack\n",
    "    from sklearn.metrics import roc_curve, auc\n",
    "    from matplotlib import pyplot as plt\n",
    "\n",
    "    # calculate predictions\n",
    "    images, gt = zip(*dataset)\n",
    "    pred = classifier(tensor(stack(images))).tolist()\n",
    "    \n",
    "    # calculate the ROC curve\n",
    "    fpr, tpr, thresholds = roc_curve(gt, pred, pos_label=1) \n",
    "    roc = auc(fpr, tpr) \n",
    "\n",
    "    # plot the ROC curve\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.plot(fpr, tpr, label=f\"ROC curve (AUC = {roc:.3f})\", linewidth=2)\n",
    "    plt.title(\"ROC curve\")\n",
    "    plt.xlabel(\"False positive rate\")\n",
    "    plt.ylabel(\"True positive rate\")\n",
    "    plt.axis(\"square\")\n",
    "    plt.xlim(0, 1)\n",
    "    plt.ylim(0, 1)\n",
    "    plt.legend(loc = 'center right')\n",
    "    plt.show()\n",
    "\n",
    "    return images, gt, pred, roc\n",
    "```\n",
    "\n",
    "... and use it to visualize the receiver operator curve (ROC). Furthermore, this function returns the test images, the groundtruths, the predictions, and the ROC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fnc_blood_smears import plot_roc\n",
    "\n",
    "_ = plot_roc(classifier=dnn_classifier, dataset=test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network with a Dense Top\n",
    "\n",
    "We'll now build a convolutional neural network a dense top using `deeplay` ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import AdaptiveAvgPool2d, MaxPool2d\n",
    "\n",
    "cnn = dl.Sequential(\n",
    "    dl.ConvolutionalNeuralNetwork(\n",
    "        in_channels=3, \n",
    "        hidden_channels=[32, 32, 64], \n",
    "        out_channels=64,\n",
    "    ),\n",
    "    dl.Layer(AdaptiveAvgPool2d, output_size=1),\n",
    "    dl.MultiLayerPerceptron(\n",
    "        in_features=64, \n",
    "        hidden_features=[], \n",
    "        out_features=1,\n",
    "        out_activation=Sigmoid,\n",
    "    )\n",
    ")\n",
    "cnn[0].blocks[2].pool.configure(MaxPool2d, kernel_size=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... define a classifier using the CNN ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_classifier_template = dl.BinaryClassifier(\n",
    "    model=cnn, \n",
    "    optimizer=dl.RMSprop(lr=.001),\n",
    ")\n",
    "\n",
    "cnn_classifier = cnn_classifier_template.create()\n",
    "\n",
    "print(cnn_classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... train it ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_trainer = dl.Trainer(max_epochs=20, accelerator=\"auto\")\n",
    "\n",
    "cnn_trainer.fit(cnn_classifier, train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... evaluate the performance over the test set ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = cnn_trainer.test(cnn_classifier, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and display the ROC curve with the AUC value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, gt, pred, _ = plot_AUROC(classifier=cnn_classifier, dataset=test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Failure analysis\n",
    "\n",
    "We'll use a function to visualize some of the wrongly classified cells, looking for common patterns.\n",
    "\n",
    "```python\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_failures(images, gt, pred, threshold=.5, plot_num=5):\n",
    "    from matplotlib import pyplot as plt \n",
    "    from numpy import array, squeeze\n",
    "    \n",
    "    pred = array(pred).squeeze()\n",
    "    gt = array(gt).squeeze()\n",
    "    images = array(images)\n",
    "\n",
    "    false_positives = (pred > threshold) & (gt == 0)\n",
    "    false_positives_images = images[false_positives]\n",
    "\n",
    "    false_negatives = (pred < threshold) & (gt == 1)\n",
    "    false_negatives_images = images[false_negatives]\n",
    "\n",
    "    plt.figure(figsize=(plot_num * 2, 5))\n",
    "    for i in range(plot_num):\n",
    "        # false positives\n",
    "        plt.subplot(2, plot_num, i + 1)\n",
    "        plt.imshow(false_positives_images[i].transpose(1, 2, 0))\n",
    "        if i == 0:\n",
    "            plt.title(\"False positives\", fontsize=16, y=1.1)\n",
    "\n",
    "        # false negatives\n",
    "        plt.subplot(2, plot_num, plot_num + i + 1)\n",
    "        plt.imshow(false_negatives_images[i].transpose(1, 2, 0))\n",
    "        if i == 0:\n",
    "            plt.title(\"False negatives\", fontsize=16, y=1.1)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from fnc_blood_smears import plot_failure\n",
    "\n",
    "plot_failures(images, gt, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Access Filters\n",
    "\n",
    "We can access the filters used by the neural network. The follwoing code accesses the 32nd filter's weights in the first convolutional layer of the CNN. It navigates through the model's first module (`model[0]`), selects the initial block (`blocks[0]`), and then targets the layer's weights (`layer.weight[31]`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter = cnn_classifier.model[0].blocks[0].layer.weight[31]\n",
    "\n",
    "print(filter)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def plot_filters(filters, cols=8):\n",
    "    from matplotlib import pyplot as plt\n",
    "\n",
    "    rows = -(filters.shape[0] // -cols)\n",
    "\n",
    "    fig, axs = plt.subplots(rows, cols, figsize=(2 * cols, 2 * rows))\n",
    "    for i, ax in enumerate(axs.ravel()):\n",
    "        ax.axis('off')\n",
    "        if i < filters.shape[0]:\n",
    "            p  = filters[i].permute(1, 2, 0).clone().detach().numpy()\n",
    "            p -= p.min(axis=(0, 1), keepdims=True)\n",
    "            p /= p.max(axis=(0, 1), keepdims=True)\n",
    "            ax.imshow(p)\n",
    "            ax.set_title(i)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#from fnc_blood_smears import plot_filters_activations\n",
    "\n",
    "plot_filters(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activations and Grad-CAM\n",
    "\n",
    "To visualize the network features, we'll use `hooks`, functions that allows us to access the information that the model sees during forward and backward passes, such as activations and gradients, respectively. We'll define them as context manager classes, so that we can use them with the `with` statement:\n",
    "\n",
    "```python\n",
    "\n",
    "```\n",
    "\n",
    "```python\n",
    "\n",
    "```\n",
    "\n",
    "The use of context managers (`with` statements) for hooks is a good practice. It ensures that the hooks are properly set up and removed when they are no longer needed, preventing potential side effects or memory leaks. This is particularly important in complex neural network models where maintaining computational efficiency and correctness is crucial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll randomly pick the image of an infected smear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch import cat\n",
    "\n",
    "# Convert the ground truth labels (gt) to a categorical format and then to a list.\n",
    "gt_as_list = (cat(gt) == 1).tolist()\n",
    "\n",
    "# Find the indices where the ground truth labels are equal to 1.\n",
    "indices = np.where(gt_as_list)[0]\n",
    "\n",
    "# Randomly select one index from those indices.\n",
    "index = np.random.choice(indices, 1)[0]\n",
    "\n",
    "# Retrieve the image corresponding to the selected index.\n",
    "image = images[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activations and gradients at a specific layer can be obtained from the forward and backward pass, respectively ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class fwd_hook():\n",
    "    # Initializer method for the fwd_hook class.\n",
    "    def __init__(self, model):\n",
    "        # Register a forward hook to the provided model. The hook is set to the hook_func method.\n",
    "        # This hook will be called every time the forward method of the model is executed.\n",
    "        self.hook = model.register_forward_hook(self.hook_func)\n",
    "\n",
    "    # The forward hook function.\n",
    "    def hook_func(self, model, i, o):\n",
    "        # This function is called during the forward pass of the model.\n",
    "        # 'model' is the model to which the hook is attached.\n",
    "        # 'i' is the input to the model's layer.\n",
    "        # 'o' is the output from the model's layer.\n",
    "\n",
    "        # Print a message indicating the forward hook is running.\n",
    "        print(\"Forward hook running ...\") \n",
    "\n",
    "        # Detach the output 'o' from the current computation graph and clone it.\n",
    "        # This prevents any modifications to 'o' from affecting the original tensor,\n",
    "        # and avoids potential issues with backpropagation.\n",
    "        self.stored = o.detach().clone()\n",
    "\n",
    "        # Print the size of the activations (the output tensor).\n",
    "        print(f\"Activations size: {self.stored.size()}\")\n",
    "\n",
    "    # Enter method for the context manager.\n",
    "    def __enter__(self, *args): \n",
    "        # When entering the context (using the 'with' statement), return this object.\n",
    "        return self\n",
    "    \n",
    "    # Exit method for the context manager.\n",
    "    def __exit__(self, *args): \n",
    "        # When exiting the context, remove the forward hook from the model.\n",
    "        # This is important for resource management and to ensure that the hook\n",
    "        # does not remain active beyond its intended scope.\n",
    "        self.hook.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class bwd_hook():\n",
    "    # Initializer method for the bwd_hook class.\n",
    "    def __init__(self, model):\n",
    "        # Register a full backward hook to the provided model. The hook is set to the hook_func method.\n",
    "        # This hook will be called every time the backward pass of the model is executed.\n",
    "        self.hook = model.register_full_backward_hook(self.hook_func)\n",
    "\n",
    "    # The backward hook function.\n",
    "    def hook_func(self, model, gi, go):\n",
    "        # This function is called during the backward pass of the model.\n",
    "        # 'model' is the model to which the hook is attached.\n",
    "        # 'gi' represents the gradients with respect to the input.\n",
    "        # 'go' represents the gradients with respect to the output.\n",
    "\n",
    "        # Print a message indicating the backward hook is running.\n",
    "        print(\"Backward hook running ...\")\n",
    "\n",
    "        # Detach the first output gradient 'go[0]' from the current computation graph and clone it.\n",
    "        # This prevents any modifications to 'go[0]' from affecting the original tensor,\n",
    "        # and avoids potential issues with backpropagation.\n",
    "        # Note: go[0] is used because go is a tuple of gradients, one for each output.\n",
    "        self.stored = go[0].detach().clone()\n",
    "\n",
    "        # Print the size of the stored gradients.\n",
    "        print(f\"Gradients size: {self.stored.size()}\")\n",
    "\n",
    "    # Enter method for the context manager.\n",
    "    def __enter__(self, *args): \n",
    "        # When entering the context (using the 'with' statement), return this object.\n",
    "        # This enables the use of the 'with' statement with instances of this class.\n",
    "        return self\n",
    "    \n",
    "    # Exit method for the context manager.\n",
    "    def __exit__(self, *args): \n",
    "        # When exiting the context, remove the backward hook from the model.\n",
    "        # This is important to ensure that the hook is only active during the intended scope\n",
    "        # and to release any resources associated with the hook.\n",
    "        self.hook.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from fnc_blood_smears import fwd_hook, bwd_hook\n",
    "\n",
    "layer = cnn_classifier.model[0].blocks[3].layer\n",
    "\n",
    "# Using backward and forward hooks on the specified layer\n",
    "with bwd_hook(layer) as bh, fwd_hook(layer) as fh:\n",
    "    # Forward pass through the model with the given image\n",
    "    pred = cnn_classifier.model(image.unsqueeze(0))\n",
    "    # Executing the backward pass for backpropagation\n",
    "    pred.backward()\n",
    "\n",
    "activations = fh.stored\n",
    "gradients = bh.stored"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... we can plot the activations ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_activations(activations, cols=8):\n",
    "    from matplotlib import pyplot as plt\n",
    "\n",
    "    rows = -(activations.shape[0] // -cols)\n",
    "\n",
    "    fig, axs = plt.subplots(rows, cols, figsize=(2 * cols, 2 * rows))\n",
    "    for i, ax in enumerate(axs.ravel()):\n",
    "        ax.axis('off')\n",
    "        if i < activations.shape[0]:\n",
    "            ax.imshow(activations[i].numpy())\n",
    "            ax.set_title(i)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from fnc_blood_smears import plot_activations\n",
    "\n",
    "plot_activations(activations[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... or combine gradients and activations to calculate Grad-CAM and inspect on which part of an image the CNN focuses on to predict its outputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.image as mpimg\n",
    "\n",
    "image_hr = mpimg.imread(dataset.imgs[subset.indices[test.indices[index]]][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gradcam(image, grad_cam):\n",
    "    from matplotlib import pyplot as plt\n",
    "    import skimage\n",
    "\n",
    "    grad_cam = skimage.transform.resize(grad_cam, image.shape, order=2)\n",
    "    grad_cam = skimage.exposure.rescale_intensity(grad_cam, out_range=(0.25, 1))\n",
    "\n",
    "    plt.figure(figsize=(12, 5)) \n",
    "\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(image, interpolation=\"bilinear\")\n",
    "    plt.title(\"Original image\")\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.imshow(grad_cam.mean(axis=-1), interpolation=\"bilinear\")\n",
    "    plt.title(\"Grad-CAM\")\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.imshow(image * grad_cam)\n",
    "    plt.title(\"Overlay\")\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import relu\n",
    "\n",
    "pooled_gradients = gradients[0].mean(dim=[1,2], keepdim=True)\n",
    "grad_cam = relu((pooled_gradients * activations[0]).sum(0)).detach().numpy()\n",
    "\n",
    "#from fnc_blood_smears import plot_gradcam\n",
    "\n",
    "plot_gradcam(image_hr, grad_cam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dt_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
