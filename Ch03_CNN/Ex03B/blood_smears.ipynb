{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blood Smears Classification\n",
    "\n",
    "We'll compare the performance of a dense neural network and of a convolutional neural network with a dense top for the classification of blood smears in a dataser of blood smears containing blood cells with and without malaria. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "We'll load and uncompress the single cell dataset from \n",
    "https://lhncbc.nlm.nih.gov/LHC-research/LHC-projects/image-processing/malaria-datasheet.html\n",
    "\n",
    "The Malaria dataset was published in S. Rajaraman, S. K. Antani, M. Poost- chi, K. Silamut, Md A. Hossain, R. J. Maude, S. Jaeger, and G. R. Thoma. Pre-trained convolutional neural net- works as feature extractors toward improved malaria parasite detection in thin blood smear images. PeerJ, 6:e4568, 2018.\n",
    "It is available at https://data.lhncbc.nlm.nih.gov/public/Malaria/cell_images.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torchvision.datasets.utils import download_url, _extract_zip\n",
    "\n",
    "dataset_path = os.path.join(\".\", \"blood_smears_dataset\")\n",
    "if not os.path.exists(dataset_path):\n",
    "    url = \"https://data.lhncbc.nlm.nih.gov/public/Malaria/cell_images.zip\"\n",
    "    download_url(url, \".\")\n",
    "    _extract_zip(\"cell_images.zip\", dataset_path, None)\n",
    "    os.remove(\"cell_images.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll define the path to the directories containing the `Infected` and `Parasitized` images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "base_dir = os.path.join(dataset_path, \"cell_images\")\n",
    "uninfected_files = glob.glob(os.path.join(base_dir, \"Uninfected\", \"*.png\"))\n",
    "parasitized_files = glob.glob(os.path.join(base_dir, \"Parasitized\", \"*.png\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Data\n",
    "\n",
    "We'll then visualize some of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_blood_smears(title, files):\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    fig, axs = plt.subplots(3, 3, figsize=(8, 8))\n",
    "    for i, ax in enumerate(axs.ravel()):\n",
    "        image = plt.imread(files[i])\n",
    "        ax.imshow(image)\n",
    "        \n",
    "    fig.suptitle(title, fontsize=16)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from blood_smears import plot_blood_smears\n",
    "\n",
    "plot_blood_smears(\"Uninfected\", uninfected_files)\n",
    "plot_blood_smears(\"Parasitized\", parasitized_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "We'll define a pipeline to resize the images to 28 by 28 pixels and convert them to PyTorch tensors (note that `ToTensor()` also normalizes their values between 0 and 1) ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import Compose, Resize, ToTensor\n",
    "\n",
    "pipeline = Compose([Resize((28, 28)), ToTensor()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... we set up a dataset where images are loaded from a structured directory (`base_dir`) ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import Lambda\n",
    "\n",
    "dataset = ImageFolder(base_dir, transform=pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Default mapping: {dataset.class_to_idx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.class_to_idx = {'Parasitized': 1, 'Uninfected': 0}\n",
    "dataset.targets = [abs(target - 1) for target in dataset.targets]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... creates a subset the full dataset and split it into `train` and `test` sets ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import randperm\n",
    "from torch.utils.data import random_split, Subset\n",
    "\n",
    "images_num = 5000\n",
    "images_idx = randperm(len(dataset))[:images_num]\n",
    "images = Subset(dataset, images_idx)\n",
    "\n",
    "train_size = int(0.8 * len(images))\n",
    "test_size = len(images) - train_size\n",
    "train, test = random_split(images, [train_size, test_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and define the dataloaders for both sets. For the training, we'll set `batch_size = 32`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test, batch_size=124, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fully-connected Neural Network\n",
    "\n",
    "We'll define a Fully-connected Neural Network (FCNN) using `deeplay`. The FCNN has 2 layers with 128 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deeplay as dl\n",
    "\n",
    "FCNN = dl.MultiLayerPerceptron(in_features = 28 * 28 * 3,\n",
    "                                hidden_features = [128, 128],\n",
    "                                out_features = 1,\n",
    "                                out_activation = torch.nn.Sigmoid,\n",
    ")\n",
    "FCNN.blocks.activation.configure(torch.nn.Sigmoid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll define a classifier based on the FCNN architecture, including loss function, evaluation metrics and othe hyperparameters ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchmetrics as tm\n",
    "\n",
    "FCNN_classifier_template = dl.BinaryClassifier(\n",
    "        model=FCNN,\n",
    "        optimizer=dl.RMSprop(lr=.001),\n",
    "        )\n",
    "\n",
    "FCNN_classifier = FCNN_classifier_template.create()\n",
    "print(FCNN_classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and a trainer including other hyperparameters ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FCNN_trainer = dl.Trainer(\n",
    "    max_epochs=20, # How many times to run through the entire dataset\n",
    "    accelerator=\"auto\", # Use GPU if available\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start the training and visualize the evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FCNN_trainer.fit(FCNN_classifier, train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll calculate the performance over the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = FCNN_trainer.test(FCNN_classifier, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC curve and AUC\n",
    "We'll use the function `plot_ROC_AUC` to get the ground truth and predictions for all the images in the test set, calculate the ROC and AUC, and visualize the results.\n",
    "```python\n",
    "def plot_ROC_AUC(classifier, dataset):\n",
    "    from torch import tensor, stack\n",
    "    from sklearn.metrics import roc_curve, auc\n",
    "    from matplotlib import pyplot as plt\n",
    "\n",
    "    im, gt = zip(*dataset)\n",
    "    pred = classifier(tensor(stack(im))).tolist()\n",
    "    # calculate the ROC curve\n",
    "    fpr, tpr, thresholds = roc_curve(gt, pred, pos_label=1) \n",
    "    roc_auc = auc(fpr, tpr) \n",
    "\n",
    "    # plot the ROC curve\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.plot(fpr, tpr, label=f\"ROC curve (AUC = {roc_auc:.3f})\", linewidth=2)\n",
    "    plt.title(\"ROC curve\")\n",
    "    plt.xlabel(\"False positive rate\")\n",
    "    plt.ylabel(\"True positive rate\")\n",
    "    plt.axis(\"square\")\n",
    "    plt.xlim(0, 1)\n",
    "    plt.ylim(0, 1)\n",
    "    plt.legend(loc = 'center right')\n",
    "    plt.show()\n",
    "\n",
    "    return im, gt, pred, roc_auc\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from blood_smears import plot_ROC_AUC\n",
    "_,_,_,_ = plot_ROC_AUC(classifier = FCNN_classifier, dataset=test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network with a dense top\n",
    "We'll now build a convolutional neural network (CNN) with a FCNN at the end ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN = dl.Sequential(\n",
    "    dl.ConvolutionalNeuralNetwork(in_channels = 3, hidden_channels = [32, 32, 64], out_channels = 64),\n",
    "    dl.Layer(torch.nn.AdaptiveAvgPool2d,output_size = 1),\n",
    "    dl.MultiLayerPerceptron(in_features = 64, hidden_features = [], out_features = 1,out_activation = torch.nn.Sigmoid)\n",
    ")\n",
    "CNN[0].blocks[2].pool.configure(torch.nn.MaxPool2d, kernel_size = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... define a classifier using the CNN ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN_classifier_template = dl.BinaryClassifier(\n",
    "    model=CNN, \n",
    "    optimizer=dl.RMSprop(lr=.001),\n",
    ")\n",
    "\n",
    "CNN_classifier = CNN_classifier_template.create()\n",
    "print(CNN_classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... train it ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN_trainer = dl.Trainer(\n",
    "    max_epochs=20, # How many times to run through the entire dataset\n",
    "    accelerator=\"auto\", # Use GPU if available\n",
    ")\n",
    "\n",
    "CNN_trainer.fit(CNN_classifier, train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... evaluate the performance over the test set ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = CNN_trainer.test(CNN_classifier, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and display the ROC curve with the AUC value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, gt, pred, _ = plot_ROC_AUC(classifier=CNN_classifier, dataset=test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Failure analysis\n",
    "We'll use a function to visualize some of the wrongly classified cells, looking for common patterns.\n",
    "```python\n",
    "def plot_failure(images, gt, pred, threshold = 0.5, num_of_plots = 5):\n",
    "    from matplotlib import pyplot as plt \n",
    "    from numpy import array, squeeze   \n",
    "    \n",
    "    pred = array(pred).squeeze()\n",
    "    gt = array(gt).squeeze()\n",
    "    images = array(images)\n",
    "\n",
    "    pred_class = pred > threshold\n",
    "\n",
    "    false_positives = (pred_class == 1) & (gt == 0)\n",
    "    false_positives_images = images[false_positives]\n",
    "\n",
    "    false_negatives = (pred_class == 0) & (gt == 1)\n",
    "    false_negatives_images = images[false_negatives]\n",
    "\n",
    "    plt.figure(figsize=(num_of_plots*2, 5))\n",
    "    for i in range(num_of_plots):\n",
    "\n",
    "        # false positives\n",
    "        plt.subplot(2, num_of_plots, i + 1)\n",
    "        plt.imshow(false_positives_images[i].transpose(1, 2, 0))\n",
    "        if i == 0:\n",
    "            plt.title(\"False positives\", fontsize=16, y=1.1)\n",
    "\n",
    "        # false negatives\n",
    "        plt.subplot(2, num_of_plots, i + num_of_plots + 1)\n",
    "        plt.imshow(false_negatives_images[i].transpose(1, 2, 0))\n",
    "        if i == 0:\n",
    "            plt.title(\"False negatives\", fontsize=16, y=1.1)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from blood_smears import plot_failure\n",
    "plot_failure(images=images, gt=gt, pred=pred, threshold = 0.5, num_of_plots = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filters\n",
    "We can access and visualize the filters used by the network at a specific layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = CNN_classifier.model[0].input_block.layer.weight\n",
    "w = weights.clone().detach()\n",
    "\n",
    "from blood_smears import plot_filters_activations\n",
    "plot_filters_activations(input = w, n_rows=4, label = 'Filters', normalize = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activations and Grad-CAM\n",
    "To visualize the network feautures, we'll use `hooks`, functions that allows us to access the information that the model sees during forward and backward passes, such as activations and gradients, respectively. We'll define them as context manager classes, so that we can use them with the `with` statement:\n",
    "```python\n",
    "class fwd_hook():\n",
    "    def __init__(self, m):\n",
    "        self.hook = m.register_forward_hook(self.hook_func)   \n",
    "    def hook_func(self, m, i, o):\n",
    "        print('Forward hook running...') \n",
    "        self.stored = o.detach().clone()\n",
    "        print(f'Activations size: {self.stored.size()}')\n",
    "    def __enter__(self, *args): \n",
    "        return self\n",
    "    def __exit__(self, *args): \n",
    "        self.hook.remove()\n",
    "\n",
    "class bwd_hook():\n",
    "    def __init__(self, m):\n",
    "        self.hook = m.register_full_backward_hook(self.hook_func)\n",
    "    def hook_func(self, m, gi, go):\n",
    "        print('Backward hook running...')\n",
    "        self.stored = go[0].detach().clone()\n",
    "        print(f'Gradients size: {self.stored.size()}')\n",
    "    def __enter__(self, *args): \n",
    "        return self\n",
    "    def __exit__(self, *args): \n",
    "        self.hook.remove()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll randomly pick the image of an infected smear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "ind_infect = np.where((torch.cat(gt)==1).tolist())[0]\n",
    "ind=np.random.choice(ind_infect,1)[0]\n",
    "\n",
    "test_image = images[ind]\n",
    "test_image_hr=mpimg.imread(dataset.imgs[subset.indices[test.indices[ind]]][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activations and gradients at a specific layer can be obtained from the forward and backward pass, respectively ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from blood_smears import fwd_hook, bwd_hook\n",
    "\n",
    "test_layer = CNN_classifier.model[0].blocks[3].layer\n",
    "\n",
    "with bwd_hook(test_layer) as bh:\n",
    "    with fwd_hook(test_layer) as fh:\n",
    "        out = CNN_classifier.model(test_image.unsqueeze(0)).backward()\n",
    "activations = fh.stored\n",
    "gradients = bh.stored"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... we can plot the activations ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_filters_activations(input = activations.permute(1,0,2,3),n_rows=8,label = 'Feature maps', normalize = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... or combine gradients and activations to calculate Grad-CAM and inspect on which part of an image the CNN focuses on to predict its outputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pooled_grad = gradients[0].mean(dim=[1,2], keepdim = True)\n",
    "grad_cam = torch.nn.functional.relu((pooled_grad*activations[0]).sum(0)).detach().numpy()\n",
    "\n",
    "from blood_smears import plot_gradcam\n",
    "plot_gradcam(grad_cam, test_image_hr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dt_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
