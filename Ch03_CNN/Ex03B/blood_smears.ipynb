{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blood Smear Classification\n",
    "\n",
    "We'll compare the performance of a dense neural network and of a convolutional neural network with a dense top for the classification of blood smears in a dataset of blood smears containing blood cells with and without malaria. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Malaria Dataset\n",
    "\n",
    "We load and uncompress the malaria dataset available at https://data.lhncbc.nlm.nih.gov/public/Malaria/cell_images.zip.\n",
    "\n",
    "This dataset was origianlly published in S. Rajaraman, S. K. Antani, M. Poost- chi, K. Silamut, Md A. Hossain, R. J. Maude, S. Jaeger, and G. R. Thoma. _Pre-trained convolutional neural networks as feature extractors toward improved malaria parasite detection in thin blood smear images._ PeerJ 6, e4568, 2018."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torchvision.datasets.utils import download_url, _extract_zip\n",
    "\n",
    "dataset_path = os.path.join(\".\", \"blood_smears_dataset\")\n",
    "if not os.path.exists(dataset_path):\n",
    "    url = \"https://data.lhncbc.nlm.nih.gov/public/Malaria/cell_images.zip\"\n",
    "    download_url(url, \".\")\n",
    "    _extract_zip(\"cell_images.zip\", dataset_path, None)\n",
    "    os.remove(\"cell_images.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Data\n",
    "\n",
    "We load the data in a dataset for further processing using the `ImageFolder` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "base_dir = os.path.join(dataset_path, \"cell_images\")\n",
    "dataset = ImageFolder(base_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implement the function `plot_blood_smears`, save it in `fnc_blood_smears.py` ...\n",
    "\n",
    "```python\n",
    "def plot_blood_smears(dataset):\n",
    "    import matplotlib.pyplot as plt\n",
    "    from numpy.random import randint\n",
    "    import torch\n",
    "\n",
    "    fig, axs = plt.subplots(3, 6, figsize=(16, 8))\n",
    "    for ax in axs.ravel():\n",
    "        image, label = dataset[randint(0, len(dataset))]\n",
    "        if isinstance(image, torch.Tensor):\n",
    "            image = image.numpy().transpose(1, 2, 0)\n",
    "            \n",
    "        ax.imshow(image)\n",
    "        ax.set_title(\"Uninfected (1)\" if label == 1 else \"Infected (0)\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "```\n",
    "\n",
    "... and use it to visualize some of the blood smears."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fnc_blood_smears import plot_blood_smears\n",
    "\n",
    "plot_blood_smears(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform Images and Labels\n",
    "\n",
    "We define a transformation to resize the images to 28 by 28 pixels and convert them to PyTorch tensors (note that `ToTensor()` also normalizes their values between 0 and 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import Compose, Resize, ToTensor\n",
    "\n",
    "image_transform = Compose([Resize((28, 28)), ToTensor()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assign binary labels to the images, coresponding to their class. We define a transformation to make the target label equal to 0 for the uninfected cells and to 1 for the parasitized ones, which is standard for medical statistical analysis, achieved through `abs(1 - target)`. This step is followed by converting it into a PyTorch tensor using `tensor()`, ensuring the data is in the correct format for PyTorch operations. Next, we convert the tensor into a floating-point type with `float()`, enhancing its compatibility with PyTorch's computational requirements. Finally, we add a new dimension to the tensor using `unsqueeze(-1)`, preparing it for batch processing in neural network models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import tensor\n",
    "\n",
    "def label_transform(target):\n",
    "    return tensor(abs(1 - target)).float().unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then set up a dataset where images are loaded from a structured directory (`base_dir`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ImageFolder(base_dir, \n",
    "                      transform=image_transform, \n",
    "                      target_transform=label_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now plot the transformed images and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_blood_smears(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split the dataset, composed by ca. 27,000 cell images, into `train` (80%) and `test` (20%) sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "train, test = random_split(dataset, [0.8, 0.2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we define the dataloaders for both sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dense Neural Network\n",
    "\n",
    "We define a dense neural network (DNN) using `deeplay` with 2 layers with 128 neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deeplay as dl\n",
    "from torch.nn import Sigmoid\n",
    "\n",
    "dnn = dl.MultiLayerPerceptron(\n",
    "    in_features=28 * 28 * 3,\n",
    "    hidden_features=[128, 128],\n",
    "    out_features=1,\n",
    "    out_activation=Sigmoid,\n",
    ")\n",
    "\n",
    "print(dnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then define a classifier using the DNN ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchmetrics as tm\n",
    "\n",
    "dnn_classifier_template = dl.BinaryClassifier(model=dnn, optimizer=dl.RMSprop(lr=0.001))\n",
    "\n",
    "dnn_classifier = dnn_classifier_template.create()\n",
    "\n",
    "print(dnn_classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and a trainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn_trainer = dl.Trainer(max_epochs=5, accelerator=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train\n",
    "\n",
    "We train the DNN classifier ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn_trainer.fit(dnn_classifier, train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and evaluate its performance over the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = dnn_trainer.test(dnn_classifier, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot ROC Curve\n",
    "\n",
    "We calculate and visualize the receiver operator curve (ROC) and the value of the corresponding area under the curve (AUC)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchmetrics as tm\n",
    "\n",
    "roc_dnn = tm.ROC(task=\"binary\")\n",
    "\n",
    "for x, y in test_loader:\n",
    "    roc_dnn.update(dnn_classifier(x), y.long())\n",
    "\n",
    "fig, ax = roc_dnn.plot(score=True)\n",
    "ax.grid(False)\n",
    "ax.axis(\"square\")\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.legend(loc=\"center right\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network with a Dense Top\n",
    "\n",
    "We now build a convolutional neural network (CNN) with a dense top using `deeplay`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import AdaptiveAvgPool2d, MaxPool2d\n",
    "\n",
    "cnn = dl.Sequential(\n",
    "    dl.ConvolutionalNeuralNetwork(\n",
    "        in_channels=3,\n",
    "        hidden_channels=[32, 32, 64],\n",
    "        out_channels=64,\n",
    "    ),\n",
    "    dl.Layer(AdaptiveAvgPool2d, output_size=1),\n",
    "    dl.MultiLayerPerceptron(\n",
    "        in_features=64,\n",
    "        hidden_features=[],\n",
    "        out_features=1,\n",
    "        out_activation=Sigmoid,\n",
    "    ),\n",
    ")\n",
    "cnn[0].blocks[2].pool.configure(MaxPool2d, kernel_size=2)\n",
    "\n",
    "print(cnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then define a classifier using the CNN ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_classifier_template = dl.BinaryClassifier(\n",
    "    model=cnn,\n",
    "    optimizer=dl.RMSprop(lr=0.001),\n",
    ")\n",
    "\n",
    "cnn_classifier = cnn_classifier_template.create()\n",
    "\n",
    "print(cnn_classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...  and a trainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_trainer = dl.Trainer(max_epochs=5, accelerator=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train\n",
    "\n",
    "We then train the CNN classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_trainer.fit(cnn_classifier, train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and evaluate its performance over the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = cnn_trainer.test(cnn_classifier, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot ROC Curve\n",
    "\n",
    "We finally display the ROC curve with the AUC value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_cnn = tm.ROC(task=\"binary\")\n",
    "\n",
    "for x, y in test_loader:\n",
    "    roc_cnn.update(cnn_classifier(x), y.long())\n",
    "\n",
    "fig, ax = roc_cnn.plot(score=True)\n",
    "ax.grid(False)\n",
    "ax.axis(\"square\")\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.legend(loc=\"center right\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Failure analysis\n",
    "\n",
    "We now write the `plot_failures` function, save it in `fnc_blood_smears.py` ...\n",
    "\n",
    "```python\n",
    "def plot_failures(images, gt, pred, threshold=.5, plot_num=5):\n",
    "    from matplotlib import pyplot as plt \n",
    "    from numpy import array, squeeze\n",
    "    \n",
    "    pred = array(pred).squeeze()\n",
    "    gt = array(gt).squeeze()\n",
    "    images = array(images)\n",
    "\n",
    "    false_positives = (pred > threshold) & (gt == 0)\n",
    "    false_positives_images = images[false_positives]\n",
    "\n",
    "    false_negatives = (pred < threshold) & (gt == 1)\n",
    "    false_negatives_images = images[false_negatives]\n",
    "\n",
    "    plt.figure(figsize=(plot_num * 2, 5))\n",
    "    for i in range(plot_num):\n",
    "        # false positives\n",
    "        plt.subplot(2, plot_num, i + 1)\n",
    "        plt.imshow(false_positives_images[i].transpose(1, 2, 0))\n",
    "        if i == 0:\n",
    "            plt.title(\"False positives\", fontsize=16, y=1.1)\n",
    "\n",
    "        # false negatives\n",
    "        plt.subplot(2, plot_num, plot_num + i + 1)\n",
    "        plt.imshow(false_negatives_images[i].transpose(1, 2, 0))\n",
    "        if i == 0:\n",
    "            plt.title(\"False negatives\", fontsize=16, y=1.1)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "```\n",
    "\n",
    "... and visualize some of the wrongly classified cells, looking for common patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fnc_blood_smears import plot_failures\n",
    "from torch import stack\n",
    "\n",
    "images, gt = zip(*test)\n",
    "pred = cnn_classifier(stack(images)).tolist()\n",
    "\n",
    "plot_failures(images, gt, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Access Filters\n",
    "\n",
    "We can access value of the filters used by the neural network. The follwoing code accesses the 32nd filter's weights in the first convolutional layer of the CNN. It navigates through the model's first module (`model[0]`), selects the initial block (`blocks[0]`), and then targets the layer's weights (`layer.weight[31]`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter = cnn_classifier.model[0].blocks[0].layer.weight[31]\n",
    "\n",
    "print(filter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activations and Grad-CAM\n",
    "\n",
    "We pick the image of an infected smear to then check the activations it produces on the last convolutional layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "im_ind = 0  # Select the index of an image\n",
    "\n",
    "image_hr = Image.open(\n",
    "    dataset.samples[im_ind][0]\n",
    ")  # Retrieve the original image corresponding to the selected index.\n",
    "image = input_transform(image_hr)  # Apply the same transformation used in preprocessing\n",
    "\n",
    "gt_label = abs(1 - dataset.imgs[im_ind][1])\n",
    "print(gt_label)  # Check that it corresponds to a Parasitized image (label = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize the network features, we can use `hooks`. These are functionalities that allows us to access the information that the model sees during forward and backward passes, such as activations and gradients. \n",
    "\n",
    "We define them as context manager classes, so that we can use them with the `with` statement. This ensures that the hooks are properly set up and removed when they are no longer needed, preventing potential side effects or memory leaks. This is particularly important in complex neural network models where maintaining computational efficiency and correctness is crucial.\n",
    "\n",
    "Activations at a specific layer can be obtained from the forward pass using the `fwd_hook` class:\n",
    "\n",
    "* The `__init__(self, layer)` method registers a forward hook to the provided model at a specific layer. The hook is set to the `hook_func()` method. This hook will be called every time the forward method of the model is executed.\n",
    "\n",
    "* The `hook_func(self, layer, i, o)` method is called during the forward pass of the model: `layer` is the layer to which the hook is attached; `input` is the input to the model's layer; and `o` is the output from the model's layer. The output `o` is detached from the current computation graph and cloned to prevent any modifications to `o` from affecting the original tensor, and avoids potential issues with backpropagation. Note: `o[0]` is used because `o` is a tuple of activations, one for each image.\n",
    "\n",
    "* The `__enter__(self, *args)` method returns this object when entering the context (using the `with` statement).\n",
    "\n",
    "* The `__exit__(self, *args)` method removes the forward hook from the model when exiting the context. This is important for resource management and to ensure that the hook does not remain active beyond its intended scope.\n",
    "\n",
    "```python\n",
    "class fwd_hook():\n",
    "    def __init__(self, layer):\n",
    "        self.hook = layer.register_forward_hook(self.hook_func)\n",
    "\n",
    "    def hook_func(self, layer, i, o):\n",
    "        print(\"Forward hook running ...\") \n",
    "        self.activations = o.detach().clone()\n",
    "        print(f\"Activations size: {self.activations.size()}\")\n",
    "\n",
    "    def __enter__(self, *args): \n",
    "        return self\n",
    "    \n",
    "    def __exit__(self, *args): \n",
    "        self.hook.remove()\n",
    "```\n",
    "\n",
    "Gradients at a specific layer can be obtained from the backward pass using the `bwd_hook` class:\n",
    "\n",
    "* The `__init__(self, layer)` method registers a full backward hook to the provided model at a specific layer. The hook is set to the `hook_func`` method. This hook will be called every time the backward pass of the model is executed.\n",
    "\n",
    "* The `hook_func(self, layer, gi, go)` function is called during the backward pass of the model: `layer` is the layer to which the hook is attached; `gi` represents the gradients with respect to the input; and `go` represents the gradients with respect to the output. The first output gradient `go[0]` is detached from the current computation graph and cloned. This prevents any modifications to `go[0]` from affecting the original tensor, and avoids potential issues with backpropagation. Note: `go[0]` is used because `go` is a tuple of gradients, one for each output (even though in this case we only have one input).\n",
    "\n",
    "* The `__enter__(self, *args)` method returns this object when entering the context (using the `with` statement).\n",
    "\n",
    "* The `__exit__(self, *args)` method removes the backward hook from the model when exiting the context. This is important to ensure that the hook is only active during the intended scope and to release any resources associated with the hook.\n",
    "\n",
    "```python\n",
    "class bwd_hook():\n",
    "    def __init__(self, layer):\n",
    "        self.hook = layer.register_full_backward_hook(self.hook_func)\n",
    "\n",
    "    def hook_func(self, layer, gi, go):\n",
    "        print(\"Backward hook running ...\")\n",
    "        self.gradients = go[0].detach().clone()\n",
    "        print(f\"Gradients size: {self.gradients.size()}\")\n",
    "\n",
    "    def __enter__(self, *args): \n",
    "        return self\n",
    "    \n",
    "    def __exit__(self, *args): \n",
    "        self.hook.remove()\n",
    "```\n",
    "\n",
    "We add both classes to `fnc_blood_smears.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fnc_blood_smears import fwd_hook, bwd_hook\n",
    "\n",
    "layer = cnn_classifier.model[0].blocks[3].layer\n",
    "\n",
    "with bwd_hook(layer) as bh, fwd_hook(layer) as fh:\n",
    "    pred = cnn_classifier.model(\n",
    "        image.unsqueeze(0)\n",
    "    )  # Forward pass through the model with the given image\n",
    "    pred.backward()  # Executing the backward pass for backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now plot the activations in the last convolutional layer using the `plot_activations()` function ...\n",
    "\n",
    "```python\n",
    "def plot_activations(activations, cols=8):\n",
    "    from matplotlib import pyplot as plt\n",
    "\n",
    "    rows = -(activations.shape[0] // -cols)\n",
    "\n",
    "    fig, axs = plt.subplots(rows, cols, figsize=(2 * cols, 2 * rows))\n",
    "    for i, ax in enumerate(axs.ravel()):\n",
    "        ax.axis('off')\n",
    "        if i < activations.shape[0]:\n",
    "            ax.imshow(activations[i].numpy())\n",
    "            ax.set_title(i)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "```\n",
    "\n",
    "... which is added to `fnc_blood_smears.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fnc_blood_smears import plot_activations\n",
    "\n",
    "plot_activations(fh.activations[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also combine activations and gradients to calculate Grad-CAM and inspect on which part of an image the CNN focuses on to predict its outputs, using the `def plot_gradcam()` function, which is added to `fns_blood_smears.py`.\n",
    "\n",
    "```python\n",
    "def plot_gradcam(image, grad_cam):\n",
    "    from matplotlib import pyplot as plt\n",
    "    import skimage\n",
    "\n",
    "    grad_cam = skimage.transform.resize(grad_cam, image.shape, order=2)\n",
    "    grad_cam = skimage.exposure.rescale_intensity(grad_cam, out_range=(0.25, 1))\n",
    "\n",
    "    plt.figure(figsize=(12, 5)) \n",
    "\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(image, interpolation=\"bilinear\")\n",
    "    plt.title(\"Original image\")\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.imshow(grad_cam.mean(axis=-1), interpolation=\"bilinear\")\n",
    "    plt.title(\"Grad-CAM\")\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.imshow(image * grad_cam)\n",
    "    plt.title(\"Overlay\")\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import relu\n",
    "from fnc_blood_smears import plot_gradcam\n",
    "\n",
    "pooled_gradients = bh.gradients[0].mean(dim=[1, 2], keepdim=True)\n",
    "grad_cam = relu((pooled_gradients * fh.activations[0]).sum(0)).detach().numpy()\n",
    "\n",
    "plot_gradcam(image_hr, grad_cam)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dt_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
